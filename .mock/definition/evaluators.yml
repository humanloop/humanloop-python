imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    list:
      path: /evaluators
      method: GET
      auth: true
      docs: Get a list of all Evaluators.
      pagination:
        offset: $request.page
        results: $response.records
      display-name: 'List '
      request:
        name: ListEvaluatorsGetRequest
        query-parameters:
          page:
            type: optional<integer>
            docs: Page offset for pagination.
          size:
            type: optional<integer>
            docs: Page size for pagination. Number of Evaluators to fetch.
          name:
            type: optional<string>
            docs: Case-insensitive filter for Evaluator name.
          user_filter:
            type: optional<string>
            docs: >-
              Case-insensitive filter for users in the Evaluator. This filter
              matches against both email address and name of users.
          sort_by:
            type: optional<root.ProjectSortBy>
            docs: Field to sort Evaluators by
          order:
            type: optional<root.SortOrder>
            docs: Direction to sort by.
      response:
        docs: Successful Response
        type: root.PaginatedDataEvaluatorResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: List evaluators
          query-parameters:
            size: 1
          response:
            body:
              records:
                - id: ev_890bcd
                  name: Accuracy Evaluator
                  path: Shared Evaluators/Accuracy Evaluator
                  version_id: evv_012def
                  type: evaluator
                  created_at: '2024-05-01T12:00:00Z'
                  updated_at: '2024-05-01T12:00:00Z'
                  status: committed
                  last_used_at: '2024-05-01T12:00:00Z'
                  spec:
                    arguments_type: target_required
                    return_type: number
                    evaluator_type: python
                    code: def evaluate(answer, target):\n    return 0.5
                  version_logs_count: 1
                  total_logs_count: 1
                  inputs:
                    - name: answer
              page: 0
              size: 1
              total: 1
    upsert:
      path: /evaluators
      method: POST
      auth: true
      docs: >-
        Create an Evaluator or update it with a new version if it already
        exists.


        Evaluators are identified by the `ID` or their `path`. The spec provided
        determines the version of the Evaluator.


        If you provide a commit message, then the new version will be committed;

        otherwise it will be uncommitted. If you try to commit an already
        committed version,

        an exception will be raised.
      display-name: Upsert
      request:
        name: EvaluatorsRequest
        body:
          properties:
            path:
              type: optional<string>
              docs: >-
                Path of the Evaluator, including the name. This locates the
                Evaluator in the Humanloop filesystem and is used as as a unique
                identifier. Example: `folder/name` or just `name`.
            id:
              type: optional<string>
              docs: ID for an existing Evaluator.
            commit_message:
              type: optional<string>
              docs: Message describing the changes made.
            spec: SrcExternalAppModelsV5EvaluatorsEvaluatorRequestSpec
      response:
        docs: Successful Response
        type: root.EvaluatorResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Create evaluator
          request:
            path: Shared Evaluators/Accuracy Evaluator
            spec:
              arguments_type: target_required
              return_type: number
              evaluator_type: python
              code: def evaluate(answer, target):\n    return 0.5
            commit_message: Initial commit
          response:
            body:
              id: ev_890bcd
              name: Accuracy Evaluator
              path: Shared Evaluators/Accuracy Evaluator
              version_id: evv_012def
              type: evaluator
              created_at: '2024-05-01T12:00:00Z'
              updated_at: '2024-05-01T12:00:00Z'
              status: committed
              last_used_at: '2024-05-01T12:00:00Z'
              spec:
                arguments_type: target_required
                return_type: number
                evaluator_type: python
                code: def evaluate(answer, target):\n    return 0.5
              version_logs_count: 1
              total_logs_count: 1
              inputs:
                - name: answer
    get:
      path: /evaluators/{id}
      method: GET
      auth: true
      docs: >-
        Retrieve the Evaluator with the given ID.


        By default, the deployed version of the Evaluator is returned. Use the
        query parameters

        `version_id` or `environment` to target a specific version of the
        Evaluator.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluator.
      display-name: Get
      request:
        name: GetEvaluatorsIdGetRequest
        query-parameters:
          version_id:
            type: optional<string>
            docs: A specific Version ID of the Evaluator to retrieve.
          environment:
            type: optional<string>
            docs: Name of the Environment to retrieve a deployed Version from.
      response:
        docs: Successful Response
        type: root.EvaluatorResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Get specific evaluator
          path-parameters:
            id: ev_890bcd
          response:
            body:
              id: ev_890bcd
              name: Accuracy Evaluator
              path: Shared Evaluators/Accuracy Evaluator
              version_id: evv_012def
              type: evaluator
              created_at: '2024-05-01T12:00:00Z'
              updated_at: '2024-05-01T12:00:00Z'
              status: committed
              last_used_at: '2024-05-01T12:00:00Z'
              spec:
                arguments_type: target_required
                return_type: number
                evaluator_type: python
                code: def evaluate(answer, target):\n    return 0.5
              version_logs_count: 1
              total_logs_count: 1
              inputs:
                - name: answer
    delete:
      path: /evaluators/{id}
      method: DELETE
      auth: true
      docs: Delete the Evaluator with the given ID.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluator.
      display-name: Delete
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Delete evaluator
          path-parameters:
            id: ev_890bcd
    move:
      path: /evaluators/{id}
      method: PATCH
      auth: true
      docs: Move the Evaluator to a different path or change the name.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluator.
      display-name: Move
      request:
        name: UpdateEvaluatorRequest
        body:
          properties:
            path:
              type: optional<string>
              docs: >-
                Path of the Evaluator including the Evaluator name, which is
                used as a unique identifier.
            name:
              type: optional<string>
              docs: Name of the Evaluator, which is used as a unique identifier.
      response:
        docs: Successful Response
        type: root.EvaluatorResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Move evaluator
          path-parameters:
            id: ev_890bcd
          request:
            path: new directory/new name
          response:
            body:
              id: ev_890bcd
              name: Accuracy Evaluator
              path: Shared Evaluators/Accuracy Evaluator
              version_id: evv_012def
              type: evaluator
              created_at: '2024-05-01T12:00:00Z'
              updated_at: '2024-05-01T12:00:00Z'
              status: committed
              last_used_at: '2024-05-01T12:00:00Z'
              spec:
                arguments_type: target_required
                return_type: number
                evaluator_type: python
                code: def evaluate(answer, target):\n    return 0.5
              version_logs_count: 1
              total_logs_count: 1
              inputs:
                - name: answer
    listVersions:
      path: /evaluators/{id}/versions
      method: GET
      auth: true
      docs: Get a list of all the versions of an Evaluator.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for the Evaluator.
      display-name: List Versions
      request:
        name: ListVersionsEvaluatorsIdVersionsGetRequest
        query-parameters:
          status:
            type: optional<root.VersionStatus>
            docs: >-
              Filter versions by status: 'uncommitted', 'committed'. If no
              status is provided, all versions are returned.
          evaluator_aggregates:
            type: optional<boolean>
            docs: >-
              Whether to include Evaluator aggregate results for the versions in
              the response
      response:
        docs: Successful Response
        type: root.ListEvaluators
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: List versions
          path-parameters:
            id: ev_890bcd
          response:
            body:
              records:
                - id: ev_890bcd
                  name: Accuracy Evaluator
                  path: Shared Evaluators/Accuracy Evaluator
                  version_id: evv_012def
                  type: evaluator
                  created_at: '2024-05-01T12:00:00Z'
                  updated_at: '2024-05-01T12:00:00Z'
                  status: committed
                  last_used_at: '2024-05-01T12:00:00Z'
                  spec:
                    arguments_type: target_required
                    return_type: number
                    evaluator_type: python
                    code: def evaluate(answer, target):\n    return 0.5
                  version_logs_count: 1
                  total_logs_count: 1
                  inputs:
                    - name: answer
    commit:
      path: /evaluators/{id}/versions/{version_id}/commit
      method: POST
      auth: true
      docs: |-
        Commit a version of the Evaluator with a commit message.

        If the version is already committed, an exception will be raised.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Prompt.
        version_id:
          type: string
          docs: Unique identifier for the specific version of the Evaluator.
      display-name: Commit
      request:
        body: root.CommitRequest
      response:
        docs: Successful Response
        type: root.EvaluatorResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Commit version
          path-parameters:
            id: ev_890bcd
            version_id: evv_012def
          request:
            commit_message: Initial commit
          response:
            body:
              id: ev_890bcd
              name: Accuracy Evaluator
              path: Shared Evaluators/Accuracy Evaluator
              version_id: evv_012def
              type: evaluator
              created_at: '2024-05-01T12:00:00Z'
              updated_at: '2024-05-01T12:00:00Z'
              status: committed
              last_used_at: '2024-05-01T12:00:00Z'
              spec:
                arguments_type: target_required
                return_type: number
                evaluator_type: python
                code: def evaluate(answer, target):\n    return 0.5
              version_logs_count: 1
              total_logs_count: 1
              inputs:
                - name: answer
    setDeployment:
      path: /evaluators/{id}/environments/{environment_id}
      method: POST
      auth: true
      docs: |-
        Deploy Evaluator to an Environment.

        Set the deployed version for the specified Environment. This Evaluator
        will be used for calls made to the Evaluator in this Environment.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluator.
        environment_id:
          type: string
          docs: Unique identifier for the Environment to deploy the Version to.
      display-name: Set Deployment
      request:
        name: SetDeploymentEvaluatorsIdEnvironmentsEnvironmentIdPostRequest
        query-parameters:
          version_id:
            type: string
            docs: Unique identifier for the specific version of the Evaluator.
      response:
        docs: Successful Response
        type: root.EvaluatorResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Deploy
          path-parameters:
            id: ev_890bcd
            environment_id: staging
          query-parameters:
            version_id: evv_012def
          response:
            body:
              id: ev_890bcd
              name: Accuracy Evaluator
              path: Shared Evaluators/Accuracy Evaluator
              version_id: evv_012def
              type: evaluator
              created_at: '2024-05-01T12:00:00Z'
              updated_at: '2024-05-01T12:00:00Z'
              status: committed
              last_used_at: '2024-05-01T12:00:00Z'
              spec:
                arguments_type: target_required
                return_type: number
                evaluator_type: python
                code: def evaluate(answer, target):\n    return 0.5
              version_logs_count: 1
              total_logs_count: 1
              inputs:
                - name: answer
    removeDeployment:
      path: /evaluators/{id}/environments/{environment_id}
      method: DELETE
      auth: true
      docs: >-
        Remove deployed Evaluator from the Environment.


        Remove the deployed version for the specified Environment. This
        Evaluator

        will no longer be used for calls made to the Evaluator in this
        Environment.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluator.
        environment_id:
          type: string
          docs: Unique identifier for the Environment to remove the deployment from.
      display-name: Remove Deployment
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Delete environment
          path-parameters:
            id: ev_890bcd
            environment_id: staging
    listEnvironments:
      path: /evaluators/{id}/environments
      method: GET
      auth: true
      docs: List all Environments and their deployed versions for the Evaluator.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluator.
      display-name: List Environments
      response:
        docs: Successful Response
        type: list<root.FileEnvironmentResponse>
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: List environments
          path-parameters:
            id: ev_890bcd
          response:
            body:
              - id: env_abc123
                created_at: '2024-05-01T12:00:00Z'
                name: production
                tag: default
                file:
                  id: ev_890bcd
                  name: Accuracy Evaluator
                  path: Shared Evaluators/Accuracy Evaluator
                  version_id: evv_012def
                  type: evaluator
                  created_at: '2024-05-01T12:00:00Z'
                  updated_at: '2024-05-01T12:00:00Z'
                  status: committed
                  last_used_at: '2024-05-01T12:00:00Z'
                  spec:
                    arguments_type: target_required
                    return_type: number
                    evaluator_type: python
                    code: def evaluate(answer, target):\n    return 0.5
                  version_logs_count: 1
                  total_logs_count: 1
                  inputs:
                    - name: answer
    log:
      path: /evaluators/log
      method: POST
      auth: true
      docs: >-
        Submit evalutor judgment for an existing Log. Creates a new Log and
        makes evaluated one its parent.
      display-name: Log
      request:
        name: CreateEvaluatorLogRequest
        query-parameters:
          version_id:
            type: optional<string>
            docs: ID of the Evaluator version to log against.
          environment:
            type: optional<string>
            docs: Name of the Environment identifying a deployed version to log to.
        body:
          properties:
            path:
              type: optional<string>
              docs: >-
                Path of the Evaluator, including the name. This locates the
                Evaluator in the Humanloop filesystem and is used as as a unique
                identifier. Example: `folder/name` or just `name`.
            id:
              type: optional<string>
              docs: ID for an existing Evaluator.
            output:
              type: optional<string>
              docs: >-
                Generated output from the LLM. Only populated for LLM Evaluator
                Logs.
            created_at:
              type: optional<datetime>
              docs: 'User defined timestamp for when the log was created. '
            error:
              type: optional<string>
              docs: Error message if the log is an error.
            provider_latency:
              type: optional<double>
              docs: Duration of the logged event in seconds.
            provider_request:
              type: optional<map<string, unknown>>
              docs: >-
                Raw request sent to provider. Only populated for LLM Evaluator
                Logs.
            provider_response:
              type: optional<map<string, unknown>>
              docs: >-
                Raw response received the provider. Only populated for LLM
                Evaluator Logs.
            session_id:
              type: optional<string>
              docs: >-
                Unique identifier for the Session to associate the Log to.
                Allows you to record multiple Logs to a Session (using an ID
                kept by your internal systems) by passing the same `session_id`
                in subsequent log requests.
            parent_id:
              type: string
              docs: >-
                Identifier of the evaluated Log. The newly created Log will have
                this one set as parent.
            inputs:
              type: optional<map<string, unknown>>
              docs: The inputs passed to the prompt template.
            source:
              type: optional<string>
              docs: Identifies where the model was called from.
            metadata:
              type: optional<map<string, unknown>>
              docs: Any additional metadata to record.
            save:
              type: optional<boolean>
              docs: >-
                Whether the request/response payloads will be stored on
                Humanloop.
              default: true
            source_datapoint_id:
              type: optional<string>
              docs: >-
                Unique identifier for the Datapoint that this Log is derived
                from. This can be used by Humanloop to associate Logs to
                Evaluations. If provided, Humanloop will automatically associate
                this Log to Evaluations that require a Log for this
                Datapoint-Version pair.
            batches:
              type: optional<list<string>>
              docs: >-
                Array of Batch Ids that this log is part of. Batches are used to
                group Logs together for offline Evaluations
            user:
              type: optional<string>
              docs: End-user ID related to the Log.
            environment:
              type: optional<string>
              docs: The name of the Environment the Log is associated to.
              name: createEvaluatorLogRequestEnvironment
            judgment: optional<unknown>
            spec: optional<CreateEvaluatorLogRequestSpec>
      response:
        docs: Successful Response
        type: root.CreateEvaluatorLogResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - request:
            parent_id: parent_id
          response:
            body:
              id: id
              parent_id: parent_id
              session_id: session_id
              version_id: version_id
types:
  SrcExternalAppModelsV5EvaluatorsEvaluatorRequestSpec:
    discriminated: false
    union:
      - root.LlmEvaluatorRequest
      - root.CodeEvaluatorRequest
      - root.HumanEvaluatorRequest
      - root.ExternalEvaluatorRequest
  CreateEvaluatorLogRequestSpec:
    discriminated: false
    union:
      - root.LlmEvaluatorRequest
      - root.CodeEvaluatorRequest
      - root.HumanEvaluatorRequest
      - root.ExternalEvaluatorRequest
