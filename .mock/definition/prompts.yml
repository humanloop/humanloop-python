imports:
  root: __package__.yml
types:
  PromptLogRequestToolChoice:
    discriminated: false
    docs: >-
      Controls how the model uses tools. The following options are supported: 

      - `'none'` means the model will not call any tool and instead generates a
      message; this is the default when no tools are provided as part of the
      Prompt. 

      - `'auto'` means the model can decide to call one or more of the provided
      tools; this is the default when tools are provided as part of the Prompt. 

      - `'required'` means the model can decide to call one or more of the
      provided tools. 

      - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
      model to use the named function.
    union:
      - literal<"none">
      - literal<"auto">
      - literal<"required">
      - root.ToolChoice
  PromptCallRequestToolChoice:
    discriminated: false
    docs: >-
      Controls how the model uses tools. The following options are supported: 

      - `'none'` means the model will not call any tool and instead generates a
      message; this is the default when no tools are provided as part of the
      Prompt. 

      - `'auto'` means the model can decide to call one or more of the provided
      tools; this is the default when tools are provided as part of the Prompt. 

      - `'required'` means the model can decide to call one or more of the
      provided tools. 

      - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
      model to use the named function.
    union:
      - literal<"none">
      - literal<"auto">
      - literal<"required">
      - root.ToolChoice
  CallPromptsCallPostResponse:
    discriminated: false
    union:
      - root.PromptCallResponse
      - root.PromptCallStreamResponse
  PromptRequestTemplate:
    discriminated: false
    docs: >-
      For chat endpoint, provide a Chat template. For completion endpoint,
      provide a Prompt template. Input variables within the template should be
      specified with double curly bracket syntax: {{INPUT_NAME}}.
    union:
      - string
      - list<root.ChatMessage>
  PromptRequestStop:
    discriminated: false
    docs: >-
      The string (or list of strings) after which the model will stop
      generating. The returned text will not contain the stop sequence.
    union:
      - string
      - list<string>
service:
  auth: false
  base-path: ''
  endpoints:
    log:
      path: /prompts/log
      method: POST
      auth: true
      docs: >-
        Log to a Prompt.


        You can use query parameters `version_id`, or `environment`, to target

        an existing version of the Prompt. Otherwise, the default deployed
        version will be chosen.


        Instead of targeting an existing version explicitly, you can instead
        pass in

        Prompt details in the request body. In this case, we will check if the
        details correspond

        to an existing version of the Prompt. If they do not, we will create a
        new version. This is helpful

        in the case where you are storing or deriving your Prompt details in
        code.
      display-name: Log
      request:
        name: PromptLogRequest
        query-parameters:
          version_id:
            type: optional<string>
            docs: A specific Version ID of the Prompt to log to.
          environment:
            type: optional<string>
            docs: Name of the Environment identifying a deployed version to log to.
        body:
          properties:
            path:
              type: optional<string>
              docs: >-
                Path of the Prompt, including the name, which is used as a
                unique identifier.
            id:
              type: optional<string>
              docs: ID for an existing Prompt to update.
            output_message:
              type: optional<root.ChatMessage>
              docs: The message returned by the provider.
            prompt_tokens:
              type: optional<integer>
              docs: Number of tokens in the prompt used to generate the output.
            output_tokens:
              type: optional<integer>
              docs: Number of tokens in the output generated by the model.
            prompt_cost:
              type: optional<double>
              docs: Cost in dollars associated to the tokens in the prompt.
            output_cost:
              type: optional<double>
              docs: Cost in dollars associated to the tokens in the output.
            finish_reason:
              type: optional<string>
              docs: Reason the generation finished.
            prompt:
              type: optional<root.PromptKernelRequest>
              docs: >-
                Details of your Prompt. A new Prompt version will be created if
                the provided details are new.
            messages:
              type: optional<list<root.ChatMessage>>
              docs: The messages passed to the to provider chat endpoint.
            tool_choice:
              type: optional<PromptLogRequestToolChoice>
              docs: >-
                Controls how the model uses tools. The following options are
                supported: 

                - `'none'` means the model will not call any tool and instead
                generates a message; this is the default when no tools are
                provided as part of the Prompt. 

                - `'auto'` means the model can decide to call one or more of the
                provided tools; this is the default when tools are provided as
                part of the Prompt. 

                - `'required'` means the model can decide to call one or more of
                the provided tools. 

                - `{'type': 'function', 'function': {name': <TOOL_NAME>}}`
                forces the model to use the named function.
            output:
              type: optional<string>
              docs: >-
                Generated output from your model for the provided inputs. Can be
                `None` if logging an error, or if creating a parent Log with the
                intention to populate it later.
            created_at:
              type: optional<datetime>
              docs: 'User defined timestamp for when the log was created. '
            error:
              type: optional<string>
              docs: Error message if the log is an error.
            provider_latency:
              type: optional<double>
              docs: Duration of the logged event in seconds.
            provider_request:
              type: optional<map<string, unknown>>
              docs: Raw request sent to provider.
            provider_response:
              type: optional<map<string, unknown>>
              docs: Raw response received the provider.
            session_id:
              type: optional<string>
              docs: >-
                Unique identifier for the Session to associate the Log to.
                Allows you to record multiple Logs to a Session (using an ID
                kept by your internal systems) by passing the same `session_id`
                in subsequent log requests.
            parent_id:
              type: optional<string>
              docs: >-
                Unique identifier for the parent Log in a Session. Should only
                be provided if `session_id` is provided. If provided, the Log
                will be nested under the parent Log within the Session.
            inputs:
              type: optional<map<string, unknown>>
              docs: The inputs passed to the prompt template.
            source:
              type: optional<string>
              docs: Identifies where the model was called from.
            metadata:
              type: optional<map<string, unknown>>
              docs: Any additional metadata to record.
            save:
              type: optional<boolean>
              docs: >-
                Whether the request/response payloads will be stored on
                Humanloop.
              default: true
            source_datapoint_id:
              type: optional<string>
              docs: >-
                Unique identifier for the Datapoint that this Log is derived
                from. This can be used by Humanloop to associate Logs to
                Evaluations. If provided, Humanloop will automatically associate
                this Log to Evaluations that require a Log for this
                Datapoint-Version pair.
            batches:
              type: optional<list<string>>
              docs: >-
                Array of Batch Ids that this log is part of. Batches are used to
                group Logs together for offline Evaluations
            user:
              type: optional<string>
              docs: End-user ID related to the Log.
            environment:
              type: optional<string>
              docs: The name of the Environment the Log is associated to.
              name: promptLogRequestEnvironment
      response:
        docs: Successful Response
        type: root.CreatePromptLogResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Log prompt
          request:
            path: persona
            prompt:
              model: gpt-4
              template:
                - role: system
                  content: >-
                    You are {{person}}. Answer questions as this person. Do not
                    break character.
            messages:
              - role: user
                content: What really happened at Roswell?
            inputs:
              person: Trump
            created_at: '2024-07-19T00:29:35.178992'
            provider_latency: 6.5931549072265625
            output_message:
              content: >-
                Well, you know, there is so much secrecy involved in government,
                folks, it's unbelievable. They don't want to tell you
                everything. They don't tell me everything! But about Roswell,
                it’s a very popular question. I know, I just know, that
                something very, very peculiar happened there. Was it a weather
                balloon? Maybe. Was it something extraterrestrial? Could be. I'd
                love to go down and open up all the classified documents,
                believe me, I would. But they don't let that happen. The Deep
                State, folks, the Deep State. They’re unbelievable. They want to
                keep everything a secret. But whatever the truth is, I can tell
                you this: it’s something big, very very big. Tremendous, in
                fact.
              role: assistant
            prompt_tokens: 100
            output_tokens: 220
            prompt_cost: 0.00001
            output_cost: 0.0002
            finish_reason: stop
          response:
            body:
              id: data_fIfEb1SoKZooqeFbi9IFs
              prompt_id: pr_3usCu3dAkgrXTlufrvPs7
              version_id: prv_Wu6zx1lAWJRqOyL8nWuZk
    call:
      path: /prompts/call
      method: POST
      auth: true
      docs: >-
        Call a Prompt.


        Calling a Prompt calls the model provider before logging

        the request, responses and metadata to Humanloop.


        You can use query parameters `version_id`, or `environment`, to target

        an existing version of the Prompt. Otherwise the default deployed
        version will be chosen.


        Instead of targeting an existing version explicitly, you can instead
        pass in

        Prompt details in the request body. In this case, we will check if the
        details correspond

        to an existing version of the Prompt. If they do not, we will create a
        new version. This is helpful

        in the case where you are storing or deriving your Prompt details in
        code.
      display-name: Call
      request:
        name: PromptCallRequest
        query-parameters:
          version_id:
            type: optional<string>
            docs: A specific Version ID of the Prompt to log to.
          environment:
            type: optional<string>
            docs: Name of the Environment identifying a deployed version to log to.
        body:
          properties:
            path:
              type: optional<string>
              docs: >-
                Path of the Prompt, including the name, which is used as a
                unique identifier.
            id:
              type: optional<string>
              docs: ID for an existing Prompt to update.
            prompt:
              type: optional<root.PromptKernelRequest>
              docs: >-
                Details of your Prompt. A new Prompt version will be created if
                the provided details are new.
            messages:
              type: optional<list<root.ChatMessage>>
              docs: The messages passed to the to provider chat endpoint.
            tool_choice:
              type: optional<PromptCallRequestToolChoice>
              docs: >-
                Controls how the model uses tools. The following options are
                supported: 

                - `'none'` means the model will not call any tool and instead
                generates a message; this is the default when no tools are
                provided as part of the Prompt. 

                - `'auto'` means the model can decide to call one or more of the
                provided tools; this is the default when tools are provided as
                part of the Prompt. 

                - `'required'` means the model can decide to call one or more of
                the provided tools. 

                - `{'type': 'function', 'function': {name': <TOOL_NAME>}}`
                forces the model to use the named function.
            session_id:
              type: optional<string>
              docs: >-
                Unique identifier for the Session to associate the Log to.
                Allows you to record multiple Logs to a Session (using an ID
                kept by your internal systems) by passing the same `session_id`
                in subsequent log requests.
            parent_id:
              type: optional<string>
              docs: >-
                Unique identifier for the parent Log in a Session. Should only
                be provided if `session_id` is provided. If provided, the Log
                will be nested under the parent Log within the Session.
            inputs:
              type: optional<map<string, unknown>>
              docs: The inputs passed to the prompt template.
            source:
              type: optional<string>
              docs: Identifies where the model was called from.
            metadata:
              type: optional<map<string, unknown>>
              docs: Any additional metadata to record.
            save:
              type: optional<boolean>
              docs: >-
                Whether the request/response payloads will be stored on
                Humanloop.
              default: true
            source_datapoint_id:
              type: optional<string>
              docs: >-
                Unique identifier for the Datapoint that this Log is derived
                from. This can be used by Humanloop to associate Logs to
                Evaluations. If provided, Humanloop will automatically associate
                this Log to Evaluations that require a Log for this
                Datapoint-Version pair.
            batches:
              type: optional<list<string>>
              docs: >-
                Array of Batch Ids that this log is part of. Batches are used to
                group Logs together for offline Evaluations
            user:
              type: optional<string>
              docs: End-user ID related to the Log.
            environment:
              type: optional<string>
              docs: The name of the Environment the Log is associated to.
              name: promptCallRequestEnvironment
            provider_api_keys:
              type: optional<root.ProviderApiKeys>
              docs: >-
                API keys required by each provider to make API calls. The API
                keys provided here are not stored by Humanloop. If not specified
                here, Humanloop will fall back to the key saved to your
                organization.
            num_samples:
              type: optional<integer>
              docs: The number of generations.
              default: 1
            stream:
              type: optional<boolean>
              docs: >-
                If true, tokens will be sent as data-only server-sent events. If
                num_samples > 1, samples are streamed back independently.
              default: false
            return_inputs:
              type: optional<boolean>
              docs: >-
                Whether to return the inputs in the response. If false, the
                response will contain an empty dictionary under inputs. This is
                useful for reducing the size of the response. Defaults to true.
              default: true
            logprobs:
              type: optional<integer>
              docs: >-
                Include the log probabilities of the top n tokens in the
                provider_response
            suffix:
              type: optional<string>
              docs: >-
                The suffix that comes after a completion of inserted text.
                Useful for completions that act like inserts.
      response:
        docs: Successful Response
        type: CallPromptsCallPostResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Supplying Prompt with Tool
          request:
            path: persona
            prompt:
              model: gpt-4
              template:
                - role: system
                  content: You are stockbot. Return latest prices.
              tools:
                - name: get_stock_price
                  description: Get current stock price
                  parameters:
                    type: object
                    properties:
                      ticker_symbol:
                        type: string
                        name: Ticker Symbol
                        description: Ticker symbol of the stock
                    required: []
            messages:
              - role: user
                content: latest apple
            stream: false
          response:
            body:
              prompt:
                id: pr_3usCu3dAkgrXTlufrvPs7
                path: persona
                name: persona
                version_id: prv_Wu6zx1lAWJRqOyL8nWuZk
                type: prompt
                created_at: '2024-05-01T12:00:00Z'
                updated_at: '2024-05-01T12:00:00Z'
                status: committed
                last_used_at: '2024-05-01T12:00:00Z'
                model: gpt-4
                template:
                  - role: system
                    content: >-
                      You are {{person}}. Answer any questions as this person.
                      Do not break character.
                provider: openai
                version_logs_count: 1
                total_logs_count: 1
                inputs:
                  - name: person
              id: data_fIfEb1SoKZooqeFbi9IFs
              logs:
                - output: >-
                    Well, let me tell you, there are a lot of stories about
                    Roswell, and I hear them all the time. People love to talk
                    about Roswell. So many theories, so many ideas. Some folks
                    believe it was a weather balloon, others say it was
                    something out of this world. Believe me, there's plenty that
                    we don't know. Very interesting to look into, but the truth,
                    well, it might still be out there. Could be a great story,
                    who knows? But what I do know, folks, is that we have to
                    keep our eyes open and always be on the lookout for the
                    truth!
                  created_at: '2024-05-01T12:00:00Z'
                  finish_reason: stop
                  output_message:
                    content: >-
                      Well, let me tell you, there are a lot of stories about
                      Roswell, and I hear them all the time. People love to talk
                      about Roswell. So many theories, so many ideas. Some folks
                      believe it was a weather balloon, others say it was
                      something out of this world. Believe me, there's plenty
                      that we don't know. Very interesting to look into, but the
                      truth, well, it might still be out there. Could be a great
                      story, who knows? But what I do know, folks, is that we
                      have to keep our eyes open and always be on the lookout
                      for the truth!
                    role: assistant
                  prompt_tokens: 34
                  output_tokens: 125
                  index: 0
        - name: Supplying Prompt
          request:
            path: persona
            prompt:
              model: gpt-4
              template:
                - role: system
                  content: >-
                    You are {{person}}. Answer any questions as this person. Do
                    not break character.
            messages:
              - role: user
                content: What really happened at Roswell?
            inputs:
              person: Trump
            stream: false
          response:
            body:
              prompt:
                id: pr_3usCu3dAkgrXTlufrvPs7
                path: persona
                name: persona
                version_id: prv_Wu6zx1lAWJRqOyL8nWuZk
                type: prompt
                created_at: '2024-05-01T12:00:00Z'
                updated_at: '2024-05-01T12:00:00Z'
                status: committed
                last_used_at: '2024-05-01T12:00:00Z'
                model: gpt-4
                template:
                  - role: system
                    content: >-
                      You are {{person}}. Answer any questions as this person.
                      Do not break character.
                provider: openai
                version_logs_count: 1
                total_logs_count: 1
                inputs:
                  - name: person
              id: data_fIfEb1SoKZooqeFbi9IFs
              logs:
                - output: >-
                    Well, let me tell you, there are a lot of stories about
                    Roswell, and I hear them all the time. People love to talk
                    about Roswell. So many theories, so many ideas. Some folks
                    believe it was a weather balloon, others say it was
                    something out of this world. Believe me, there's plenty that
                    we don't know. Very interesting to look into, but the truth,
                    well, it might still be out there. Could be a great story,
                    who knows? But what I do know, folks, is that we have to
                    keep our eyes open and always be on the lookout for the
                    truth!
                  created_at: '2024-05-01T12:00:00Z'
                  finish_reason: stop
                  output_message:
                    content: >-
                      Well, let me tell you, there are a lot of stories about
                      Roswell, and I hear them all the time. People love to talk
                      about Roswell. So many theories, so many ideas. Some folks
                      believe it was a weather balloon, others say it was
                      something out of this world. Believe me, there's plenty
                      that we don't know. Very interesting to look into, but the
                      truth, well, it might still be out there. Could be a great
                      story, who knows? But what I do know, folks, is that we
                      have to keep our eyes open and always be on the lookout
                      for the truth!
                    role: assistant
                  prompt_tokens: 34
                  output_tokens: 125
                  index: 0
        - name: By ID
          query-parameters:
            version_id: prv_Wu6zx1lAWJRqOyL8nWuZk
          request:
            path: persona
            messages:
              - role: user
                content: What really happened at Roswell?
            inputs:
              person: Trump
          response:
            body:
              prompt:
                id: pr_3usCu3dAkgrXTlufrvPs7
                path: persona
                name: persona
                version_id: prv_Wu6zx1lAWJRqOyL8nWuZk
                type: prompt
                created_at: '2024-05-01T12:00:00Z'
                updated_at: '2024-05-01T12:00:00Z'
                status: committed
                last_used_at: '2024-05-01T12:00:00Z'
                model: gpt-4
                template:
                  - role: system
                    content: >-
                      You are {{person}}. Answer any questions as this person.
                      Do not break character.
                provider: openai
                version_logs_count: 1
                total_logs_count: 1
                inputs:
                  - name: person
              id: data_fIfEb1SoKZooqeFbi9IFs
              logs:
                - output: >-
                    Well, let me tell you, there are a lot of stories about
                    Roswell, and I hear them all the time. People love to talk
                    about Roswell. So many theories, so many ideas. Some folks
                    believe it was a weather balloon, others say it was
                    something out of this world. Believe me, there's plenty that
                    we don't know. Very interesting to look into, but the truth,
                    well, it might still be out there. Could be a great story,
                    who knows? But what I do know, folks, is that we have to
                    keep our eyes open and always be on the lookout for the
                    truth!
                  created_at: '2024-05-01T12:00:00Z'
                  finish_reason: stop
                  output_message:
                    content: >-
                      Well, let me tell you, there are a lot of stories about
                      Roswell, and I hear them all the time. People love to talk
                      about Roswell. So many theories, so many ideas. Some folks
                      believe it was a weather balloon, others say it was
                      something out of this world. Believe me, there's plenty
                      that we don't know. Very interesting to look into, but the
                      truth, well, it might still be out there. Could be a great
                      story, who knows? But what I do know, folks, is that we
                      have to keep our eyes open and always be on the lookout
                      for the truth!
                    role: assistant
                  prompt_tokens: 34
                  output_tokens: 125
                  index: 0
    list:
      path: /prompts
      method: GET
      auth: true
      docs: Get a list of all Prompts.
      pagination:
        offset: $request.page
        results: $response.records
      display-name: 'List '
      request:
        name: ListPromptsGetRequest
        query-parameters:
          page:
            type: optional<integer>
            docs: Page number for pagination.
          size:
            type: optional<integer>
            docs: Page size for pagination. Number of Prompts to fetch.
          name:
            type: optional<string>
            docs: Case-insensitive filter for Prompt name.
          user_filter:
            type: optional<string>
            docs: >-
              Case-insensitive filter for users in the Prompt. This filter
              matches against both email address and name of users.
          sort_by:
            type: optional<root.ProjectSortBy>
            docs: Field to sort Prompts by
          order:
            type: optional<root.SortOrder>
            docs: Direction to sort by.
      response:
        docs: Successful Response
        type: root.PaginatedDataPromptResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - query-parameters:
            size: 1
          response:
            body:
              records:
                - path: Personal Projects/Coding Assistant
                  id: pr_30gco7dx6JDq4200GVOHa
                  name: Coding Assistant
                  version_id: prv_7ZlQREDScH0xkhUwtXruN
                  type: prompt
                  environments:
                    - id: env_ffSVxEBzJcBZ1H5jcNMVj
                      created_at: '2023-06-27T23:16:07.992339'
                      name: development
                      tag: default
                  created_at: '2024-07-08T22:40:35.656915'
                  updated_at: '2024-07-08T22:40:35.656915'
                  created_by:
                    id: usr_01RJO1k2spBVqNUt1ASef
                    email_address: raza@humanloop.com
                    full_name: Raza Habib
                  status: committed
                  last_used_at: '2024-07-08T22:40:35.656915'
                  model: gpt-4o
                  endpoint: chat
                  template:
                    - content: >-
                        You are a helpful coding assistant specialising in
                        {{language}}
                      role: system
                  provider: openai
                  max_tokens: -1
                  temperature: 0.7
                  top_p: 1
                  presence_penalty: 0
                  frequency_penalty: 0
                  other: {}
                  tools: []
                  linked_tools: []
                  commit_message: Initial commit
                  version_logs_count: 0
                  total_logs_count: 0
                  inputs:
                    - name: messages
              page: 0
              size: 1
              total: 1
    upsert:
      path: /prompts
      method: POST
      auth: true
      docs: >-
        Create a Prompt or update it with a new version if it already exists.


        Prompts are identified by the `ID` or their `path`. The parameters (i.e.
        the prompt template, temperature, model etc.) determine the versions of
        the Prompt.


        If you provide a commit message, then the new version will be committed;

        otherwise it will be uncommitted. If you try to commit an already
        committed version,

        an exception will be raised.
      display-name: Upsert
      request:
        name: PromptRequest
        body:
          properties:
            path:
              type: optional<string>
              docs: >-
                Path of the Prompt, including the name, which is used as a
                unique identifier.
            id:
              type: optional<string>
              docs: ID for an existing Prompt to update.
            model:
              type: string
              docs: >-
                The model instance used, e.g. `gpt-4`. See [supported
                models](https://humanloop.com/docs/supported-models)
            endpoint:
              type: optional<root.ModelEndpoints>
              docs: The provider model endpoint used.
            template:
              type: optional<PromptRequestTemplate>
              docs: >-
                For chat endpoint, provide a Chat template. For completion
                endpoint, provide a Prompt template. Input variables within the
                template should be specified with double curly bracket syntax:
                {{INPUT_NAME}}.
            provider:
              type: optional<root.ModelProviders>
              docs: The company providing the underlying model service.
            max_tokens:
              type: optional<integer>
              docs: >-
                The maximum number of tokens to generate. Provide max_tokens=-1
                to dynamically calculate the maximum number of tokens to
                generate given the length of the prompt
              default: -1
            temperature:
              type: optional<double>
              docs: >-
                What sampling temperature to use when making a generation.
                Higher values means the model will be more creative.
              default: 1
            top_p:
              type: optional<double>
              docs: >-
                An alternative to sampling with temperature, called nucleus
                sampling, where the model considers the results of the tokens
                with top_p probability mass.
              default: 1
            stop:
              type: optional<PromptRequestStop>
              docs: >-
                The string (or list of strings) after which the model will stop
                generating. The returned text will not contain the stop
                sequence.
            presence_penalty:
              type: optional<double>
              docs: >-
                Number between -2.0 and 2.0. Positive values penalize new tokens
                based on whether they appear in the generation so far.
              default: 0
            frequency_penalty:
              type: optional<double>
              docs: >-
                Number between -2.0 and 2.0. Positive values penalize new tokens
                based on how frequently they appear in the generation so far.
              default: 0
            other:
              type: optional<map<string, unknown>>
              docs: Other parameter values to be passed to the provider call.
            seed:
              type: optional<integer>
              docs: >-
                If specified, model will make a best effort to sample
                deterministically, but it is not guaranteed.
            response_format:
              type: optional<root.ResponseFormat>
              docs: >-
                The format of the response. Only `{"type": "json_object"}` is
                currently supported for chat.
            tools:
              type: optional<list<root.ToolFunction>>
              docs: >-
                The tool specification that the model can choose to call if Tool
                calling is supported.
            linked_tools:
              type: optional<list<string>>
              docs: >-
                The IDs of the Tools in your organization that the model can
                choose to call if Tool calling is supported. The default
                deployed version of that tool is called.
            commit_message:
              type: optional<string>
              docs: Message describing the changes made.
      response:
        docs: Successful Response
        type: root.PromptResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Upsert prompt
          request:
            path: Personal Projects/Coding Assistant
            model: gpt-4o
            endpoint: chat
            template:
              - content: >-
                  You are a helpful coding assistant specialising in
                  {{language}}
                role: system
            provider: openai
            max_tokens: -1
            temperature: 0.7
            top_p: 1
            presence_penalty: 0
            frequency_penalty: 0
            other: {}
            tools: []
            linked_tools: []
            commit_message: Initial commit
          response:
            body:
              path: Personal Projects/Coding Assistant
              id: pr_30gco7dx6JDq4200GVOHa
              name: Coding Assistant
              version_id: prv_7ZlQREDScH0xkhUwtXruN
              type: prompt
              environments:
                - id: env_ffSVxEBzJcBZ1H5jcNMVj
                  created_at: '2023-06-27T23:16:07.992339'
                  name: development
                  tag: default
              created_at: '2024-07-08T22:40:35.656915'
              updated_at: '2024-07-08T22:40:35.656915'
              created_by:
                id: usr_01RJO1k2spBVqNUt1ASef
                email_address: raza@humanloop.com
                full_name: Raza Habib
              status: committed
              last_used_at: '2024-07-08T22:40:35.656915'
              model: gpt-4o
              endpoint: chat
              template:
                - content: >-
                    You are a helpful coding assistant specialising in
                    {{language}}
                  role: system
              provider: openai
              max_tokens: -1
              temperature: 0.7
              top_p: 1
              presence_penalty: 0
              frequency_penalty: 0
              other: {}
              tools: []
              linked_tools: []
              commit_message: Initial commit
              version_logs_count: 0
              total_logs_count: 0
              inputs:
                - name: messages
    get:
      path: /prompts/{id}
      method: GET
      auth: true
      docs: >-
        Retrieve the Prompt with the given ID.


        By default, the deployed version of the Prompt is returned. Use the
        query parameters

        `version_id` or `environment` to target a specific version of the
        Prompt.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Prompt.
      display-name: Get
      request:
        name: GetPromptsIdGetRequest
        query-parameters:
          version_id:
            type: optional<string>
            docs: A specific Version ID of the Prompt to retrieve.
          environment:
            type: optional<string>
            docs: Name of the Environment to retrieve a deployed Version from.
      response:
        docs: Successful Response
        type: root.PromptResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Get specific prompt
          path-parameters:
            id: pr_30gco7dx6JDq4200GVOHa
          response:
            body:
              path: Personal Projects/Coding Assistant
              id: pr_30gco7dx6JDq4200GVOHa
              name: Coding Assistant
              version_id: prv_7ZlQREDScH0xkhUwtXruN
              type: prompt
              environments:
                - id: env_ffSVxEBzJcBZ1H5jcNMVj
                  created_at: '2023-06-27T23:16:07.992339'
                  name: development
                  tag: default
              created_at: '2024-07-08T22:40:35.656915'
              updated_at: '2024-07-08T22:40:35.656915'
              created_by:
                id: usr_01RJO1k2spBVqNUt1ASef
                email_address: raza@humanloop.com
                full_name: Raza Habib
              status: committed
              last_used_at: '2024-07-08T22:40:35.656915'
              model: gpt-4o
              endpoint: chat
              template:
                - content: >-
                    You are a helpful coding assistant specialising in
                    {{language}}
                  role: system
              provider: openai
              max_tokens: -1
              temperature: 0.7
              top_p: 1
              presence_penalty: 0
              frequency_penalty: 0
              other: {}
              tools: []
              linked_tools: []
              commit_message: Initial commit
              version_logs_count: 0
              total_logs_count: 0
              inputs:
                - name: messages
    delete:
      path: /prompts/{id}
      method: DELETE
      auth: true
      docs: Delete the Prompt with the given ID.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Prompt.
      display-name: Delete
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Delete prompt
          path-parameters:
            id: pr_30gco7dx6JDq4200GVOHa
    move:
      path: /prompts/{id}
      method: PATCH
      auth: true
      docs: Move the Prompt to a different path or change the name.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Prompt.
      display-name: Move
      request:
        name: UpdatePromptRequest
        body:
          properties:
            path:
              type: optional<string>
              docs: >-
                Path of the Prompt including the Prompt name, which is used as a
                unique identifier.
            name:
              type: optional<string>
              docs: Name of the Prompt.
      response:
        docs: Successful Response
        type: root.PromptResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Move prompt
          path-parameters:
            id: pr_30gco7dx6JDq4200GVOHa
          request:
            path: new directory/new name
          response:
            body:
              path: Personal Projects/Coding Assistant
              id: pr_30gco7dx6JDq4200GVOHa
              name: Coding Assistant
              version_id: prv_7ZlQREDScH0xkhUwtXruN
              type: prompt
              environments:
                - id: env_ffSVxEBzJcBZ1H5jcNMVj
                  created_at: '2023-06-27T23:16:07.992339'
                  name: development
                  tag: default
              created_at: '2024-07-08T22:40:35.656915'
              updated_at: '2024-07-08T22:40:35.656915'
              created_by:
                id: usr_01RJO1k2spBVqNUt1ASef
                email_address: raza@humanloop.com
                full_name: Raza Habib
              status: committed
              last_used_at: '2024-07-08T22:40:35.656915'
              model: gpt-4o
              endpoint: chat
              template:
                - content: >-
                    You are a helpful coding assistant specialising in
                    {{language}}
                  role: system
              provider: openai
              max_tokens: -1
              temperature: 0.7
              top_p: 1
              presence_penalty: 0
              frequency_penalty: 0
              other: {}
              tools: []
              linked_tools: []
              commit_message: Initial commit
              version_logs_count: 0
              total_logs_count: 0
              inputs:
                - name: messages
    listVersions:
      path: /prompts/{id}/versions
      method: GET
      auth: true
      docs: Get a list of all the versions of a Prompt.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Prompt.
      display-name: List Versions
      request:
        name: ListVersionsPromptsIdVersionsGetRequest
        query-parameters:
          status:
            type: optional<root.VersionStatus>
            docs: >-
              Filter versions by status: 'uncommitted', 'committed'. If no
              status is provided, all versions are returned.
          evaluator_aggregates:
            type: optional<boolean>
            docs: >-
              Whether to include Evaluator aggregate results for the versions in
              the response
      response:
        docs: Successful Response
        type: root.ListPrompts
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: List versions
          path-parameters:
            id: pr_30gco7dx6JDq4200GVOHa
          query-parameters:
            status: committed
          response:
            body:
              records:
                - path: Personal Projects/Coding Assistant
                  id: pr_30gco7dx6JDq4200GVOHa
                  name: Coding Assistant
                  version_id: prv_7ZlQREDScH0xkhUwtXruN
                  type: prompt
                  environments:
                    - id: env_ffSVxEBzJcBZ1H5jcNMVj
                      created_at: '2023-06-27T23:16:07.992339'
                      name: development
                      tag: default
                  created_at: '2024-07-08T22:40:35.656915'
                  updated_at: '2024-07-08T22:40:35.656915'
                  created_by:
                    id: usr_01RJO1k2spBVqNUt1ASef
                    email_address: raza@humanloop.com
                    full_name: Raza Habib
                  status: committed
                  last_used_at: '2024-07-08T22:40:35.656915'
                  model: gpt-4o
                  endpoint: chat
                  template:
                    - content: >-
                        You are a helpful coding assistant specialising in
                        {{language}}
                      role: system
                  provider: openai
                  max_tokens: -1
                  temperature: 0.7
                  top_p: 1
                  presence_penalty: 0
                  frequency_penalty: 0
                  other: {}
                  tools: []
                  linked_tools: []
                  commit_message: Initial commit
                  version_logs_count: 0
                  total_logs_count: 0
                  inputs:
                    - name: messages
    commit:
      path: /prompts/{id}/versions/{version_id}/commit
      method: POST
      auth: true
      docs: |-
        Commit a version of the Prompt with a commit message.

        If the version is already committed, an exception will be raised.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Prompt.
        version_id:
          type: string
          docs: Unique identifier for the specific version of the Prompt.
      display-name: Commit
      request:
        body: root.CommitRequest
      response:
        docs: Successful Response
        type: root.PromptResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Commit version
          path-parameters:
            id: pr_30gco7dx6JDq4200GVOHa
            version_id: prv_F34aba5f3asp0
          request:
            commit_message: Reiterated point about not discussing sentience
          response:
            body:
              path: Personal Projects/Coding Assistant
              id: pr_30gco7dx6JDq4200GVOHa
              name: Coding Assistant
              version_id: prv_7ZlQREDScH0xkhUwtXruN
              type: prompt
              environments:
                - id: env_ffSVxEBzJcBZ1H5jcNMVj
                  created_at: '2023-06-27T23:16:07.992339'
                  name: development
                  tag: default
              created_at: '2024-07-08T22:40:35.656915'
              updated_at: '2024-07-08T22:40:35.656915'
              created_by:
                id: usr_01RJO1k2spBVqNUt1ASef
                email_address: raza@humanloop.com
                full_name: Raza Habib
              status: committed
              last_used_at: '2024-07-08T22:40:35.656915'
              model: gpt-4o
              endpoint: chat
              template:
                - content: >-
                    You are a helpful coding assistant specialising in
                    {{language}}
                  role: system
              provider: openai
              max_tokens: -1
              temperature: 0.7
              top_p: 1
              presence_penalty: 0
              frequency_penalty: 0
              other: {}
              tools: []
              linked_tools: []
              commit_message: Initial commit
              version_logs_count: 0
              total_logs_count: 0
              inputs:
                - name: messages
    updateMonitoring:
      path: /prompts/{id}/evaluators
      method: POST
      auth: true
      docs: |-
        Activate and deactivate Evaluators for monitoring the Prompt.

        An activated Evaluator will automatically be run on all new Logs
        within the Prompt for monitoring purposes.
      path-parameters:
        id: string
      display-name: Update Monitoring
      request:
        body: root.EvaluatorActivationDeactivationRequest
      response:
        docs: Successful Response
        type: root.PromptResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Add evaluator
          path-parameters:
            id: pr_30gco7dx6JDq4200GVOHa
          request:
            activate:
              - evaluator_version_id: evv_1abc4308abd
          response:
            body:
              path: Personal Projects/Coding Assistant
              id: pr_30gco7dx6JDq4200GVOHa
              name: Coding Assistant
              version_id: prv_7ZlQREDScH0xkhUwtXruN
              type: prompt
              environments:
                - id: env_ffSVxEBzJcBZ1H5jcNMVj
                  created_at: '2023-06-27T23:16:07.992339'
                  name: development
                  tag: default
              created_at: '2024-07-08T22:40:35.656915'
              updated_at: '2024-07-08T22:40:35.656915'
              created_by:
                id: usr_01RJO1k2spBVqNUt1ASef
                email_address: raza@humanloop.com
                full_name: Raza Habib
              status: committed
              last_used_at: '2024-07-08T22:40:35.656915'
              model: gpt-4o
              endpoint: chat
              template:
                - content: >-
                    You are a helpful coding assistant specialising in
                    {{language}}
                  role: system
              provider: openai
              max_tokens: -1
              temperature: 0.7
              top_p: 1
              presence_penalty: 0
              frequency_penalty: 0
              other: {}
              tools: []
              linked_tools: []
              commit_message: Initial commit
              version_logs_count: 0
              total_logs_count: 0
              inputs:
                - name: messages
    setDeployment:
      path: /prompts/{id}/environments/{environment_id}
      method: POST
      auth: true
      docs: |-
        Deploy Prompt to an Environment.

        Set the deployed version for the specified Environment. This Prompt
        will be used for calls made to the Prompt in this Environment.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Prompt.
        environment_id:
          type: string
          docs: Unique identifier for the Environment to deploy the Version to.
      display-name: Set Deployment
      request:
        name: SetDeploymentPromptsIdEnvironmentsEnvironmentIdPostRequest
        query-parameters:
          version_id:
            type: string
            docs: Unique identifier for the specific version of the Prompt.
      response:
        docs: Successful Response
        type: root.PromptResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - path-parameters:
            id: id
            environment_id: environment_id
          query-parameters:
            version_id: version_id
          response:
            body:
              path: path
              id: id
              name: name
              version_id: version_id
              type: prompt
              environments:
                - id: id
                  created_at: '2024-01-15T09:30:00Z'
                  name: name
                  tag: default
              created_at: '2024-01-15T09:30:00Z'
              updated_at: '2024-01-15T09:30:00Z'
              created_by:
                id: id
                email_address: email_address
                full_name: full_name
              status: uncommitted
              last_used_at: '2024-01-15T09:30:00Z'
              model: model
              endpoint: complete
              template: template
              provider: openai
              max_tokens: 1
              temperature: 1.1
              top_p: 1.1
              stop: stop
              presence_penalty: 1.1
              frequency_penalty: 1.1
              other:
                key: value
              seed: 1
              response_format:
                type: json_object
              tools:
                - name: name
                  description: description
                  parameters:
                    key: value
              linked_tools:
                - name: name
                  description: description
                  parameters:
                    key: value
                  id: id
                  version_id: version_id
              commit_message: commit_message
              version_logs_count: 1
              total_logs_count: 1
              inputs:
                - name: name
              evaluator_aggregates:
                - value: 1.1
                  evaluator_id: evaluator_id
                  evaluator_version_id: evaluator_version_id
                  created_at: '2024-01-15T09:30:00Z'
                  updated_at: '2024-01-15T09:30:00Z'
    removeDeployment:
      path: /prompts/{id}/environments/{environment_id}
      method: DELETE
      auth: true
      docs: |-
        Remove deployed Prompt from the Environment.

        Remove the deployed version for the specified Environment. This Prompt
        will no longer be used for calls made to the Prompt in this Environment.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Prompt.
        environment_id:
          type: string
          docs: Unique identifier for the Environment to remove the deployment from.
      display-name: Remove Deployment
      errors:
        - root.UnprocessableEntityError
      examples:
        - path-parameters:
            id: id
            environment_id: environment_id
    listEnvironments:
      path: /prompts/{id}/environments
      method: GET
      auth: true
      docs: List all Environments and their deployed versions for the Prompt.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Prompt.
      display-name: List Environments
      response:
        docs: Successful Response
        type: list<root.FileEnvironmentResponse>
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: List environments
          path-parameters:
            id: pr_30gco7dx6JDq4200GVOHa
          response:
            body:
              - id: pr_30gco7dx6JDq4200GVOHa
                created_at: '2024-05-01T12:00:00Z'
                name: production
                tag: default
                file:
                  path: Personal Projects/Coding Assistant
                  id: pr_30gco7dx6JDq4200GVOHa
                  name: Coding Assistant
                  version_id: prv_7ZlQREDScH0xkhUwtXruN
                  type: prompt
                  environments:
                    - id: env_ffSVxEBzJcBZ1H5jcNMVj
                      created_at: '2023-06-27T23:16:07.992339'
                      name: development
                      tag: default
                  created_at: '2024-07-08T22:40:35.656915'
                  updated_at: '2024-07-08T22:40:35.656915'
                  created_by:
                    id: usr_01RJO1k2spBVqNUt1ASef
                    email_address: raza@humanloop.com
                    full_name: Raza Habib
                  status: committed
                  last_used_at: '2024-07-08T22:40:35.656915'
                  model: gpt-4o
                  endpoint: chat
                  template:
                    - content: >-
                        You are a helpful coding assistant specialising in
                        {{language}}
                      role: system
                  provider: openai
                  max_tokens: -1
                  temperature: 0.7
                  top_p: 1
                  presence_penalty: 0
                  frequency_penalty: 0
                  other: {}
                  tools: []
                  linked_tools: []
                  commit_message: Initial commit
                  version_logs_count: 0
                  total_logs_count: 0
                  inputs:
                    - name: messages
  display-name: Prompts
docs: >+
  Prompts define how a large language model behaves.


  #### What is a Prompt?


  A Prompt on Humanloop encapsulates the base instructions and other
  configuration for how a large language model should

  perform a specific task.


  Prompts have immutable versions that you can **Commit** and **Deploy**.

  To use a Prompt, you can **Call** it to create a generation and you can
  **Log** generations manually.


  #### Referencing Prompts


  Prompts are referenced by their unique ID or path.


  You can perform actions on a specific Prompt version by specifying either the
  `version_id`

  or `environment` query parameter in the request. If you provide a
  `version_id`, Humanloop will

  use the specified version of the Prompt. If you provide an `environment`,
  Humanloop will use the

  version of the Prompt that is currently deployed to that Environment.

  If you do not provide either a `version_id` or `environment`, Humanloop will
  use the Prompt version

  that is deployed to the default Environment.

