errors:
  UnprocessableEntityError:
    status-code: 422
    type: HttpValidationError
    docs: Validation Error
types:
  AgentConfigResponse:
    properties:
      id:
        type: string
        docs: String ID of config. Starts with `config_`.
      other:
        type: optional<map<string, unknown>>
        docs: Other parameters that define the config.
      type: literal<"agent">
      created_by:
        type: optional<BaseModelsUserResponse>
        docs: The user who created the config.
      status:
        type: string
        docs: Whether the config is committed or not.
      name:
        type: string
        docs: Name of config.
      description:
        type: optional<string>
        docs: Description of config.
      agent_class:
        type: string
        docs: Class of the agent.
      tools:
        type: optional<list<ToolConfigRequest>>
        docs: Tools associated with the agent.
  BaseMetricResponse:
    properties:
      id:
        type: string
        docs: ID of the metric. Starts with 'metric_'.
      name:
        type: string
        docs: The name of the metric.
      description:
        type: string
        docs: A description of what the metric measures.
      code:
        type: string
        docs: Python code used to calculate a metric value on each logged datapoint.
      default:
        type: boolean
        docs: >-
          Whether the metric is a global default metric. Metrics with this flag
          enabled cannot be deleted or modified.
      active:
        type: boolean
        docs: If enabled, the metric is calculated for every logged datapoint.
      created_at: datetime
      updated_at: datetime
  BooleanEvaluatorVersionStats:
    docs: |-
      Base attributes for stats for an Evaluator Version-Evaluated Version pair
      in the Evaluation Report.
    properties:
      evaluator_version_id:
        type: string
        docs: Unique identifier for the Evaluator Version.
      total_logs:
        type: integer
        docs: >-
          The total number of Logs generated by this Evaluator Version on the
          Evaluated Version's Logs. This includes Nulls and Errors.
      num_judgments:
        type: integer
        docs: >-
          The total number of Evaluator judgments for this Evaluator Version.
          This excludes Nulls and Errors.
      num_nulls:
        type: integer
        docs: >-
          The total number of null judgments (i.e. abstentions) for this
          Evaluator Version.
      num_errors:
        type: integer
        docs: The total number of errored Evaluators for this Evaluator Version.
      num_true:
        type: integer
        docs: The total number of `True` judgments for this Evaluator Version.
      num_false:
        type: integer
        docs: The total number of `False` judgments for this Evaluator Version.
  CategoricalFeedbackLabel:
    properties:
      value: string
      sentiment:
        type: LabelSentiment
        docs: Whether the feedback sentiment is positive or negative.
      status:
        type: FeedbackLabelStatus
        docs: Whether the feedback label is active or inactive.
  ChatMessageContentItem:
    discriminated: false
    union:
      - TextChatContent
      - ImageChatContent
  ChatMessageContent:
    discriminated: false
    docs: The content of the message.
    union:
      - string
      - list<ChatMessageContentItem>
  ChatMessage:
    properties:
      content:
        type: optional<ChatMessageContent>
        docs: The content of the message.
      name:
        type: optional<string>
        docs: Optional name of the message author.
      tool_call_id:
        type: optional<string>
        docs: Tool call that this message is responding to.
      role:
        type: ChatRole
        docs: Role of the message author.
      tool_calls:
        type: optional<list<ToolCall>>
        docs: A list of tool calls requested by the assistant.
  ChatMessageWithToolCallContentItem:
    discriminated: false
    union:
      - TextChatContent
      - ImageChatContent
  ChatMessageWithToolCallContent:
    discriminated: false
    docs: The content of the message.
    union:
      - string
      - list<ChatMessageWithToolCallContentItem>
  ChatMessageWithToolCall:
    properties:
      content:
        type: optional<ChatMessageWithToolCallContent>
        docs: The content of the message.
      name:
        type: optional<string>
        docs: Optional name of the message author.
      tool_call_id:
        type: optional<string>
        docs: Tool call that this message is responding to.
      role:
        type: ChatRole
        docs: Role of the message author.
      tool_calls:
        type: optional<list<ToolCall>>
        docs: A list of tool calls requested by the assistant.
      tool_call:
        type: optional<FunctionTool>
        docs: >-
          NB: Deprecated in favour of tool_calls. A tool call requested by the
          assistant.
        availability: deprecated
  ChatRole:
    enum:
      - user
      - assistant
      - system
      - tool
    docs: An enumeration.
  CodeEvaluatorRequest:
    properties:
      arguments_type:
        type: EvaluatorArgumentsType
        docs: Whether this evaluator is target-free or target-required.
      return_type:
        type: EvaluatorReturnTypeEnum
        docs: The type of the return value of the evaluator.
      evaluator_type: literal<"python">
      code:
        type: optional<string>
        docs: >-
          The code for the evaluator. This code will be executed in a sandboxed
          environment.
  CommitRequest:
    properties:
      commit_message:
        type: string
        docs: Message describing the changes made.
  ConfigResponse:
    discriminated: false
    union:
      - ModelConfigResponse
      - ToolConfigResponse
      - EvaluatorConfigResponse
      - AgentConfigResponse
      - GenericConfigResponse
  CreateDatapointRequestTargetValue:
    discriminated: false
    union:
      - string
      - integer
      - double
      - boolean
      - map<string, unknown>
      - list<unknown>
  CreateDatapointRequest:
    properties:
      inputs:
        type: optional<map<string, string>>
        docs: The inputs to the prompt template.
      messages:
        type: optional<list<ChatMessage>>
        docs: List of chat messages to provide to the model.
      target:
        type: optional<map<string, CreateDatapointRequestTargetValue>>
        docs: >-
          Object with criteria necessary to evaluate generations with this
          Datapoint. This is passed in as an argument to Evaluators when used in
          an Evaluation.
  CreateEvaluationRequest:
    docs: >-
      Request model for creating an Evaluation.


      Evaluation benchmark your Prompt/Tool Versions. With the Datapoints in a
      Dataset Version,

      Logs corresponding to the Datapoint and each Evaluated Version are
      evaluated by the specified Evaluator Versions.

      Aggregated statistics are then calculated and presented in the Evaluation.
    properties:
      dataset:
        type: EvaluationsDatasetRequest
        docs: The Dataset Version to use in this Evaluation.
      evaluatees:
        docs: >-
          Unique identifiers for the Prompt/Tool Versions to include in the
          Evaluation Report.
        type: list<EvaluateeRequest>
      evaluators:
        docs: The Evaluators used to evaluate.
        type: list<EvaluationsRequest>
  CreateEvaluatorLogResponse:
    properties:
      id:
        type: string
        docs: String identifier of the new Log.
      parent_id:
        type: string
        docs: Identifier of the evaluated parent Log.
      session_id:
        type: optional<string>
        docs: >-
          Identifier of the Session containing both the parent and the new child
          Log. If the parent Log does not belong to a Session, a new Session is
          created with this ID.
      version_id:
        type: string
        docs: Identifier of Evaluator Version for which the Log was registered.
  CreatePromptLogResponse:
    properties:
      id:
        type: string
        docs: String ID of log.
      prompt_id:
        type: string
        docs: ID of the Prompt the log belongs to.
      version_id:
        type: string
        docs: ID of the specific version of the Prompt.
      session_id:
        type: optional<string>
        docs: String ID of session the log belongs to.
  CreateToolLogResponse:
    properties:
      id:
        type: string
        docs: String ID of log.
      tool_id:
        type: string
        docs: ID of the Tool the log belongs to.
      version_id:
        type: string
        docs: ID of the specific version of the Tool.
      session_id:
        type: optional<string>
        docs: String ID of session the log belongs to.
  DashboardConfiguration:
    properties:
      time_unit: TimeUnit
      time_range_days: integer
      model_config_ids: list<string>
  DatapointResponseTargetValue:
    discriminated: false
    union:
      - string
      - integer
      - double
      - boolean
      - map<string, unknown>
      - list<unknown>
  DatapointResponse:
    properties:
      inputs:
        type: optional<map<string, string>>
        docs: The inputs to the prompt template.
      messages:
        type: optional<list<ChatMessage>>
        docs: List of chat messages to provide to the model.
      target:
        type: optional<map<string, DatapointResponseTargetValue>>
        docs: >-
          Object with criteria necessary to evaluate generations with this
          Datapoint. This is passed in as an argument to Evaluators when used in
          an Evaluation.
      id:
        type: string
        docs: Unique identifier for the Datapoint. Starts with `dp_`.
  DatasetResponse:
    docs: >-
      Base type that all File Responses should inherit from.


      Attributes defined here are common to all File Responses and should be
      overridden

      in the inheriting classes with documentation and appropriate Field
      definitions.
    properties:
      path:
        type: string
        docs: >-
          Path of the Dataset, including the name, which is used as a unique
          identifier.
      id:
        type: string
        docs: Unique identifier for the Dataset. Starts with `ds_`.
      directory_id:
        type: optional<string>
        docs: ID of the directory that the file is in on Humanloop.
      name:
        type: string
        docs: Name of the Dataset, which is used as a unique identifier.
      version_id:
        type: string
        docs: >-
          Unique identifier for the specific Dataset Version. If no query params
          provided, the default deployed Dataset Version is returned. Starts
          with `dsv_`.
      type: optional<literal<"dataset">>
      environments:
        type: optional<list<EnvironmentResponse>>
        docs: The list of environments the Dataset Version is deployed to.
      created_at: datetime
      updated_at: datetime
      created_by:
        type: optional<UserResponse>
        docs: The user who created the Dataset.
      status:
        type: VersionStatus
        docs: The status of the Dataset Version.
      last_used_at: datetime
      commit_message:
        type: optional<string>
        docs: >-
          Message describing the changes made. If provided, a committed version
          of the Dataset is created. Otherwise, an uncommitted version is
          created.
      datapoints_count:
        type: integer
        docs: The number of Datapoints in this Dataset version.
      datapoints:
        type: optional<list<DatapointResponse>>
        docs: >-
          The list of Datapoints in this Dataset version. Only provided if
          explicitly requested.
  EnvironmentResponse:
    properties:
      id: string
      created_at: datetime
      name: string
      tag: EnvironmentTag
  EnvironmentTag:
    enum:
      - default
      - other
    docs: An enumeration.
  EvaluatedVersionResponse:
    discriminated: false
    union:
      - PromptResponse
      - ToolResponse
      - EvaluatorResponse
  EvaluateeRequest:
    properties:
      version_id:
        type: string
        docs: >-
          Unique identifier for the Prompt/Tool Version to include in the
          Evaluation Report. Starts with `pv_` for Prompts and `tv_` for Tools.
      batch_id:
        type: optional<string>
        docs: >-
          Unique identifier for the batch of Logs to include in the Evaluation
          Report.
      orchestrated:
        type: optional<boolean>
        docs: >-
          Whether the Prompt/Tool is orchestrated by Humanloop. Default is
          `True`. If `False`, a log for the Prompt/Tool should be submitted by
          the user via the API.
        default: true
  EvaluateeResponse:
    docs: Version of the Evaluatee being evaluated.
    properties:
      version: EvaluatedVersionResponse
      batch_id:
        type: optional<string>
        docs: >-
          Unique identifier for the batch of Logs to include in the Evaluation
          Report.
      orchestrated:
        type: boolean
        docs: >-
          Whether the Prompt/Tool is orchestrated by Humanloop. Default is
          `True`. If `False`, a log for the Prompt/Tool should be submitted by
          the user via the API.
  EvaluationEvaluatorResponse:
    properties:
      version: EvaluatorResponse
      orchestrated:
        type: boolean
        docs: >-
          Whether the Evaluator is orchestrated by Humanloop. Default is `True`.
          If `False`, a log for the Evaluator should be submitted by the user
          via the API.
  EvaluationReportLogResponse:
    properties:
      evaluated_version:
        type: EvaluatedVersionResponse
        docs: The version of the Prompt, Tool or Evaluator that the Log belongs to.
      datapoint:
        type: DatapointResponse
        docs: The Datapoint used to generate the Log
      log:
        type: optional<SrcExternalAppModelsV5LogsLogResponse>
        docs: The Log that was evaluated by the Evaluator.
      evaluator_logs:
        docs: The Evaluator Logs containing the judgments for the Log.
        type: list<SrcExternalAppModelsV5LogsLogResponse>
  EvaluationResponse:
    properties:
      id:
        type: string
        docs: Unique identifier for the Evaluation. Starts with `evr`.
      dataset:
        type: DatasetResponse
        docs: The Dataset used in the Evaluation.
      evaluatees:
        docs: The Prompt/Tool Versions included in the Evaluation.
        type: list<EvaluateeResponse>
      evaluators:
        docs: The Evaluator Versions used to evaluate.
        type: list<EvaluationEvaluatorResponse>
      status:
        type: EvaluationStatus
        docs: >
          The current status of the Evaluation.


          - `"pending"`: The Evaluation has been created but is not actively
          being worked on by Humanloop.

          - `"running"`: Humanloop is checking for any missing Logs and
          Evaluator Logs, and will generate them where appropriate.

          - `"completed"`: All Logs an Evaluator Logs have been generated.

          - `"cancelled"`: The Evaluation has been cancelled by the user.
          Humanloop will stop generating Logs and Evaluator Logs.
      created_at: datetime
      created_by: optional<UserResponse>
      updated_at: datetime
  EvaluationResultResponseValue:
    discriminated: false
    union:
      - boolean
      - double
  EvaluationResultResponse:
    properties:
      id: string
      evaluator_id: string
      evaluator_version_id: string
      evaluation_id: optional<string>
      log_id: string
      log: optional<SrcExternalAppModelsV4LogLogResponse>
      version_id: optional<string>
      version: optional<unknown>
      value: optional<EvaluationResultResponseValue>
      error: optional<string>
      updated_at: datetime
      created_at: datetime
      llm_evaluator_log: optional<SrcExternalAppModelsV4LogLogResponse>
  EvaluationStats:
    properties:
      overall_stats:
        type: OverallStats
        docs: Stats for the Evaluation Report as a whole.
      version_stats:
        docs: Stats for each Evaluated Version in the Evaluation Report.
        type: list<VersionStats>
  EvaluationStatus:
    enum:
      - pending
      - running
      - completed
      - cancelled
      - failed
    docs: Status of an evaluation.
  EvaluatorActivationDeactivationRequestActivateItem:
    discriminated: false
    union:
      - MonitoringEvaluatorVersionRequest
      - MonitoringEvaluatorEnvironmentRequest
  EvaluatorActivationDeactivationRequestDeactivateItem:
    discriminated: false
    union:
      - MonitoringEvaluatorVersionRequest
      - MonitoringEvaluatorEnvironmentRequest
  EvaluatorActivationDeactivationRequest:
    properties:
      activate:
        type: optional<list<EvaluatorActivationDeactivationRequestActivateItem>>
        docs: >-
          Evaluators to activate for Monitoring. These will be automatically run
          on new Logs.
      deactivate:
        type: optional<list<EvaluatorActivationDeactivationRequestDeactivateItem>>
        docs: Evaluators to deactivate. These will not be run on new Logs.
  EvaluatorAggregate:
    properties:
      value:
        type: double
        docs: The aggregated value of the evaluator.
      evaluator_id:
        type: string
        docs: ID of the evaluator.
      evaluator_version_id:
        type: string
        docs: ID of the evaluator version.
      created_at: datetime
      updated_at: datetime
  EvaluatorArgumentsType:
    enum:
      - target_free
      - target_required
    docs: Enum representing the possible argument types of an evaluator.
  EvaluatorConfigResponse:
    properties:
      id:
        type: string
        docs: String ID of config. Starts with `config_`.
      other:
        type: optional<map<string, unknown>>
        docs: Other parameters that define the config.
      type: literal<"evaluator">
      created_by:
        type: optional<BaseModelsUserResponse>
        docs: The user who created the config.
      status:
        type: string
        docs: Whether the config is committed or not.
      name:
        type: string
        docs: Name of config.
      description:
        type: optional<string>
        docs: Description of config.
      evaluator_type:
        type: string
        docs: Type of evaluator.
      code:
        type: optional<string>
        docs: >-
          The code for the evaluator. This code will be executed in a sandboxed
          environment.
      arguments_type:
        type: optional<EvaluatorArgumentsType>
        docs: Whether this evaluator is target-free or target-required.
      return_type:
        type: optional<EvaluatorReturnTypeEnum>
        docs: The type of the return value of the evaluator.
  EvaluatorLogResponse:
    docs: General request for creating a Log
    properties:
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      provider_request:
        type: optional<map<string, unknown>>
        docs: Raw request sent to provider.
      provider_response:
        type: optional<map<string, unknown>>
        docs: Raw response received the provider.
      session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Session to associate the Log to. Allows you
          to record multiple Logs to a Session (using an ID kept by your
          internal systems) by passing the same `session_id` in subsequent log
          requests.
      parent_id:
        type: string
        docs: >-
          Identifier of the evaluated Log. The newly created Log will have this
          one set as parent.
      inputs:
        type: optional<map<string, unknown>>
        docs: The inputs passed to the prompt template.
      source:
        type: optional<string>
        docs: Identifies where the model was called from.
      metadata:
        type: optional<map<string, unknown>>
        docs: Any additional metadata to record.
      save:
        type: optional<boolean>
        docs: Whether the request/response payloads will be stored on Humanloop.
        default: true
      source_datapoint_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Datapoint that this Log is derived from.
          This can be used by Humanloop to associate Logs to Evaluations. If
          provided, Humanloop will automatically associate this Log to
          Evaluations that require a Log for this Datapoint-Version pair.
      batches:
        type: optional<list<string>>
        docs: >-
          Array of Batch Ids that this log is part of. Batches are used to group
          Logs together for offline Evaluations
      user:
        type: optional<string>
        docs: End-user ID related to the Log.
      environment:
        type: optional<string>
        docs: The name of the Environment the Log is associated to.
      judgment: optional<unknown>
      id:
        type: string
        docs: Unique identifier for the Log.
      evaluator:
        type: EvaluatorResponse
        docs: The Evaluator used to generate the judgment.
  EvaluatorResponseSpec:
    discriminated: false
    union:
      - LlmEvaluatorRequest
      - CodeEvaluatorRequest
      - HumanEvaluatorRequest
      - ExternalEvaluatorRequest
  EvaluatorResponse:
    docs: Version of the Evaluator used to provide judgments.
    properties:
      path:
        type: string
        docs: >-
          Path of the Evaluator including the Evaluator name, which is used as a
          unique identifier.
      id:
        type: string
        docs: Unique identifier for the Evaluator.
      directory_id:
        type: optional<string>
        docs: ID of the directory that the file is in on Humanloop.
      commit_message:
        type: optional<string>
        docs: Message describing the changes made.
      spec: EvaluatorResponseSpec
      name:
        type: string
        docs: Name of the Evaluator, which is used as a unique identifier.
      version_id:
        type: string
        docs: >-
          Unique identifier for the specific Evaluator Version. If no query
          params provided, the default deployed Evaluator Version is returned.
      type: optional<literal<"evaluator">>
      environments:
        type: optional<list<EnvironmentResponse>>
        docs: The list of environments the Prompt Version is deployed to.
      created_at: datetime
      updated_at: datetime
      created_by:
        type: optional<UserResponse>
        docs: The user who created the Prompt.
      status: VersionStatus
      last_used_at: datetime
      version_logs_count:
        type: integer
        docs: The number of logs that have been generated for this Prompt Version
      total_logs_count:
        type: integer
        docs: The number of logs that have been generated across all Prompt Versions
      inputs:
        docs: >-
          Inputs associated to the Prompt. Inputs correspond to any of the
          variables used within the Prompt template.
        type: list<InputResponse>
      evaluators:
        type: optional<list<MonitoringEvaluatorResponse>>
        docs: >-
          Evaluators that have been attached to this Prompt that are used for
          monitoring logs.
      evaluator_aggregates:
        type: optional<list<EvaluatorAggregate>>
        docs: Aggregation of Evaluator results for the Evaluator Version.
  EvaluatorReturnTypeEnum:
    enum:
      - boolean
      - number
    docs: Enum representing the possible return types of an evaluator.
  ExperimentResponse:
    properties:
      id:
        type: string
        docs: String ID of experiment. Starts with `exp_`.
      file_id:
        type: string
        docs: String ID of file the experiment belongs to.
      name:
        type: string
        docs: Name of experiment.
      status:
        type: ExperimentStatus
        docs: Status of experiment.
      versions:
        type: optional<list<ExperimentVersionResponse>>
        docs: List of Versions associated to the experiment.
      metric:
        type: BaseMetricResponse
        docs: Metric used as the experiment's objective.
      positive_labels:
        docs: >-
          Feedback labels to treat as positive user feedback. Used to monitor
          the performance of model configs in the experiment.
        type: list<PositiveLabel>
      created_at: datetime
      updated_at: datetime
  ExperimentStatus:
    enum:
      - Initialized
      - value: In progress
        name: InProgress
    docs: An enumeration.
  ExperimentVersionResponse:
    properties:
      mean:
        type: optional<double>
        docs: The mean performance of the Version.
      spread:
        type: optional<double>
        docs: The spread of performance of the Version.
      trials_count:
        type: integer
        docs: Number of datapoints with feedback associated to the experiment.
      active:
        type: boolean
        docs: >-
          Whether the Version is active in the experiment. Only active model
          configs can be sampled from the experiment.
      id:
        type: string
        docs: String ID of Version.
      commit_message:
        type: optional<string>
        docs: Commit message of Version.
      version_id:
        type: string
        docs: Version of the Prompt or Tool.
      created_at: datetime
      updated_at: datetime
  ExternalEvaluatorRequest:
    properties:
      arguments_type:
        type: EvaluatorArgumentsType
        docs: Whether this evaluator is target-free or target-required.
      return_type:
        type: EvaluatorReturnTypeEnum
        docs: The type of the return value of the evaluator.
      evaluator_type: literal<"external">
      metadata:
        type: optional<map<string, unknown>>
        docs: Metadata describing the external Evaluator.
  FeedbackClass:
    enum:
      - select
      - multi_select
      - text
      - number
    docs: An enumeration.
  FeedbackLabelStatus:
    enum:
      - unset
      - active
      - inactive
    docs: Controls whether the label is displayed in the UI.
  FeedbackResponseType:
    discriminated: false
    docs: >-
      The type of feedback. The default feedback types available are 'rating',
      'action', 'issue', 'correction', and 'comment'.
    union:
      - FeedbackType
      - string
  FeedbackResponseValue:
    discriminated: false
    docs: >-
      The feedback value to set. This would be the appropriate text for
      'correction' or 'comment', or a label to apply for 'rating', 'action', or
      'issue'.
    union:
      - double
      - string
  FeedbackResponse:
    properties:
      type:
        type: FeedbackResponseType
        docs: >-
          The type of feedback. The default feedback types available are
          'rating', 'action', 'issue', 'correction', and 'comment'.
      value:
        type: FeedbackResponseValue
        docs: >-
          The feedback value to set. This would be the appropriate text for
          'correction' or 'comment', or a label to apply for 'rating', 'action',
          or 'issue'.
      data_id:
        type: optional<string>
        docs: ID to associate the feedback to a previously logged datapoint.
      user:
        type: optional<string>
        docs: A unique identifier to who provided the feedback.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the feedback was created. '
      id:
        type: string
        docs: String ID of user feedback. Starts with `ann_`, short for annotation.
  FeedbackTypeModelType:
    discriminated: false
    docs: >-
      The type of feedback. The default feedback types available are 'rating',
      'action', 'issue', 'correction', and 'comment'.
    union:
      - FeedbackType
      - string
  FeedbackTypeModel:
    properties:
      type:
        type: FeedbackTypeModelType
        docs: >-
          The type of feedback. The default feedback types available are
          'rating', 'action', 'issue', 'correction', and 'comment'.
      values:
        type: optional<list<CategoricalFeedbackLabel>>
        docs: >-
          The allowed values for categorical feedback types. Not populated for
          `correction` and `comment`.
  FeedbackTypes: list<FeedbackTypeModel>
  FileEnvironmentResponseFile:
    discriminated: false
    docs: >-
      The version of the File that is deployed to the Environment, if one is
      deployed.
    union:
      - PromptResponse
      - ToolResponse
      - DatasetResponse
      - EvaluatorResponse
  FileEnvironmentResponse:
    docs: >-
      Response model for the List Environments endpoint under Files.


      Contains the deployed version of the File, if one is deployed to the
      Environment.
    properties:
      id: string
      created_at: datetime
      name: string
      tag: EnvironmentTag
      file:
        type: optional<FileEnvironmentResponseFile>
        docs: >-
          The version of the File that is deployed to the Environment, if one is
          deployed.
  FunctionTool:
    docs: A function tool to be called by the model where user owns runtime.
    properties:
      name: string
      arguments: optional<string>
  FunctionToolChoice:
    docs: A function tool to be called by the model where user owns runtime.
    properties:
      name: string
  GenericConfigResponse:
    properties:
      id:
        type: string
        docs: String ID of config. Starts with `config_`.
      other:
        type: optional<map<string, unknown>>
        docs: Other parameters that define the config.
      type: literal<"generic">
      created_by:
        type: optional<BaseModelsUserResponse>
        docs: The user who created the config.
      status:
        type: string
        docs: Whether the config is committed or not.
      name:
        type: string
        docs: Name of config.
      description:
        type: optional<string>
        docs: Description of config.
  HttpValidationError:
    properties:
      detail: optional<list<ValidationError>>
  HumanEvaluatorRequest:
    properties:
      arguments_type:
        type: EvaluatorArgumentsType
        docs: Whether this evaluator is target-free or target-required.
      return_type:
        type: EvaluatorReturnTypeEnum
        docs: The type of the return value of the evaluator.
      evaluator_type: literal<"human">
  ImageChatContent:
    properties:
      type: literal<"image_url">
      image_url:
        type: ImageUrl
        docs: The message's image content.
  ImageUrlDetail:
    enum:
      - high
      - low
      - auto
    docs: >-
      Specify the detail level of the image provided to the model. For more
      details see:
      https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding
  ImageUrl:
    properties:
      url:
        type: string
        docs: Either a URL of the image or the base64 encoded image data.
      detail:
        type: optional<ImageUrlDetail>
        docs: >-
          Specify the detail level of the image provided to the model. For more
          details see:
          https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding
  InputResponse:
    properties:
      name:
        type: string
        docs: Type of input.
  LlmEvaluatorRequest:
    properties:
      arguments_type:
        type: EvaluatorArgumentsType
        docs: Whether this evaluator is target-free or target-required.
      return_type:
        type: EvaluatorReturnTypeEnum
        docs: The type of the return value of the evaluator.
      evaluator_type: literal<"llm">
      prompt:
        type: optional<PromptKernelRequest>
        docs: The prompt parameters used to generate.
  LabelSentiment:
    enum:
      - positive
      - negative
      - neutral
      - unset
    docs: |-
      How a label should be treated in calculating Version performance.

      Used by a File's PAPV (Positive Action Per View) metric.
  LinkedToolRequest:
    properties:
      id:
        type: string
        docs: The ID of the linked tool. Starts with "oc_"
      source:
        type: literal<"organization">
        docs: >-
          The source of the linked tool. For a linked tool it should be
          `organization`
      name:
        type: optional<string>
        docs: The name of the linked tool.
      description:
        type: optional<string>
        docs: The description of the linked tool.
      parameters:
        type: optional<map<string, unknown>>
        docs: The parameters of the linked tool.
  LinkedToolResponse:
    properties:
      name:
        type: string
        docs: Name for the tool referenced by the model.
      description:
        type: string
        docs: Description of the tool referenced by the model
      parameters:
        type: optional<map<string, unknown>>
        docs: >-
          Parameters needed to run the Tool, defined in JSON Schema format:
          https://json-schema.org/
      id:
        type: string
        docs: Unique identifier for the Tool linked.
      version_id:
        type: string
        docs: Unique identifier for the Tool Version linked.
  ListDatasets:
    properties:
      records:
        docs: The list of Datasets.
        type: list<DatasetResponse>
  ListEvaluators:
    properties:
      records:
        docs: The list of Evaluators.
        type: list<EvaluatorResponse>
  ListPrompts:
    properties:
      records:
        docs: The list of Prompts.
        type: list<PromptResponse>
  ListTools:
    properties:
      records:
        docs: The list of Tools.
        type: list<ToolResponse>
  MetricValueResponse:
    properties:
      metric_id: string
      metric_name: string
      metric_value: double
  ModelConfigRequestStop:
    discriminated: false
    docs: >-
      The string (or list of strings) after which the model will stop
      generating. The returned text will not contain the stop sequence.
    union:
      - string
      - list<string>
  ModelConfigRequestToolsItem:
    discriminated: false
    union:
      - LinkedToolRequest
      - ModelConfigToolRequest
  ModelConfigRequest:
    docs: Model config used for logging both chat and completion.
    properties:
      name:
        type: optional<string>
        docs: >-
          A friendly display name for the model config. If not provided, a name
          will be generated.
      description:
        type: optional<string>
        docs: A description of the model config.
      provider:
        type: optional<ModelProviders>
        docs: The company providing the underlying model service.
      model:
        type: string
        docs: The model instance used. E.g. text-davinci-002.
      max_tokens:
        type: optional<integer>
        docs: >-
          The maximum number of tokens to generate. Provide max_tokens=-1 to
          dynamically calculate the maximum number of tokens to generate given
          the length of the prompt
        default: -1
      temperature:
        type: optional<double>
        docs: >-
          What sampling temperature to use when making a generation. Higher
          values means the model will be more creative.
        default: 1
      top_p:
        type: optional<double>
        docs: >-
          An alternative to sampling with temperature, called nucleus sampling,
          where the model considers the results of the tokens with top_p
          probability mass.
        default: 1
      stop:
        type: optional<ModelConfigRequestStop>
        docs: >-
          The string (or list of strings) after which the model will stop
          generating. The returned text will not contain the stop sequence.
      presence_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on whether they appear in the generation so far.
        default: 0
      frequency_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on how frequently they appear in the generation so far.
        default: 0
      other:
        type: optional<map<string, unknown>>
        docs: Other parameter values to be passed to the provider call.
      seed:
        type: optional<integer>
        docs: >-
          If specified, model will make a best effort to sample
          deterministically, but it is not guaranteed.
      response_format:
        type: optional<ResponseFormat>
        docs: >-
          The format of the response. Only type json_object is currently
          supported for chat.
      endpoint:
        type: optional<ModelEndpoints>
        docs: The provider model endpoint used.
      prompt_template:
        type: optional<string>
        docs: >-
          Prompt template that will take your specified inputs to form your
          final request to the model. Input variables within the prompt template
          should be specified with syntax: {{INPUT_NAME}}.
      chat_template:
        type: optional<list<ChatMessageWithToolCall>>
        docs: >-
          Messages prepended to the list of messages sent to the provider. These
          messages that will take your specified inputs to form your final
          request to the provider model. Input variables within the template
          should be specified with syntax: {{INPUT_NAME}}.
      tools:
        type: optional<list<ModelConfigRequestToolsItem>>
        docs: Make tools available to OpenAIs chat model as functions.
      type: optional<literal<"model">>
  ModelConfigResponseStop:
    discriminated: false
    docs: >-
      The string (or list of strings) after which the model will stop
      generating. The returned text will not contain the stop sequence.
    union:
      - string
      - list<string>
  ModelConfigResponse:
    docs: >-
      Model config request.


      Contains fields that are common to all (i.e. both chat and complete)
      endpoints.
    properties:
      id:
        type: string
        docs: String ID of config. Starts with `config_`.
      other:
        type: optional<map<string, unknown>>
        docs: Other parameter values to be passed to the provider call.
      type: literal<"model">
      name:
        type: optional<string>
        docs: >-
          A friendly display name for the model config. If not provided, a name
          will be generated.
      description:
        type: optional<string>
        docs: A description of the model config.
      provider:
        type: optional<ModelProviders>
        docs: The company providing the underlying model service.
      model:
        type: string
        docs: The model instance used. E.g. text-davinci-002.
      max_tokens:
        type: optional<integer>
        docs: >-
          The maximum number of tokens to generate. Provide max_tokens=-1 to
          dynamically calculate the maximum number of tokens to generate given
          the length of the prompt
        default: -1
      temperature:
        type: optional<double>
        docs: >-
          What sampling temperature to use when making a generation. Higher
          values means the model will be more creative.
        default: 1
      top_p:
        type: optional<double>
        docs: >-
          An alternative to sampling with temperature, called nucleus sampling,
          where the model considers the results of the tokens with top_p
          probability mass.
        default: 1
      stop:
        type: optional<ModelConfigResponseStop>
        docs: >-
          The string (or list of strings) after which the model will stop
          generating. The returned text will not contain the stop sequence.
      presence_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on whether they appear in the generation so far.
        default: 0
      frequency_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on how frequently they appear in the generation so far.
        default: 0
      seed:
        type: optional<integer>
        docs: >-
          If specified, model will make a best effort to sample
          deterministically, but it is not guaranteed.
      response_format:
        type: optional<ResponseFormat>
        docs: >-
          The format of the response. Only type json_object is currently
          supported for chat.
      prompt_template:
        type: optional<string>
        docs: >-
          Prompt template that will take your specified inputs to form your
          final request to the model. NB: Input variables within the prompt
          template should be specified with syntax: {{INPUT_NAME}}.
      chat_template:
        type: optional<list<ChatMessageWithToolCall>>
        docs: >-
          Messages prepended to the list of messages sent to the provider. These
          messages that will take your specified inputs to form your final
          request to the provider model. NB: Input variables within the template
          should be specified with syntax: {{INPUT_NAME}}.
      tool_configs:
        type: optional<list<ToolConfigResponse>>
        docs: >-
          NB: Deprecated with tools field. Definition of tools shown to the
          model.
        availability: deprecated
      tools:
        type: optional<list<ConfigToolResponse>>
        docs: Tools shown to the model.
      endpoint:
        type: optional<ModelEndpoints>
        docs: The provider model endpoint used.
  ModelConfigToolRequest:
    docs: |-
      Definition of tool within a model config.

      The subset of ToolConfig parameters received by the chat endpoint.
      Does not have things like the signature or setup schema.
    properties:
      name:
        type: string
        docs: The name of the tool shown to the model.
      description:
        type: optional<string>
        docs: The description of the tool shown to the model.
      parameters:
        type: optional<map<string, unknown>>
        docs: >-
          Definition of parameters needed to run the tool. Provided in
          jsonschema format: https://json-schema.org/
      source:
        type: optional<ToolSource>
        docs: >-
          Source of the tool. If defined at an organization level will be
          'organization' else 'inline'.
      source_code:
        type: optional<string>
        docs: Code source of the tool.
      other:
        type: optional<map<string, unknown>>
        docs: Other parameters that define the config.
      preset_name:
        type: optional<string>
        docs: >-
          If is_preset = true, this is the name of the preset tool on Humanloop.
          This is used as the key to look up the Humanloop runtime of the tool
  ModelEndpoints:
    enum:
      - complete
      - chat
      - edit
    docs: Supported model provider endpoints.
  ModelProviders:
    enum:
      - openai
      - openai_azure
      - mock
      - anthropic
      - cohere
      - replicate
      - google
      - groq
    docs: Supported model providers.
  MonitoringEvaluatorEnvironmentRequest:
    properties:
      evaluator_id:
        type: string
        docs: Unique identifier for the Evaluator to be used for monitoring.
      environment_id:
        type: string
        docs: >-
          Unique identifier for the Environment. The Evaluator Version deployed
          to this Environment will be used for monitoring.
  MonitoringEvaluatorResponse:
    properties:
      version_reference:
        type: VersionReferenceResponse
        docs: >-
          The Evaluator Version used for monitoring. This can be a specific
          Version by ID, or a Version deployed to an Environment.
      version:
        type: optional<EvaluatorResponse>
        docs: The deployed Version.
      state:
        type: MonitoringEvaluatorState
        docs: The state of the Monitoring Evaluator. Either `active` or `inactive`
      created_at: datetime
      updated_at: datetime
  MonitoringEvaluatorState:
    enum:
      - active
      - inactive
    docs: State of an evaluator connected to a file
  MonitoringEvaluatorVersionRequest:
    properties:
      evaluator_version_id:
        type: string
        docs: Unique identifier for the Evaluator Version to be used for monitoring.
  NumericEvaluatorVersionStats:
    docs: |-
      Base attributes for stats for an Evaluator Version-Evaluated Version pair
      in the Evaluation Report.
    properties:
      evaluator_version_id:
        type: string
        docs: Unique identifier for the Evaluator Version.
      total_logs:
        type: integer
        docs: >-
          The total number of Logs generated by this Evaluator Version on the
          Evaluated Version's Logs. This includes Nulls and Errors.
      num_judgments:
        type: integer
        docs: >-
          The total number of Evaluator judgments for this Evaluator Version.
          This excludes Nulls and Errors.
      num_nulls:
        type: integer
        docs: >-
          The total number of null judgments (i.e. abstentions) for this
          Evaluator Version.
      num_errors:
        type: integer
        docs: The total number of errored Evaluators for this Evaluator Version.
      mean: optional<double>
      std: optional<double>
      percentiles: map<string, double>
  ObservabilityStatus:
    enum:
      - pending
      - running
      - completed
      - failed
    docs: |-
      Status of a Log for observability.

      Observability is implemented by running monitoring Evaluators on Logs.
  OverallStats:
    properties:
      num_datapoints:
        type: integer
        docs: >-
          The total number of Datapoints in the Evaluation Report's Dataset
          Version.
      total_logs:
        type: integer
        docs: The total number of Logs in the Evaluation Report.
      total_evaluator_logs:
        type: integer
        docs: The total number of Evaluator Logs in the Evaluation Report.
  PaginatedDatapointResponse:
    properties:
      records: list<DatapointResponse>
      page: integer
      size: integer
      total: integer
  PaginatedDatasetResponse:
    properties:
      records: list<DatasetResponse>
      page: integer
      size: integer
      total: integer
  PaginatedDataEvaluationReportLogResponse:
    properties:
      records: list<EvaluationReportLogResponse>
      page: integer
      size: integer
      total: integer
  PaginatedEvaluationResponse:
    properties:
      records: list<EvaluationResponse>
      page: integer
      size: integer
      total: integer
  PaginatedDataEvaluatorResponse:
    properties:
      records: list<EvaluatorResponse>
      page: integer
      size: integer
      total: integer
  PaginatedDataLogResponse:
    properties:
      records: list<SrcExternalAppModelsV5LogsLogResponse>
      page: integer
      size: integer
      total: integer
  PaginatedDataPromptResponse:
    properties:
      records: list<PromptResponse>
      page: integer
      size: integer
      total: integer
  PaginatedSessionResponse:
    properties:
      records: list<SessionResponse>
      page: integer
      size: integer
      total: integer
  PaginatedDataToolResponse:
    properties:
      records: list<ToolResponse>
      page: integer
      size: integer
      total: integer
  PlatformAccessEnum:
    enum:
      - superadmin
      - supportadmin
      - user
    docs: An enumeration.
  PositiveLabel:
    properties:
      type: string
      value: string
  ProjectSortBy:
    enum:
      - created_at
      - updated_at
      - name
    docs: An enumeration.
  PromptCallLogResponse:
    docs: Sample specific response details for a Prompt call
    properties:
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      output_message:
        type: optional<ChatMessage>
        docs: The message returned by the provider.
      prompt_tokens:
        type: optional<integer>
        docs: Number of tokens in the prompt used to generate the output.
      output_tokens:
        type: optional<integer>
        docs: Number of tokens in the output generated by the model.
      prompt_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the prompt.
      output_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the output.
      finish_reason:
        type: optional<string>
        docs: Reason the generation finished.
      index:
        type: integer
        docs: The index of the sample in the batch.
  PromptCallResponseToolChoice:
    discriminated: false
    docs: >-
      Controls how the model uses tools. The following options are supported: 

      - `'none'` means the model will not call any tool and instead generates a
      message; this is the default when no tools are provided as part of the
      Prompt. 

      - `'auto'` means the model can decide to call one or more of the provided
      tools; this is the default when tools are provided as part of the Prompt. 

      - `'required'` means the model can decide to call one or more of the
      provided tools. 

      - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
      model to use the named function.
    union:
      - literal<"none">
      - literal<"auto">
      - literal<"required">
      - ToolChoice
  PromptCallResponse:
    docs: Response model for a Prompt call with potentially multiple log samples.
    properties:
      prompt:
        type: PromptResponse
        docs: Prompt details used to generate the log.
      messages:
        type: optional<list<ChatMessage>>
        docs: The messages passed to the to provider chat endpoint.
      tool_choice:
        type: optional<PromptCallResponseToolChoice>
        docs: >-
          Controls how the model uses tools. The following options are
          supported: 

          - `'none'` means the model will not call any tool and instead
          generates a message; this is the default when no tools are provided as
          part of the Prompt. 

          - `'auto'` means the model can decide to call one or more of the
          provided tools; this is the default when tools are provided as part of
          the Prompt. 

          - `'required'` means the model can decide to call one or more of the
          provided tools. 

          - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
          model to use the named function.
      session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Session to associate the Log to. Allows you
          to record multiple Logs to a Session (using an ID kept by your
          internal systems) by passing the same `session_id` in subsequent log
          requests.
      parent_id:
        type: optional<string>
        docs: >-
          Unique identifier for the parent Log in a Session. Should only be
          provided if `session_id` is provided. If provided, the Log will be
          nested under the parent Log within the Session.
      inputs:
        type: optional<map<string, unknown>>
        docs: The inputs passed to the prompt template.
      source:
        type: optional<string>
        docs: Identifies where the model was called from.
      metadata:
        type: optional<map<string, unknown>>
        docs: Any additional metadata to record.
      save:
        type: optional<boolean>
        docs: Whether the request/response payloads will be stored on Humanloop.
        default: true
      source_datapoint_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Datapoint that this Log is derived from.
          This can be used by Humanloop to associate Logs to Evaluations. If
          provided, Humanloop will automatically associate this Log to
          Evaluations that require a Log for this Datapoint-Version pair.
      batches:
        type: optional<list<string>>
        docs: >-
          Array of Batch Ids that this log is part of. Batches are used to group
          Logs together for offline Evaluations
      user:
        type: optional<string>
        docs: End-user ID related to the Log.
      environment:
        type: optional<string>
        docs: The name of the Environment the Log is associated to.
      id:
        type: string
        docs: ID of the log.
      logs:
        docs: The logs generated by the Prompt call.
        type: list<PromptCallLogResponse>
  PromptCallStreamResponse:
    docs: Response model for calling Prompt in streaming mode.
    properties:
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      output_message:
        type: optional<ChatMessage>
        docs: The message returned by the provider.
      prompt_tokens:
        type: optional<integer>
        docs: Number of tokens in the prompt used to generate the output.
      output_tokens:
        type: optional<integer>
        docs: Number of tokens in the output generated by the model.
      prompt_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the prompt.
      output_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the output.
      finish_reason:
        type: optional<string>
        docs: Reason the generation finished.
      index:
        type: integer
        docs: The index of the sample in the batch.
      id:
        type: string
        docs: ID of the log.
      prompt_id:
        type: string
        docs: ID of the Prompt the log belongs to.
      version_id:
        type: string
        docs: ID of the specific version of the Prompt.
  PromptKernelRequestTemplate:
    discriminated: false
    docs: >-
      For chat endpoint, provide a Chat template. For completion endpoint,
      provide a Prompt template. Input variables within the template should be
      specified with double curly bracket syntax: {{INPUT_NAME}}.
    union:
      - string
      - list<ChatMessage>
  PromptKernelRequestStop:
    discriminated: false
    docs: >-
      The string (or list of strings) after which the model will stop
      generating. The returned text will not contain the stop sequence.
    union:
      - string
      - list<string>
  PromptKernelRequest:
    properties:
      model:
        type: string
        docs: >-
          The model instance used, e.g. `gpt-4`. See [supported
          models](https://humanloop.com/docs/supported-models)
      endpoint:
        type: optional<ModelEndpoints>
        docs: The provider model endpoint used.
      template:
        type: optional<PromptKernelRequestTemplate>
        docs: >-
          For chat endpoint, provide a Chat template. For completion endpoint,
          provide a Prompt template. Input variables within the template should
          be specified with double curly bracket syntax: {{INPUT_NAME}}.
      provider:
        type: optional<ModelProviders>
        docs: The company providing the underlying model service.
      max_tokens:
        type: optional<integer>
        docs: >-
          The maximum number of tokens to generate. Provide max_tokens=-1 to
          dynamically calculate the maximum number of tokens to generate given
          the length of the prompt
        default: -1
      temperature:
        type: optional<double>
        docs: >-
          What sampling temperature to use when making a generation. Higher
          values means the model will be more creative.
        default: 1
      top_p:
        type: optional<double>
        docs: >-
          An alternative to sampling with temperature, called nucleus sampling,
          where the model considers the results of the tokens with top_p
          probability mass.
        default: 1
      stop:
        type: optional<PromptKernelRequestStop>
        docs: >-
          The string (or list of strings) after which the model will stop
          generating. The returned text will not contain the stop sequence.
      presence_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on whether they appear in the generation so far.
        default: 0
      frequency_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on how frequently they appear in the generation so far.
        default: 0
      other:
        type: optional<map<string, unknown>>
        docs: Other parameter values to be passed to the provider call.
      seed:
        type: optional<integer>
        docs: >-
          If specified, model will make a best effort to sample
          deterministically, but it is not guaranteed.
      response_format:
        type: optional<ResponseFormat>
        docs: >-
          The format of the response. Only `{"type": "json_object"}` is
          currently supported for chat.
      tools:
        type: optional<list<ToolFunction>>
        docs: >-
          The tool specification that the model can choose to call if Tool
          calling is supported.
      linked_tools:
        type: optional<list<string>>
        docs: >-
          The IDs of the Tools in your organization that the model can choose to
          call if Tool calling is supported. The default deployed version of
          that tool is called.
  PromptLogResponseToolChoice:
    discriminated: false
    docs: >-
      Controls how the model uses tools. The following options are supported: 

      - `'none'` means the model will not call any tool and instead generates a
      message; this is the default when no tools are provided as part of the
      Prompt. 

      - `'auto'` means the model can decide to call one or more of the provided
      tools; this is the default when tools are provided as part of the Prompt. 

      - `'required'` means the model can decide to call one or more of the
      provided tools. 

      - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
      model to use the named function.
    union:
      - literal<"none">
      - literal<"auto">
      - literal<"required">
      - ToolChoice
  PromptLogResponse:
    docs: General request for creating a Log
    properties:
      output_message:
        type: optional<ChatMessage>
        docs: The message returned by the provider.
      prompt_tokens:
        type: optional<integer>
        docs: Number of tokens in the prompt used to generate the output.
      output_tokens:
        type: optional<integer>
        docs: Number of tokens in the output generated by the model.
      prompt_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the prompt.
      output_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the output.
      finish_reason:
        type: optional<string>
        docs: Reason the generation finished.
      prompt:
        type: PromptResponse
        docs: Prompt details used to generate the Log.
      messages:
        type: optional<list<ChatMessage>>
        docs: The messages passed to the to provider chat endpoint.
      tool_choice:
        type: optional<PromptLogResponseToolChoice>
        docs: >-
          Controls how the model uses tools. The following options are
          supported: 

          - `'none'` means the model will not call any tool and instead
          generates a message; this is the default when no tools are provided as
          part of the Prompt. 

          - `'auto'` means the model can decide to call one or more of the
          provided tools; this is the default when tools are provided as part of
          the Prompt. 

          - `'required'` means the model can decide to call one or more of the
          provided tools. 

          - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the
          model to use the named function.
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      provider_request:
        type: optional<map<string, unknown>>
        docs: Raw request sent to provider.
      provider_response:
        type: optional<map<string, unknown>>
        docs: Raw response received the provider.
      session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Session to associate the Log to. Allows you
          to record multiple Logs to a Session (using an ID kept by your
          internal systems) by passing the same `session_id` in subsequent log
          requests.
      parent_id:
        type: optional<string>
        docs: >-
          Unique identifier for the parent Log in a Session. Should only be
          provided if `session_id` is provided. If provided, the Log will be
          nested under the parent Log within the Session.
      inputs:
        type: optional<map<string, unknown>>
        docs: The inputs passed to the prompt template.
      source:
        type: optional<string>
        docs: Identifies where the model was called from.
      metadata:
        type: optional<map<string, unknown>>
        docs: Any additional metadata to record.
      save:
        type: optional<boolean>
        docs: Whether the request/response payloads will be stored on Humanloop.
        default: true
      source_datapoint_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Datapoint that this Log is derived from.
          This can be used by Humanloop to associate Logs to Evaluations. If
          provided, Humanloop will automatically associate this Log to
          Evaluations that require a Log for this Datapoint-Version pair.
      batches:
        type: optional<list<string>>
        docs: >-
          Array of Batch Ids that this log is part of. Batches are used to group
          Logs together for offline Evaluations
      user:
        type: optional<string>
        docs: End-user ID related to the Log.
      environment:
        type: optional<string>
        docs: The name of the Environment the Log is associated to.
      id:
        type: string
        docs: Unique identifier for the Log.
  PromptResponseTemplate:
    discriminated: false
    docs: >-
      For chat endpoint, provide a Chat template. For completion endpoint,
      provide a Prompt template. Input variables within the template should be
      specified with double curly bracket syntax: {{INPUT_NAME}}.
    union:
      - string
      - list<ChatMessage>
  PromptResponseStop:
    discriminated: false
    docs: >-
      The string (or list of strings) after which the model will stop
      generating. The returned text will not contain the stop sequence.
    union:
      - string
      - list<string>
  PromptResponse:
    docs: >-
      Base type that all File Responses should inherit from.


      Attributes defined here are common to all File Responses and should be
      overridden

      in the inheriting classes with documentation and appropriate Field
      definitions.
    properties:
      path:
        type: string
        docs: >-
          Path of the Prompt, including the name, which is used as a unique
          identifier.
      id:
        type: string
        docs: Unique identifier for the Prompt.
      directory_id:
        type: optional<string>
        docs: ID of the directory that the file is in on Humanloop.
      model:
        type: string
        docs: >-
          The model instance used, e.g. `gpt-4`. See [supported
          models](https://humanloop.com/docs/supported-models)
      endpoint:
        type: optional<ModelEndpoints>
        docs: The provider model endpoint used.
      template:
        type: optional<PromptResponseTemplate>
        docs: >-
          For chat endpoint, provide a Chat template. For completion endpoint,
          provide a Prompt template. Input variables within the template should
          be specified with double curly bracket syntax: {{INPUT_NAME}}.
      provider:
        type: optional<ModelProviders>
        docs: The company providing the underlying model service.
      max_tokens:
        type: optional<integer>
        docs: >-
          The maximum number of tokens to generate. Provide max_tokens=-1 to
          dynamically calculate the maximum number of tokens to generate given
          the length of the prompt
        default: -1
      temperature:
        type: optional<double>
        docs: >-
          What sampling temperature to use when making a generation. Higher
          values means the model will be more creative.
        default: 1
      top_p:
        type: optional<double>
        docs: >-
          An alternative to sampling with temperature, called nucleus sampling,
          where the model considers the results of the tokens with top_p
          probability mass.
        default: 1
      stop:
        type: optional<PromptResponseStop>
        docs: >-
          The string (or list of strings) after which the model will stop
          generating. The returned text will not contain the stop sequence.
      presence_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on whether they appear in the generation so far.
        default: 0
      frequency_penalty:
        type: optional<double>
        docs: >-
          Number between -2.0 and 2.0. Positive values penalize new tokens based
          on how frequently they appear in the generation so far.
        default: 0
      other:
        type: optional<map<string, unknown>>
        docs: Other parameter values to be passed to the provider call.
      seed:
        type: optional<integer>
        docs: >-
          If specified, model will make a best effort to sample
          deterministically, but it is not guaranteed.
      response_format:
        type: optional<ResponseFormat>
        docs: >-
          The format of the response. Only `{"type": "json_object"}` is
          currently supported for chat.
      tools:
        type: optional<list<ToolFunction>>
        docs: >-
          The tool specification that the model can choose to call if Tool
          calling is supported.
      linked_tools:
        type: optional<list<LinkedToolResponse>>
        docs: The tools linked to your prompt that the model can call.
      commit_message:
        type: optional<string>
        docs: Message describing the changes made.
      name:
        type: string
        docs: Name of the Prompt.
      version_id:
        type: string
        docs: >-
          Unique identifier for the specific Prompt Version. If no query params
          provided, the default deployed Prompt Version is returned.
      type: optional<literal<"prompt">>
      environments:
        type: optional<list<EnvironmentResponse>>
        docs: The list of environments the Prompt Version is deployed to.
      created_at: datetime
      updated_at: datetime
      created_by:
        type: optional<UserResponse>
        docs: The user who created the Prompt.
      status:
        type: VersionStatus
        docs: The status of the Prompt Version.
      last_used_at: datetime
      version_logs_count:
        type: integer
        docs: The number of logs that have been generated for this Prompt Version
      total_logs_count:
        type: integer
        docs: The number of logs that have been generated across all Prompt Versions
      inputs:
        docs: >-
          Inputs associated to the Prompt. Inputs correspond to any of the
          variables used within the Prompt template.
        type: list<InputResponse>
      evaluators:
        type: optional<list<MonitoringEvaluatorResponse>>
        docs: >-
          Evaluators that have been attached to this Prompt that are used for
          monitoring logs.
      evaluator_aggregates:
        type: optional<list<EvaluatorAggregate>>
        docs: Aggregation of Evaluator results for the Prompt Version.
  ProviderApiKeys:
    properties:
      openai: optional<string>
      ai21: optional<string>
      mock: optional<string>
      anthropic: optional<string>
      cohere: optional<string>
      openai_azure: optional<string>
      openai_azure_endpoint: optional<string>
  ResponseFormat:
    docs: Response format of the model.
    properties:
      type: literal<"json_object">
  SessionResponse:
    properties:
      id:
        type: string
        docs: Unique identifier for the Session.
      created_at: datetime
      updated_at: datetime
      logs:
        docs: List of Logs associated with this Session.
        type: list<SrcExternalAppModelsV5LogsLogResponse>
  SortOrder:
    enum:
      - asc
      - desc
    docs: An enumeration.
  TextChatContent:
    properties:
      type: literal<"text">
      text:
        type: string
        docs: The message's text content.
  TimeUnit:
    enum:
      - day
      - week
      - month
    docs: An enumeration.
  ToolCall:
    docs: A tool call to be made.
    properties:
      id: string
      type: ChatToolType
      function: FunctionTool
  ToolChoice:
    docs: Tool choice to force the model to use a tool.
    properties:
      type: ChatToolType
      function: FunctionToolChoice
  ToolConfigRequest:
    docs: |-
      Definition of tool within a model config.

      The subset of ToolConfig parameters received by the chat endpoint.
      Does not have things like the signature or setup schema.
    properties:
      name:
        type: string
        docs: The name of the tool shown to the model.
      description:
        type: optional<string>
        docs: The description of the tool shown to the model.
      parameters:
        type: optional<map<string, unknown>>
        docs: >-
          Definition of parameters needed to run the tool. Provided in
          jsonschema format: https://json-schema.org/
      source:
        type: optional<ToolSource>
        docs: >-
          Source of the tool. If defined at an organization level will be
          'organization' else 'inline'.
      source_code:
        type: optional<string>
        docs: Code source of the tool.
      other:
        type: optional<map<string, unknown>>
        docs: Other parameters that define the config.
      preset_name:
        type: optional<string>
        docs: >-
          If is_preset = true, this is the name of the preset tool on Humanloop.
          This is used as the key to look up the Humanloop runtime of the tool
      type: literal<"tool">
  ToolConfigResponse:
    properties:
      id:
        type: string
        docs: String ID of config. Starts with `config_`.
      other:
        type: optional<map<string, unknown>>
        docs: Other parameters that define the config.
      type: literal<"tool">
      created_by:
        type: optional<BaseModelsUserResponse>
        docs: The user who created the config.
      status:
        type: string
        docs: Whether the config is committed or not.
      name:
        type: string
        docs: Name for the tool referenced by the model.
      description:
        type: optional<string>
        docs: Description of the tool referenced by the model
      source:
        type: optional<ToolSource>
        docs: >-
          Source of the tool. If defined at an organization level will be
          'organization' else 'inline'.
      source_code:
        type: optional<string>
        docs: Code source of the tool.
      setup_schema:
        type: optional<map<string, unknown>>
        docs: >-
          Definition of parameters needed to run the tool. Provided in
          jsonschema format: https://json-schema.org/
      parameters:
        type: optional<map<string, unknown>>
        docs: >-
          Definition of parameters needed to run the tool. Provided in
          jsonschema format: https://json-schema.org/
      signature:
        type: optional<string>
        docs: The function signature of the tool when being called.
      is_preset:
        type: optional<boolean>
        docs: Whether the tool is one where Humanloop defines runtime or not.
      preset_name:
        type: optional<string>
        docs: >-
          If is_preset = true, this is the name of the preset tool on Humanloop.
          This is used as the key to lookup the Humanloop runtime of the tool
  ToolFunction:
    properties:
      name:
        type: string
        docs: Name for the tool referenced by the model.
      description:
        type: string
        docs: Description of the tool referenced by the model
      parameters:
        type: optional<map<string, unknown>>
        docs: >-
          Parameters needed to run the Tool, defined in JSON Schema format:
          https://json-schema.org/
  ToolKernelRequest:
    properties:
      function:
        type: optional<ToolFunction>
        docs: >-
          Callable function specification of the Tool shown to the model for
          tool calling.
      source_code:
        type: optional<string>
        docs: Code source of the Tool.
      setup_values:
        type: optional<map<string, unknown>>
        docs: >-
          Values needed to setup the Tool, defined in JSON Schema format:
          https://json-schema.org/
  ToolLogResponse:
    docs: General request for creating a Log
    properties:
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if creating a parent Log with the
          intention to populate it later.
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      provider_latency:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      provider_request:
        type: optional<map<string, unknown>>
        docs: Raw request sent to provider.
      provider_response:
        type: optional<map<string, unknown>>
        docs: Raw response received the provider.
      session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Session to associate the Log to. Allows you
          to record multiple Logs to a Session (using an ID kept by your
          internal systems) by passing the same `session_id` in subsequent log
          requests.
      parent_id:
        type: optional<string>
        docs: >-
          Unique identifier for the parent Log in a Session. Should only be
          provided if `session_id` is provided. If provided, the Log will be
          nested under the parent Log within the Session.
      inputs:
        type: optional<map<string, unknown>>
        docs: The inputs passed to the prompt template.
      source:
        type: optional<string>
        docs: Identifies where the model was called from.
      metadata:
        type: optional<map<string, unknown>>
        docs: Any additional metadata to record.
      save:
        type: optional<boolean>
        docs: Whether the request/response payloads will be stored on Humanloop.
        default: true
      source_datapoint_id:
        type: optional<string>
        docs: >-
          Unique identifier for the Datapoint that this Log is derived from.
          This can be used by Humanloop to associate Logs to Evaluations. If
          provided, Humanloop will automatically associate this Log to
          Evaluations that require a Log for this Datapoint-Version pair.
      batches:
        type: optional<list<string>>
        docs: >-
          Array of Batch Ids that this log is part of. Batches are used to group
          Logs together for offline Evaluations
      user:
        type: optional<string>
        docs: End-user ID related to the Log.
      environment:
        type: optional<string>
        docs: The name of the Environment the Log is associated to.
      id:
        type: string
        docs: Unique identifier for the Log.
      tool:
        type: ToolResponse
        docs: Tool details used to generate the Log.
  ToolResultResponse:
    docs: A result from a tool used to populate the prompt template
    properties:
      id: string
      name: string
      signature: string
      result: string
  ToolSource:
    enum:
      - organization
      - inline
    docs: >-
      Source of tool. Used to differentiate between tools and tool versions when
      they are combined in a list.


      V4 uses organization and inline. Those are deprecated and will be removed
      in favour of tool and tool_version.
  UpdateDatesetAction:
    enum:
      - set
      - add
      - remove
    docs: An enumeration.
  ValidationErrorLocItem:
    discriminated: false
    union:
      - string
      - integer
  ValidationError:
    properties:
      loc: list<ValidationErrorLocItem>
      msg: string
      type: string
  VersionDeploymentResponseFile:
    discriminated: false
    docs: The File that the deployed Version belongs to.
    union:
      - PromptResponse
      - ToolResponse
      - DatasetResponse
      - EvaluatorResponse
  VersionDeploymentResponse:
    docs: A variable reference to the Version deployed to an Environment
    properties:
      file:
        type: VersionDeploymentResponseFile
        docs: The File that the deployed Version belongs to.
      environment:
        type: EnvironmentResponse
        docs: The Environment that the Version is deployed to.
      type: literal<"environment">
  VersionIdResponseVersion:
    discriminated: false
    docs: The specific Version being referenced.
    union:
      - PromptResponse
      - ToolResponse
      - DatasetResponse
      - EvaluatorResponse
  VersionIdResponse:
    docs: A reference to a specific Version by its ID
    properties:
      version:
        type: VersionIdResponseVersion
        docs: The specific Version being referenced.
      type: literal<"version">
  VersionReferenceResponse:
    discriminated: false
    union:
      - VersionDeploymentResponse
      - VersionIdResponse
  VersionStatsEvaluatorVersionStatsItem:
    discriminated: false
    union:
      - NumericEvaluatorVersionStats
      - BooleanEvaluatorVersionStats
  VersionStats:
    docs: Stats for an Evaluated Version in the Evaluation Report.
    properties:
      version_id:
        type: string
        docs: Unique identifier for the Evaluated Version.
      num_logs:
        type: integer
        docs: >-
          The total number of existing Logs for this Evaluated Version within
          the Evaluation Report. These are Logs that have been generated by this
          Evaluated Version on a Datapoint belonging to the Evaluation Report's
          Dataset Version.
      evaluator_version_stats:
        docs: >-
          Stats for each Evaluator Version used to evaluate this Evaluated
          Version.
        type: list<VersionStatsEvaluatorVersionStatsItem>
  VersionStatus:
    enum:
      - uncommitted
      - committed
      - deleted
    docs: An enumeration.
  ChatToolType:
    type: literal<"function">
    docs: The type of tool to call.
  FilesToolType:
    enum:
      - pinecone_search
      - google
      - mock
      - snippet
      - json_schema
      - get_api_call
    docs: Type of tool.
  ConfigToolResponse:
    properties:
      id:
        type: string
        docs: The ID of the tool. Starts with either `config_` or `oc_`.
      name:
        type: string
        docs: Name for the tool referenced by the model.
      description:
        type: optional<string>
        docs: Description of the tool referenced by the model
      parameters:
        type: optional<map<string, unknown>>
        docs: >-
          Definition of parameters needed to run the tool. Provided in
          jsonschema format: https://json-schema.org/
      source:
        type: optional<string>
        docs: The origin of the tool
  FeedbackType:
    enum:
      - rating
      - action
      - issue
      - correction
      - comment
    docs: An enumeration.
  SrcExternalAppModelsV4LogLogResponseJudgment:
    discriminated: false
    union:
      - boolean
      - double
  SrcExternalAppModelsV4LogLogResponseToolChoice:
    discriminated: false
    docs: >-
      Controls how the model uses tools. The following options are supported:
      'none' forces the model to not call a tool; the default when no tools are
      provided as part of the model config. 'auto' the model can decide to call
      one of the provided tools; the default when tools are provided as part of
      the model config. Providing {'type': 'function', 'function': {name':
      <TOOL_NAME>}} forces the model to use the named function.
    union:
      - literal<"none">
      - literal<"auto">
      - literal<"required">
      - ToolChoice
  SrcExternalAppModelsV4LogLogResponse:
    docs: Request model for logging a datapoint.
    properties:
      project:
        type: optional<string>
        docs: The name of the project associated with this log
      project_id:
        type: optional<string>
        docs: The unique ID of the project associated with this log.
      session_id:
        type: optional<string>
        docs: ID of the session to associate the datapoint.
      session_reference_id:
        type: optional<string>
        docs: >-
          A unique string identifying the session to associate the datapoint to.
          Allows you to log multiple datapoints to a session (using an ID kept
          by your internal systems) by passing the same `session_reference_id`
          in subsequent log requests. Specify at most one of this or
          `session_id`.
      parent_id:
        type: optional<string>
        docs: ID associated to the parent datapoint in a session.
      parent_reference_id:
        type: optional<string>
        docs: >-
          A unique string identifying the previously-logged parent datapoint in
          a session. Allows you to log nested datapoints with your internal
          system IDs by passing the same reference ID as `parent_id` in a prior
          log request. Specify at most one of this or `parent_id`. Note that
          this cannot refer to a datapoint being logged in the same request.
      inputs:
        type: optional<map<string, unknown>>
        docs: The inputs passed to the prompt template.
      source:
        type: optional<string>
        docs: Identifies where the model was called from.
      metadata:
        type: optional<map<string, unknown>>
        docs: Any additional metadata to record.
      save:
        type: optional<boolean>
        docs: Whether the request/response payloads will be stored on Humanloop.
        default: true
      source_datapoint_id:
        type: optional<string>
        docs: >-
          ID of the source datapoint if this is a log derived from a datapoint
          in a dataset.
      id:
        type: string
        docs: String ID of logged datapoint. Starts with `data_`.
      reference_id:
        type: optional<string>
        docs: Unique user-provided string identifying the datapoint.
      trial_id:
        type: optional<string>
        docs: Unique ID of an experiment trial to associate to the log.
      messages:
        type: optional<list<ChatMessageWithToolCall>>
        docs: The messages passed to the to provider chat endpoint.
      output:
        type: optional<string>
        docs: >-
          Generated output from your model for the provided inputs. Can be
          `None` if logging an error, or if logging a parent datapoint with the
          intention to populate it later
      judgment: optional<SrcExternalAppModelsV4LogLogResponseJudgment>
      config_id:
        type: optional<string>
        docs: Unique ID of a config to associate to the log.
      config: ConfigResponse
      environment:
        type: optional<string>
        docs: The environment name used to create the log.
      feedback: optional<list<FeedbackResponse>>
      created_at:
        type: optional<datetime>
        docs: 'User defined timestamp for when the log was created. '
      error:
        type: optional<string>
        docs: Error message if the log is an error.
      duration:
        type: optional<double>
        docs: Duration of the logged event in seconds.
      output_message:
        type: optional<ChatMessageWithToolCall>
        docs: The message returned by the provider.
      prompt_tokens:
        type: optional<integer>
        docs: Number of tokens in the prompt used to generate the output.
      output_tokens:
        type: optional<integer>
        docs: Number of tokens in the output generated by the model.
      prompt_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the prompt.
      output_cost:
        type: optional<double>
        docs: Cost in dollars associated to the tokens in the output.
      provider_request:
        type: optional<map<string, unknown>>
        docs: Raw request sent to provider.
      provider_response:
        type: optional<map<string, unknown>>
        docs: Raw response received the provider.
      user:
        type: optional<string>
        docs: User email address provided when creating the datapoint.
      provider_latency:
        type: optional<double>
        docs: Latency of provider response.
      tokens:
        type: optional<integer>
        docs: Total number of tokens in the prompt and output.
      raw_output:
        type: optional<string>
        docs: Raw output from the provider.
      finish_reason:
        type: optional<string>
        docs: Reason the generation finished.
      metric_values: optional<list<MetricValueResponse>>
      tools: optional<list<ToolResultResponse>>
      tool_choice:
        type: optional<SrcExternalAppModelsV4LogLogResponseToolChoice>
        docs: >-
          Controls how the model uses tools. The following options are
          supported: 'none' forces the model to not call a tool; the default
          when no tools are provided as part of the model config. 'auto' the
          model can decide to call one of the provided tools; the default when
          tools are provided as part of the model config. Providing {'type':
          'function', 'function': {name': <TOOL_NAME>}} forces the model to use
          the named function.
      evaluation_results: list<EvaluationResultResponse>
      observability_status: ObservabilityStatus
      updated_at: datetime
      batch_ids:
        type: optional<list<string>>
        docs: List of batch IDs the log belongs to.
  BaseModelsUserResponse:
    properties:
      id:
        type: string
        docs: String ID of user. Starts with `usr_`.
      email_address:
        type: string
        docs: The user's email address.
      full_name:
        type: optional<string>
        docs: The user's full name.
      verified:
        type: boolean
        docs: Whether the user has verified their email address.
  EvaluationsDatasetRequest:
    properties:
      version_id:
        type: string
        docs: >-
          Unique identifier for the Dataset Version to use in this evaluation.
          Starts with `dsv_`.
  EvaluationsRequest:
    properties:
      version_id:
        type: string
        docs: >-
          Unique identifier for the Evaluator Version to use in this evaluation.
          Starts with `evv_`.
      orchestrated:
        type: optional<boolean>
        docs: >-
          Whether the Evaluator is orchestrated by Humanloop. Default is `True`.
          If `False`, a log for the Evaluator should be submitted by the user
          via the API.
        default: true
  SrcExternalAppModelsV5LogsLogResponse:
    discriminated: false
    union:
      - PromptLogResponse
      - ToolLogResponse
      - EvaluatorLogResponse
  ToolResponse:
    docs: >-
      Base type that all File Responses should inherit from.


      Attributes defined here are common to all File Responses and should be
      overridden

      in the inheriting classes with documentation and appropriate Field
      definitions.
    properties:
      path:
        type: string
        docs: >-
          Path of the Tool, including the name, which is used as a unique
          identifier.
      id:
        type: string
        docs: Unique identifier for the Tool.
      directory_id:
        type: optional<string>
        docs: ID of the directory that the file is in on Humanloop.
      function:
        type: optional<ToolFunction>
        docs: >-
          Callable function specification of the Tool shown to the model for
          tool calling.
      source_code:
        type: optional<string>
        docs: Code source of the Tool.
      setup_values:
        type: optional<map<string, unknown>>
        docs: >-
          Values needed to setup the Tool, defined in JSON Schema format:
          https://json-schema.org/
      tool_type:
        type: optional<FilesToolType>
        docs: Type of Tool.
      commit_message:
        type: optional<string>
        docs: Message describing the changes made.
      name:
        type: string
        docs: Name of the Tool, which is used as a unique identifier.
      version_id:
        type: string
        docs: >-
          Unique identifier for the specific Tool Version. If no query params
          provided, the default deployed Tool Version is returned.
      type: optional<literal<"tool">>
      environments:
        type: optional<list<EnvironmentResponse>>
        docs: The list of environments the Tool Version is deployed to.
      created_at: datetime
      updated_at: datetime
      created_by:
        type: optional<UserResponse>
        docs: The user who created the Tool.
      status:
        type: VersionStatus
        docs: The status of the Tool Version.
      last_used_at: datetime
      version_logs_count:
        type: integer
        docs: The number of logs that have been generated for this Tool Version
      total_logs_count:
        type: integer
        docs: The number of logs that have been generated across all Tool Versions
      inputs:
        docs: >-
          Inputs associated to the Prompt. Inputs correspond to any of the
          variables used within the Tool template.
        type: list<InputResponse>
      evaluators:
        type: optional<list<MonitoringEvaluatorResponse>>
        docs: >-
          Evaluators that have been attached to this Tool that are used for
          monitoring logs.
      signature:
        type: optional<string>
        docs: Signature of the Tool.
      evaluator_aggregates:
        type: optional<list<EvaluatorAggregate>>
        docs: Aggregation of Evaluator results for the Tool Version.
  UserResponse:
    properties:
      id:
        type: string
        docs: Unique identifier for User. Starts with `usr`.
      email_address:
        type: string
        docs: The User's email address.
      full_name:
        type: optional<string>
        docs: The User's full name.
  UpdateEvaluationStatusRequest: unknown
  PaginatedPromptLogResponse: unknown
