imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    list:
      path: /evaluations
      method: GET
      auth: true
      docs: >-
        List all Evaluations for the specified `file_id`.


        Retrieve a list of Evaluations that evaluate versions of the specified
        File.
      pagination:
        offset: $request.page
        results: $response.records
      display-name: 'List '
      request:
        name: ListEvaluationsGetRequest
        query-parameters:
          file_id:
            type: string
            docs: >-
              Filter by File ID. Only Evaluations for the specified File will be
              returned.
          page:
            type: optional<integer>
            docs: Page number for pagination.
          size:
            type: optional<integer>
            docs: Page size for pagination. Number of Evaluations to fetch.
      response:
        docs: Successful Response
        type: root.PaginatedEvaluationResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: List evaluations for file
          query-parameters:
            file_id: pr_30gco7dx6JDq4200GVOHa
            size: 1
          response:
            body:
              page: 1
              size: 10
              total: 1
              records:
                - id: ev_567yza
                  created_at: '2024-05-01T12:00:00Z'
                  updated_at: '2024-05-01T12:00:00Z'
                  status: completed
                  dataset:
                    id: ds_345mno
                    path: test-questions
                    name: test-questions
                    version_id: dsv_678pqr
                    type: dataset
                    created_at: '2024-05-01T12:00:00Z'
                    updated_at: '2024-05-01T12:00:00Z'
                    created_by:
                      id: usr_v23rSVAgas2a
                      full_name: Jordan Burges
                      email_address: jordan@humanloop.com
                    status: committed
                    commit_message: initial commit
                    last_used_at: '2024-05-01T12:00:00Z'
                    datapoints_count: 2
                  evaluatees:
                    - version:
                        path: Personal Projects/Coding Assistant
                        id: pr_30gco7dx6JDq4200GVOHa
                        name: Coding Assistant
                        version_id: prv_7ZlQREDScH0xkhUwtXruN
                        type: prompt
                        environments:
                          - id: env_ffSVxEBzJcBZ1H5jcNMVj
                            created_at: '2023-06-27T23:16:07.992339'
                            name: development
                            tag: default
                        created_at: '2024-07-08T22:40:35.656915'
                        updated_at: '2024-07-08T22:40:35.656915'
                        created_by:
                          id: usr_01RJO1k2spBVqNUt1ASef
                          email_address: raza@humanloop.com
                          full_name: Raza Habib
                        status: committed
                        last_used_at: '2024-07-08T22:40:35.656915'
                        model: gpt-4o
                        endpoint: chat
                        template:
                          - content: >-
                              You are a helpful coding assistant specialising in
                              {{language}}
                            role: system
                        provider: openai
                        max_tokens: -1
                        temperature: 0.7
                        top_p: 1
                        presence_penalty: 0
                        frequency_penalty: 0
                        other: {}
                        tools: []
                        linked_tools: []
                        commit_message: Initial commit
                        version_logs_count: 0
                        total_logs_count: 0
                        inputs:
                          - name: messages
                      orchestrated: false
                  evaluators:
                    - version:
                        id: ev_890bcd
                        name: Accuracy Evaluator
                        path: Shared Evaluators/Accuracy Evaluator
                        version_id: evv_012def
                        type: evaluator
                        created_at: '2024-05-01T12:00:00Z'
                        updated_at: '2024-05-01T12:00:00Z'
                        status: committed
                        last_used_at: '2024-05-01T12:00:00Z'
                        spec:
                          arguments_type: target_required
                          return_type: number
                          evaluator_type: python
                          code: def evaluate(answer, target):\n    return 0.5
                        version_logs_count: 1
                        total_logs_count: 1
                        inputs:
                          - name: answer
                      orchestrated: false
    create:
      path: /evaluations
      method: POST
      auth: true
      docs: >-
        Create an Evaluation.


        Create a new Evaluation by specifying the Dataset, versions to be

        evaluated (Evaluatees), and which Evaluators to provide judgments.


        Humanloop will automatically start generating Logs and running
        Evaluators where

        `orchestrated=true`. If you own the runtime for the Evaluatee or
        Evaluator, you

        can set `orchestrated=false` and then generate and submit the required
        logs using

        your runtime.


        To keep updated on the progress of the Evaluation, you can poll the
        Evaluation using

        the GET /evaluations/{id} endpoint and check its status.
      display-name: Create
      request:
        body: root.CreateEvaluationRequest
      response:
        docs: Successful Response
        type: root.EvaluationResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Create evaluation
          request:
            dataset:
              version_id: dsv_6L78pqrdFi2xa
            evaluatees:
              - version_id: prv_7ZlQREDScH0xkhUwtXruN
                orchestrated: false
            evaluators:
              - version_id: evv_012def
                orchestrated: false
          response:
            body:
              id: ev_567yza
              created_at: '2024-05-01T12:00:00Z'
              updated_at: '2024-05-01T12:00:00Z'
              status: completed
              dataset:
                id: ds_345mno
                path: test-questions
                name: test-questions
                version_id: dsv_678pqr
                type: dataset
                created_at: '2024-05-01T12:00:00Z'
                updated_at: '2024-05-01T12:00:00Z'
                created_by:
                  id: usr_v23rSVAgas2a
                  full_name: Jordan Burges
                  email_address: jordan@humanloop.com
                status: committed
                commit_message: initial commit
                last_used_at: '2024-05-01T12:00:00Z'
                datapoints_count: 2
              evaluatees:
                - version:
                    path: Personal Projects/Coding Assistant
                    id: pr_30gco7dx6JDq4200GVOHa
                    name: Coding Assistant
                    version_id: prv_7ZlQREDScH0xkhUwtXruN
                    type: prompt
                    environments:
                      - id: env_ffSVxEBzJcBZ1H5jcNMVj
                        created_at: '2023-06-27T23:16:07.992339'
                        name: development
                        tag: default
                    created_at: '2024-07-08T22:40:35.656915'
                    updated_at: '2024-07-08T22:40:35.656915'
                    created_by:
                      id: usr_01RJO1k2spBVqNUt1ASef
                      email_address: raza@humanloop.com
                      full_name: Raza Habib
                    status: committed
                    last_used_at: '2024-07-08T22:40:35.656915'
                    model: gpt-4o
                    endpoint: chat
                    template:
                      - content: >-
                          You are a helpful coding assistant specialising in
                          {{language}}
                        role: system
                    provider: openai
                    max_tokens: -1
                    temperature: 0.7
                    top_p: 1
                    presence_penalty: 0
                    frequency_penalty: 0
                    other: {}
                    tools: []
                    linked_tools: []
                    commit_message: Initial commit
                    version_logs_count: 0
                    total_logs_count: 0
                    inputs:
                      - name: messages
                  orchestrated: false
              evaluators:
                - version:
                    id: ev_890bcd
                    name: Accuracy Evaluator
                    path: Shared Evaluators/Accuracy Evaluator
                    version_id: evv_012def
                    type: evaluator
                    created_at: '2024-05-01T12:00:00Z'
                    updated_at: '2024-05-01T12:00:00Z'
                    status: committed
                    last_used_at: '2024-05-01T12:00:00Z'
                    spec:
                      arguments_type: target_required
                      return_type: number
                      evaluator_type: python
                      code: def evaluate(answer, target):\n    return 0.5
                    version_logs_count: 1
                    total_logs_count: 1
                    inputs:
                      - name: answer
                  orchestrated: false
    get:
      path: /evaluations/{id}
      method: GET
      auth: true
      docs: Get an Evaluation.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluation.
      display-name: Get
      response:
        docs: Successful Response
        type: root.EvaluationResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Get evaluation
          path-parameters:
            id: ev_567yza
          response:
            body:
              id: ev_567yza
              created_at: '2024-05-01T12:00:00Z'
              updated_at: '2024-05-01T12:00:00Z'
              status: completed
              dataset:
                id: ds_345mno
                path: test-questions
                name: test-questions
                version_id: dsv_678pqr
                type: dataset
                created_at: '2024-05-01T12:00:00Z'
                updated_at: '2024-05-01T12:00:00Z'
                created_by:
                  id: usr_v23rSVAgas2a
                  full_name: Jordan Burges
                  email_address: jordan@humanloop.com
                status: committed
                commit_message: initial commit
                last_used_at: '2024-05-01T12:00:00Z'
                datapoints_count: 2
              evaluatees:
                - version:
                    path: Personal Projects/Coding Assistant
                    id: pr_30gco7dx6JDq4200GVOHa
                    name: Coding Assistant
                    version_id: prv_7ZlQREDScH0xkhUwtXruN
                    type: prompt
                    environments:
                      - id: env_ffSVxEBzJcBZ1H5jcNMVj
                        created_at: '2023-06-27T23:16:07.992339'
                        name: development
                        tag: default
                    created_at: '2024-07-08T22:40:35.656915'
                    updated_at: '2024-07-08T22:40:35.656915'
                    created_by:
                      id: usr_01RJO1k2spBVqNUt1ASef
                      email_address: raza@humanloop.com
                      full_name: Raza Habib
                    status: committed
                    last_used_at: '2024-07-08T22:40:35.656915'
                    model: gpt-4o
                    endpoint: chat
                    template:
                      - content: >-
                          You are a helpful coding assistant specialising in
                          {{language}}
                        role: system
                    provider: openai
                    max_tokens: -1
                    temperature: 0.7
                    top_p: 1
                    presence_penalty: 0
                    frequency_penalty: 0
                    other: {}
                    tools: []
                    linked_tools: []
                    commit_message: Initial commit
                    version_logs_count: 0
                    total_logs_count: 0
                    inputs:
                      - name: messages
                  orchestrated: false
              evaluators:
                - version:
                    id: ev_890bcd
                    name: Accuracy Evaluator
                    path: Shared Evaluators/Accuracy Evaluator
                    version_id: evv_012def
                    type: evaluator
                    created_at: '2024-05-01T12:00:00Z'
                    updated_at: '2024-05-01T12:00:00Z'
                    status: committed
                    last_used_at: '2024-05-01T12:00:00Z'
                    spec:
                      arguments_type: target_required
                      return_type: number
                      evaluator_type: python
                      code: def evaluate(answer, target):\n    return 0.5
                    version_logs_count: 1
                    total_logs_count: 1
                    inputs:
                      - name: answer
                  orchestrated: false
    delete:
      path: /evaluations/{id}
      method: DELETE
      auth: true
      docs: >-
        Delete an Evaluation.


        Remove an Evaluation from Humanloop. The Logs and Versions used in the
        Evaluation

        will not be deleted.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluation.
      display-name: Delete
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Delete evaluation
          path-parameters:
            id: ev_567yza
    updateSetup:
      path: /evaluations/{id}
      method: PATCH
      auth: true
      docs: >-
        Update an Evaluation.


        Update the setup of an Evaluation by specifying the Dataset, versions to
        be

        evaluated (Evaluatees), and which Evaluators to provide judgments.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluation.
      display-name: Update Setup
      request:
        body: root.CreateEvaluationRequest
      response:
        docs: Successful Response
        type: root.EvaluationResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - name: Update evaluation
          path-parameters:
            id: ev_567yza
          request:
            dataset:
              version_id: dsv_6L78pqrdFi2xa
            evaluatees:
              - version_id: prv_7ZlQREDScH0xkhUwtXruN
                orchestrated: false
            evaluators:
              - version_id: evv_012def
                orchestrated: false
          response:
            body:
              id: ev_567yza
              created_at: '2024-05-01T12:00:00Z'
              updated_at: '2024-05-01T12:00:00Z'
              status: completed
              dataset:
                id: ds_345mno
                path: test-questions
                name: test-questions
                version_id: dsv_678pqr
                type: dataset
                created_at: '2024-05-01T12:00:00Z'
                updated_at: '2024-05-01T12:00:00Z'
                created_by:
                  id: usr_v23rSVAgas2a
                  full_name: Jordan Burges
                  email_address: jordan@humanloop.com
                status: committed
                commit_message: initial commit
                last_used_at: '2024-05-01T12:00:00Z'
                datapoints_count: 2
              evaluatees:
                - version:
                    path: Personal Projects/Coding Assistant
                    id: pr_30gco7dx6JDq4200GVOHa
                    name: Coding Assistant
                    version_id: prv_7ZlQREDScH0xkhUwtXruN
                    type: prompt
                    environments:
                      - id: env_ffSVxEBzJcBZ1H5jcNMVj
                        created_at: '2023-06-27T23:16:07.992339'
                        name: development
                        tag: default
                    created_at: '2024-07-08T22:40:35.656915'
                    updated_at: '2024-07-08T22:40:35.656915'
                    created_by:
                      id: usr_01RJO1k2spBVqNUt1ASef
                      email_address: raza@humanloop.com
                      full_name: Raza Habib
                    status: committed
                    last_used_at: '2024-07-08T22:40:35.656915'
                    model: gpt-4o
                    endpoint: chat
                    template:
                      - content: >-
                          You are a helpful coding assistant specialising in
                          {{language}}
                        role: system
                    provider: openai
                    max_tokens: -1
                    temperature: 0.7
                    top_p: 1
                    presence_penalty: 0
                    frequency_penalty: 0
                    other: {}
                    tools: []
                    linked_tools: []
                    commit_message: Initial commit
                    version_logs_count: 0
                    total_logs_count: 0
                    inputs:
                      - name: messages
                  orchestrated: false
              evaluators:
                - version:
                    id: ev_890bcd
                    name: Accuracy Evaluator
                    path: Shared Evaluators/Accuracy Evaluator
                    version_id: evv_012def
                    type: evaluator
                    created_at: '2024-05-01T12:00:00Z'
                    updated_at: '2024-05-01T12:00:00Z'
                    status: committed
                    last_used_at: '2024-05-01T12:00:00Z'
                    spec:
                      arguments_type: target_required
                      return_type: number
                      evaluator_type: python
                      code: def evaluate(answer, target):\n    return 0.5
                    version_logs_count: 1
                    total_logs_count: 1
                    inputs:
                      - name: answer
                  orchestrated: false
    updateStatus:
      path: /evaluations/{id}/status
      method: PATCH
      auth: true
      docs: >-
        Update the status of an Evaluation.


        Can be used to cancel a running Evaluation, or mark an Evaluation that
        uses

        external or human evaluators as completed.
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluation.
      display-name: Update Status
      request:
        name: BodyUpdateStatusEvaluationsIdStatusPatch
        body:
          properties:
            status: root.EvaluationStatus
      response:
        docs: Successful Response
        type: root.EvaluationResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - path-parameters:
            id: id
          request:
            status: pending
          response:
            body:
              id: id
              dataset:
                path: path
                id: id
                name: name
                version_id: version_id
                type: dataset
                environments:
                  - id: id
                    created_at: '2024-01-15T09:30:00Z'
                    name: name
                    tag: default
                created_at: '2024-01-15T09:30:00Z'
                updated_at: '2024-01-15T09:30:00Z'
                created_by:
                  id: id
                  email_address: email_address
                  full_name: full_name
                status: uncommitted
                last_used_at: '2024-01-15T09:30:00Z'
                commit_message: commit_message
                datapoints_count: 1
                datapoints:
                  - id: id
              evaluatees:
                - version:
                    path: path
                    id: id
                    name: name
                    version_id: version_id
                    created_at: '2024-01-15T09:30:00Z'
                    updated_at: '2024-01-15T09:30:00Z'
                    status: uncommitted
                    last_used_at: '2024-01-15T09:30:00Z'
                    model: model
                    version_logs_count: 1
                    total_logs_count: 1
                    inputs:
                      - name: name
                  batch_id: batch_id
                  orchestrated: true
              evaluators:
                - version:
                    path: path
                    id: id
                    name: name
                    version_id: version_id
                    created_at: '2024-01-15T09:30:00Z'
                    updated_at: '2024-01-15T09:30:00Z'
                    status: uncommitted
                    last_used_at: '2024-01-15T09:30:00Z'
                    spec:
                      arguments_type: target_free
                      return_type: boolean
                    version_logs_count: 1
                    total_logs_count: 1
                    inputs:
                      - name: name
                  orchestrated: true
              status: pending
              created_at: '2024-01-15T09:30:00Z'
              created_by:
                id: id
                email_address: email_address
                full_name: full_name
              updated_at: '2024-01-15T09:30:00Z'
    getStats:
      path: /evaluations/{id}/stats
      method: GET
      auth: true
      docs: >-
        Get Evaluation Stats.


        Retrieve aggregate stats for the specified Evaluation.

        This includes the number of generated Logs for each evaluated version
        and the

        corresponding Evaluator statistics (such as the mean and percentiles).
      path-parameters:
        id:
          type: string
          docs: Unique identifier for Evaluation.
      display-name: Get Stats
      response:
        docs: Successful Response
        type: root.EvaluationStats
      errors:
        - root.UnprocessableEntityError
      examples:
        - path-parameters:
            id: id
          response:
            body:
              overall_stats:
                num_datapoints: 1
                total_logs: 1
                total_evaluator_logs: 1
              version_stats:
                - version_id: version_id
                  num_logs: 1
                  evaluator_version_stats:
                    - evaluator_version_id: evaluator_version_id
                      total_logs: 1
                      num_judgments: 1
                      num_nulls: 1
                      num_errors: 1
                      mean: 0
                      std: 1
                      percentiles:
                        '0': -2.5
                        '25': -0.6745
                        '50': 0
                        '75': 0.6745
                        '100': 2.5
    getLogs:
      path: /evaluations/{id}/logs
      method: GET
      auth: true
      docs: >-
        Get the Logs associated to a specific Evaluation.


        Each Datapoint in your Dataset will have a corresponding Log for each
        File version evaluated.

        e.g. If you have 50 Datapoints and are evaluating 2 Prompts, there will
        be 100 Logs associated with the Evaluation.
      path-parameters:
        id:
          type: string
          docs: String ID of evaluation. Starts with `ev_` or `evr_`.
      display-name: Get Logs
      request:
        name: GetLogsEvaluationsIdLogsGetRequest
        query-parameters:
          page:
            type: optional<integer>
            docs: Page number for pagination.
          size:
            type: optional<integer>
            docs: Page size for pagination. Number of Logs to fetch.
      response:
        docs: Successful Response
        type: root.PaginatedDataEvaluationReportLogResponse
      errors:
        - root.UnprocessableEntityError
      examples:
        - path-parameters:
            id: id
          response:
            body:
              records:
                - evaluated_version:
                    path: path
                    id: id
                    name: name
                    version_id: version_id
                    created_at: '2024-01-15T09:30:00Z'
                    updated_at: '2024-01-15T09:30:00Z'
                    status: uncommitted
                    last_used_at: '2024-01-15T09:30:00Z'
                    model: model
                    version_logs_count: 1
                    total_logs_count: 1
                    inputs:
                      - name: name
                  datapoint:
                    id: id
                  log:
                    id: id
                    prompt:
                      path: path
                      id: id
                      name: name
                      version_id: version_id
                      created_at: '2024-01-15T09:30:00Z'
                      updated_at: '2024-01-15T09:30:00Z'
                      status: uncommitted
                      last_used_at: '2024-01-15T09:30:00Z'
                      model: model
                      version_logs_count: 1
                      total_logs_count: 1
                      inputs:
                        - name: name
                  evaluator_logs:
                    - id: id
                      prompt:
                        path: path
                        id: id
                        name: name
                        version_id: version_id
                        created_at: '2024-01-15T09:30:00Z'
                        updated_at: '2024-01-15T09:30:00Z'
                        status: uncommitted
                        last_used_at: '2024-01-15T09:30:00Z'
                        model: model
                        version_logs_count: 1
                        total_logs_count: 1
                        inputs:
                          - name: name
              page: 1
              size: 1
              total: 1
  display-name: Evaluations
docs: >+
  Evaluations help you measure the performance of your Prompts, Tools and LLM
  Evaluators.


  An Evaluation consists of a Dataset, Evaluatees (i.e. Versions to evaluate),
  and Evaluators.

  When an Evaluation is created, Humanloop will start generating Logs, iterating
  through Datapoints in the Dataset,

  for each Evaluatee. The Evaluators will then be run on these Logs.


  Aggregate stats can be viewed in the Humanloop app or retrieved with the **Get
  Evaluation Stats** endpoint.


  Note that when an Evaluation is created, Humanloop will attempt to reuse any
  existing Logs for each Datapoint-Evaluatee

  pair. This means that you can create multiple Evaluations without generating
  new Logs unnecessarily.

