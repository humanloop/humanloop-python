# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.pagination import AsyncPager, SyncPager
from ..core.request_options import RequestOptions
from ..requests.chat_message import ChatMessageParams
from ..requests.evaluator_activation_deactivation_request_activate_item import (
    EvaluatorActivationDeactivationRequestActivateItemParams,
)
from ..requests.evaluator_activation_deactivation_request_deactivate_item import (
    EvaluatorActivationDeactivationRequestDeactivateItemParams,
)
from ..types.create_evaluator_log_response import CreateEvaluatorLogResponse
from ..types.evaluator_response import EvaluatorResponse
from ..types.file_environment_response import FileEnvironmentResponse
from ..types.file_sort_by import FileSortBy
from ..types.list_evaluators import ListEvaluators
from ..types.log_status import LogStatus
from ..types.sort_order import SortOrder
from .raw_client import AsyncRawEvaluatorsClient, RawEvaluatorsClient
from .requests.create_evaluator_log_request_judgment import CreateEvaluatorLogRequestJudgmentParams
from .requests.create_evaluator_log_request_spec import CreateEvaluatorLogRequestSpecParams
from .requests.evaluator_request_spec import EvaluatorRequestSpecParams

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class EvaluatorsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawEvaluatorsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawEvaluatorsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawEvaluatorsClient
        """
        return self._raw_client

    def log(
        self,
        *,
        parent_id: str,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        create_evaluator_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        judgment: typing.Optional[CreateEvaluatorLogRequestJudgmentParams] = OMIT,
        marked_completed: typing.Optional[bool] = OMIT,
        spec: typing.Optional[CreateEvaluatorLogRequestSpecParams] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreateEvaluatorLogResponse:
        """
        Submit Evaluator judgment for an existing Log.

        Creates a new Log. The evaluated Log will be set as the parent of the created Log.

        Parameters
        ----------
        parent_id : str
            Identifier of the evaluated Log. The newly created Log will have this one set as parent.

        version_id : typing.Optional[str]
            ID of the Evaluator version to log against.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Evaluator, including the name. This locates the Evaluator in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Evaluator.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from the LLM. Only populated for LLM Evaluator Logs.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider. Only populated for LLM Evaluator Logs.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider. Only populated for LLM Evaluator Logs.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        create_evaluator_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the LLM. Only populated for LLM Evaluator Logs.

        judgment : typing.Optional[CreateEvaluatorLogRequestJudgmentParams]
            Evaluator assessment of the Log.

        marked_completed : typing.Optional[bool]
            Whether the Log has been manually marked as completed by a user.

        spec : typing.Optional[CreateEvaluatorLogRequestSpecParams]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreateEvaluatorLogResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.log(parent_id='parent_id', )
        """
        _response = self._raw_client.log(
            parent_id=parent_id,
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            start_time=start_time,
            end_time=end_time,
            output=output,
            created_at=created_at,
            error=error,
            provider_latency=provider_latency,
            stdout=stdout,
            provider_request=provider_request,
            provider_response=provider_response,
            inputs=inputs,
            source=source,
            metadata=metadata,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            create_evaluator_log_request_environment=create_evaluator_log_request_environment,
            save=save,
            log_id=log_id,
            output_message=output_message,
            judgment=judgment,
            marked_completed=marked_completed,
            spec=spec,
            request_options=request_options,
        )
        return _response.data

    def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[FileSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> SyncPager[EvaluatorResponse]:
        """
        Get a list of all Evaluators.

        Parameters
        ----------
        page : typing.Optional[int]
            Page offset for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Evaluators to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Evaluator name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Evaluator. This filter matches against both email address and name of users.

        sort_by : typing.Optional[FileSortBy]
            Field to sort Evaluators by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        SyncPager[EvaluatorResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        response = client.evaluators.list(size=1, )
        for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        for page in response.iter_pages():
            yield page
        """
        return self._raw_client.list(
            page=page,
            size=size,
            name=name,
            user_filter=user_filter,
            sort_by=sort_by,
            order=order,
            request_options=request_options,
        )

    def upsert(
        self,
        *,
        spec: EvaluatorRequestSpecParams,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Create an Evaluator or update it with a new version if it already exists.

        Evaluators are identified by the `ID` or their `path`. The spec provided determines the version of the Evaluator.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within an Evaluator - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        spec : EvaluatorRequestSpecParams

        path : typing.Optional[str]
            Path of the Evaluator, including the name. This locates the Evaluator in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Evaluator.

        version_name : typing.Optional[str]
            Unique name for the Evaluator version. Version names must be unique for a given Evaluator.

        version_description : typing.Optional[str]
            Description of the version, e.g., the changes made in this version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.upsert(path='Shared Evaluators/Accuracy Evaluator', spec={'arguments_type': "target_required", 'return_type': "number", 'evaluator_type': 'python', 'code': 'def evaluate(answer, target):\n    return 0.5'}, version_name='simple-evaluator', version_description='Simple evaluator that returns 0.5', )
        """
        _response = self._raw_client.upsert(
            spec=spec,
            path=path,
            id=id,
            version_name=version_name,
            version_description=version_description,
            request_options=request_options,
        )
        return _response.data

    def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Retrieve the Evaluator with the given ID.

        By default, the deployed version of the Evaluator is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Evaluator.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        version_id : typing.Optional[str]
            A specific Version ID of the Evaluator to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.get(id='ev_890bcd', )
        """
        _response = self._raw_client.get(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return _response.data

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Evaluator with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.delete(id='ev_890bcd', )
        """
        _response = self._raw_client.delete(id, request_options=request_options)
        return _response.data

    def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Move the Evaluator to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        path : typing.Optional[str]
            Path of the Evaluator including the Evaluator name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Evaluator, which is used as a unique identifier.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.move(id='ev_890bcd', path='new directory/new name', )
        """
        _response = self._raw_client.move(id, path=path, name=name, request_options=request_options)
        return _response.data

    def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListEvaluators:
        """
        Get a list of all the versions of an Evaluator.

        Parameters
        ----------
        id : str
            Unique identifier for the Evaluator.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListEvaluators
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.list_versions(id='ev_890bcd', )
        """
        _response = self._raw_client.list_versions(
            id, evaluator_aggregates=evaluator_aggregates, request_options=request_options
        )
        return _response.data

    def delete_evaluator_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a version of the Evaluator.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        version_id : str
            Unique identifier for the specific version of the Evaluator.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.delete_evaluator_version(id='id', version_id='version_id', )
        """
        _response = self._raw_client.delete_evaluator_version(id, version_id, request_options=request_options)
        return _response.data

    def update_evaluator_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Update the name or description of the Evaluator version.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        version_id : str
            Unique identifier for the specific version of the Evaluator.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.update_evaluator_version(id='id', version_id='version_id', )
        """
        _response = self._raw_client.update_evaluator_version(
            id, version_id, name=name, description=description, request_options=request_options
        )
        return _response.data

    def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluatorResponse:
        """
        Deploy Evaluator to an Environment.

        Set the deployed version for the specified Environment. This Evaluator
        will be used for calls made to the Evaluator in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Evaluator.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.set_deployment(id='ev_890bcd', environment_id='staging', version_id='evv_012def', )
        """
        _response = self._raw_client.set_deployment(
            id, environment_id, version_id=version_id, request_options=request_options
        )
        return _response.data

    def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Evaluator from the Environment.

        Remove the deployed version for the specified Environment. This Evaluator
        will no longer be used for calls made to the Evaluator in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.remove_deployment(id='ev_890bcd', environment_id='staging', )
        """
        _response = self._raw_client.remove_deployment(id, environment_id, request_options=request_options)
        return _response.data

    def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Evaluator.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.list_environments(id='ev_890bcd', )
        """
        _response = self._raw_client.list_environments(id, request_options=request_options)
        return _response.data

    def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Activate and deactivate Evaluators for monitoring the Evaluator.

        An activated Evaluator will automatically be run on all new Logs
        within the Evaluator for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluators.update_monitoring(id='id', )
        """
        _response = self._raw_client.update_monitoring(
            id, activate=activate, deactivate=deactivate, request_options=request_options
        )
        return _response.data


class AsyncEvaluatorsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawEvaluatorsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawEvaluatorsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawEvaluatorsClient
        """
        return self._raw_client

    async def log(
        self,
        *,
        parent_id: str,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        create_evaluator_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        judgment: typing.Optional[CreateEvaluatorLogRequestJudgmentParams] = OMIT,
        marked_completed: typing.Optional[bool] = OMIT,
        spec: typing.Optional[CreateEvaluatorLogRequestSpecParams] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreateEvaluatorLogResponse:
        """
        Submit Evaluator judgment for an existing Log.

        Creates a new Log. The evaluated Log will be set as the parent of the created Log.

        Parameters
        ----------
        parent_id : str
            Identifier of the evaluated Log. The newly created Log will have this one set as parent.

        version_id : typing.Optional[str]
            ID of the Evaluator version to log against.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Evaluator, including the name. This locates the Evaluator in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Evaluator.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from the LLM. Only populated for LLM Evaluator Logs.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider. Only populated for LLM Evaluator Logs.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider. Only populated for LLM Evaluator Logs.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        create_evaluator_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the LLM. Only populated for LLM Evaluator Logs.

        judgment : typing.Optional[CreateEvaluatorLogRequestJudgmentParams]
            Evaluator assessment of the Log.

        marked_completed : typing.Optional[bool]
            Whether the Log has been manually marked as completed by a user.

        spec : typing.Optional[CreateEvaluatorLogRequestSpecParams]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreateEvaluatorLogResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.log(parent_id='parent_id', )
        asyncio.run(main())
        """
        _response = await self._raw_client.log(
            parent_id=parent_id,
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            start_time=start_time,
            end_time=end_time,
            output=output,
            created_at=created_at,
            error=error,
            provider_latency=provider_latency,
            stdout=stdout,
            provider_request=provider_request,
            provider_response=provider_response,
            inputs=inputs,
            source=source,
            metadata=metadata,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            create_evaluator_log_request_environment=create_evaluator_log_request_environment,
            save=save,
            log_id=log_id,
            output_message=output_message,
            judgment=judgment,
            marked_completed=marked_completed,
            spec=spec,
            request_options=request_options,
        )
        return _response.data

    async def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[FileSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncPager[EvaluatorResponse]:
        """
        Get a list of all Evaluators.

        Parameters
        ----------
        page : typing.Optional[int]
            Page offset for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Evaluators to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Evaluator name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Evaluator. This filter matches against both email address and name of users.

        sort_by : typing.Optional[FileSortBy]
            Field to sort Evaluators by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncPager[EvaluatorResponse]
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            response = await client.evaluators.list(size=1, )
            async for item in response:
                yield item

            # alternatively, you can paginate page-by-page
            async for page in response.iter_pages():
                yield page
        asyncio.run(main())
        """
        return await self._raw_client.list(
            page=page,
            size=size,
            name=name,
            user_filter=user_filter,
            sort_by=sort_by,
            order=order,
            request_options=request_options,
        )

    async def upsert(
        self,
        *,
        spec: EvaluatorRequestSpecParams,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Create an Evaluator or update it with a new version if it already exists.

        Evaluators are identified by the `ID` or their `path`. The spec provided determines the version of the Evaluator.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within an Evaluator - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        spec : EvaluatorRequestSpecParams

        path : typing.Optional[str]
            Path of the Evaluator, including the name. This locates the Evaluator in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Evaluator.

        version_name : typing.Optional[str]
            Unique name for the Evaluator version. Version names must be unique for a given Evaluator.

        version_description : typing.Optional[str]
            Description of the version, e.g., the changes made in this version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.upsert(path='Shared Evaluators/Accuracy Evaluator', spec={'arguments_type': "target_required", 'return_type': "number", 'evaluator_type': 'python', 'code': 'def evaluate(answer, target):\n    return 0.5'}, version_name='simple-evaluator', version_description='Simple evaluator that returns 0.5', )
        asyncio.run(main())
        """
        _response = await self._raw_client.upsert(
            spec=spec,
            path=path,
            id=id,
            version_name=version_name,
            version_description=version_description,
            request_options=request_options,
        )
        return _response.data

    async def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Retrieve the Evaluator with the given ID.

        By default, the deployed version of the Evaluator is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Evaluator.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        version_id : typing.Optional[str]
            A specific Version ID of the Evaluator to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.get(id='ev_890bcd', )
        asyncio.run(main())
        """
        _response = await self._raw_client.get(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return _response.data

    async def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Evaluator with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.delete(id='ev_890bcd', )
        asyncio.run(main())
        """
        _response = await self._raw_client.delete(id, request_options=request_options)
        return _response.data

    async def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Move the Evaluator to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        path : typing.Optional[str]
            Path of the Evaluator including the Evaluator name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Evaluator, which is used as a unique identifier.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.move(id='ev_890bcd', path='new directory/new name', )
        asyncio.run(main())
        """
        _response = await self._raw_client.move(id, path=path, name=name, request_options=request_options)
        return _response.data

    async def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListEvaluators:
        """
        Get a list of all the versions of an Evaluator.

        Parameters
        ----------
        id : str
            Unique identifier for the Evaluator.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListEvaluators
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.list_versions(id='ev_890bcd', )
        asyncio.run(main())
        """
        _response = await self._raw_client.list_versions(
            id, evaluator_aggregates=evaluator_aggregates, request_options=request_options
        )
        return _response.data

    async def delete_evaluator_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a version of the Evaluator.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        version_id : str
            Unique identifier for the specific version of the Evaluator.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.delete_evaluator_version(id='id', version_id='version_id', )
        asyncio.run(main())
        """
        _response = await self._raw_client.delete_evaluator_version(id, version_id, request_options=request_options)
        return _response.data

    async def update_evaluator_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Update the name or description of the Evaluator version.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        version_id : str
            Unique identifier for the specific version of the Evaluator.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.update_evaluator_version(id='id', version_id='version_id', )
        asyncio.run(main())
        """
        _response = await self._raw_client.update_evaluator_version(
            id, version_id, name=name, description=description, request_options=request_options
        )
        return _response.data

    async def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluatorResponse:
        """
        Deploy Evaluator to an Environment.

        Set the deployed version for the specified Environment. This Evaluator
        will be used for calls made to the Evaluator in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Evaluator.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.set_deployment(id='ev_890bcd', environment_id='staging', version_id='evv_012def', )
        asyncio.run(main())
        """
        _response = await self._raw_client.set_deployment(
            id, environment_id, version_id=version_id, request_options=request_options
        )
        return _response.data

    async def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Evaluator from the Environment.

        Remove the deployed version for the specified Environment. This Evaluator
        will no longer be used for calls made to the Evaluator in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.remove_deployment(id='ev_890bcd', environment_id='staging', )
        asyncio.run(main())
        """
        _response = await self._raw_client.remove_deployment(id, environment_id, request_options=request_options)
        return _response.data

    async def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Evaluator.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluator.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.list_environments(id='ev_890bcd', )
        asyncio.run(main())
        """
        _response = await self._raw_client.list_environments(id, request_options=request_options)
        return _response.data

    async def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluatorResponse:
        """
        Activate and deactivate Evaluators for monitoring the Evaluator.

        An activated Evaluator will automatically be run on all new Logs
        within the Evaluator for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluatorResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluators.update_monitoring(id='id', )
        asyncio.run(main())
        """
        _response = await self._raw_client.update_monitoring(
            id, activate=activate, deactivate=deactivate, request_options=request_options
        )
        return _response.data
