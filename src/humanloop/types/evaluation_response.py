# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

import pydantic

from ..core.pydantic_utilities import IS_PYDANTIC_V2
from ..core.unchecked_base_model import UncheckedBaseModel
from .dataset_response import DatasetResponse
from .evaluatee_response import EvaluateeResponse
from .evaluation_evaluator_response import EvaluationEvaluatorResponse
from .evaluation_status import EvaluationStatus
from .user_response import UserResponse


class EvaluationResponse(UncheckedBaseModel):
    id: str = pydantic.Field()
    """
    Unique identifier for the Evaluation. Starts with `evr`.
    """

    dataset: DatasetResponse = pydantic.Field()
    """
    The Dataset used in the Evaluation.
    """

    evaluatees: typing.List[EvaluateeResponse] = pydantic.Field()
    """
    The Prompt/Tool Versions included in the Evaluation.
    """

    evaluators: typing.List[EvaluationEvaluatorResponse] = pydantic.Field()
    """
    The Evaluator Versions used to evaluate.
    """

    status: EvaluationStatus = pydantic.Field()
    """
    The current status of the Evaluation.
    
    - `"pending"`: The Evaluation has been created but is not actively being worked on by Humanloop.
    - `"running"`: Humanloop is checking for any missing Logs and Evaluator Logs, and will generate them where appropriate.
    - `"completed"`: All Logs an Evaluator Logs have been generated.
    - `"cancelled"`: The Evaluation has been cancelled by the user. Humanloop will stop generating Logs and Evaluator Logs.
    """

    created_at: dt.datetime
    created_by: typing.Optional[UserResponse] = None
    updated_at: dt.datetime

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow
