# This file was auto-generated by Fern from our API Definition.

from __future__ import annotations
from ..core.unchecked_base_model import UncheckedBaseModel
from .evaluator_response import EvaluatorResponse
from .flow_response import FlowResponse
from .monitoring_evaluator_response import MonitoringEvaluatorResponse
from .prompt_response import PromptResponse
from .tool_response import ToolResponse
from .version_deployment_response import VersionDeploymentResponse
from .version_id_response import VersionIdResponse
import pydantic
from .dataset_response import DatasetResponse
import typing
from .evaluatee_response import EvaluateeResponse
from .evaluation_evaluator_response import EvaluationEvaluatorResponse
from .evaluation_status import EvaluationStatus
import datetime as dt
from .user_response import UserResponse
from ..core.pydantic_utilities import IS_PYDANTIC_V2
from ..core.pydantic_utilities import update_forward_refs


class EvaluationResponse(UncheckedBaseModel):
    id: str = pydantic.Field()
    """
    Unique identifier for the Evaluation. Starts with `evr`.
    """

    dataset: DatasetResponse = pydantic.Field()
    """
    The Dataset used in the Evaluation.
    """

    evaluatees: typing.List[EvaluateeResponse] = pydantic.Field()
    """
    The Prompt/Tool Versions included in the Evaluation.
    """

    evaluators: typing.List[EvaluationEvaluatorResponse] = pydantic.Field()
    """
    The Evaluator Versions used to evaluate.
    """

    status: EvaluationStatus = pydantic.Field()
    """
    The current status of the Evaluation.
    
    - `"pending"`: The Evaluation has been created but is not actively being worked on by Humanloop.
    - `"running"`: Humanloop is checking for any missing Logs and Evaluator Logs, and will generate them where appropriate.
    - `"completed"`: All Logs an Evaluator Logs have been generated.
    - `"cancelled"`: The Evaluation has been cancelled by the user. Humanloop will stop generating Logs and Evaluator Logs.
    """

    name: typing.Optional[str] = pydantic.Field(default=None)
    """
    Name of the Evaluation to help identify it. Must be unique among Evaluations associated with File.
    """

    file_id: typing.Optional[str] = pydantic.Field(default=None)
    """
    Unique identifier for the File associated with the Evaluation.
    """

    created_at: dt.datetime
    created_by: typing.Optional[UserResponse] = None
    updated_at: dt.datetime
    url: typing.Optional[str] = pydantic.Field(default=None)
    """
    URL to view the Evaluation on the Humanloop.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow


update_forward_refs(EvaluatorResponse, EvaluationResponse=EvaluationResponse)
update_forward_refs(FlowResponse, EvaluationResponse=EvaluationResponse)
update_forward_refs(MonitoringEvaluatorResponse, EvaluationResponse=EvaluationResponse)
update_forward_refs(PromptResponse, EvaluationResponse=EvaluationResponse)
update_forward_refs(ToolResponse, EvaluationResponse=EvaluationResponse)
update_forward_refs(VersionDeploymentResponse, EvaluationResponse=EvaluationResponse)
update_forward_refs(VersionIdResponse, EvaluationResponse=EvaluationResponse)
