# This file was auto-generated by Fern from our API Definition.

import typing

import pydantic

from ..core.pydantic_utilities import IS_PYDANTIC_V2
from ..core.unchecked_base_model import UncheckedBaseModel
from .evaluatee_request import EvaluateeRequest
from .evaluations_dataset_request import EvaluationsDatasetRequest
from .evaluations_request import EvaluationsRequest


class CreateEvaluationRequest(UncheckedBaseModel):
    """
    Request model for creating an Evaluation.

    Evaluation benchmark your Prompt/Tool Versions. With the Datapoints in a Dataset Version,
    Logs corresponding to the Datapoint and each Evaluated Version are evaluated by the specified Evaluator Versions.
    Aggregated statistics are then calculated and presented in the Evaluation.
    """

    dataset: EvaluationsDatasetRequest = pydantic.Field()
    """
    The Dataset Version to use in this Evaluation.
    """

    evaluatees: typing.List[EvaluateeRequest] = pydantic.Field()
    """
    Unique identifiers for the Prompt/Tool Versions to include in the Evaluation Report.
    """

    evaluators: typing.List[EvaluationsRequest] = pydantic.Field()
    """
    The Evaluators used to evaluate.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow
