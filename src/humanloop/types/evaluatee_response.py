# This file was auto-generated by Fern from our API Definition.

from ..core.unchecked_base_model import UncheckedBaseModel
from .agent_linked_file_response import AgentLinkedFileResponse
from .agent_response import AgentResponse
from .evaluator_response import EvaluatorResponse
from .flow_response import FlowResponse
from .monitoring_evaluator_response import MonitoringEvaluatorResponse
from .prompt_response import PromptResponse
from .tool_response import ToolResponse
from .version_deployment_response import VersionDeploymentResponse
from .version_id_response import VersionIdResponse
import typing
from .run_version_response import RunVersionResponse
import pydantic
import datetime as dt
from ..core.pydantic_utilities import IS_PYDANTIC_V2


class EvaluateeResponse(UncheckedBaseModel):
    """
    Version of the Evaluatee being evaluated.
    """

    version: typing.Optional[RunVersionResponse] = None
    batch_id: typing.Optional[str] = pydantic.Field(default=None)
    """
    Unique identifier for the batch of Logs to include in the Evaluation. 
    """

    orchestrated: bool = pydantic.Field()
    """
    Whether the Prompt/Tool is orchestrated by Humanloop. Default is `True`. If `False`, a log for the Prompt/Tool should be submitted by the user via the API.
    """

    pinned: bool = pydantic.Field()
    """
    Pinned Evaluatees are shown in Humanloop's Overview, allowing you to use them as baselines for comparison.
    """

    added_at: typing.Optional[dt.datetime] = pydantic.Field(default=None)
    """
    When the Evaluatee was added to the Evaluation.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow
