# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

import pydantic
from ..core.pydantic_utilities import IS_PYDANTIC_V2
from ..core.unchecked_base_model import UncheckedBaseModel
from .run_version_response import RunVersionResponse


class EvaluateeResponse(UncheckedBaseModel):
    """
    Version of the Evaluatee being evaluated.
    """

    version: typing.Optional[RunVersionResponse] = None
    batch_id: typing.Optional[str] = pydantic.Field(default=None)
    """
    Unique identifier for the batch of Logs to include in the Evaluation. 
    """

    orchestrated: bool = pydantic.Field()
    """
    Whether the Prompt/Tool is orchestrated by Humanloop. Default is `True`. If `False`, a log for the Prompt/Tool should be submitted by the user via the API.
    """

    pinned: bool = pydantic.Field()
    """
    Pinned Evaluatees are shown in Humanloop's Overview, allowing you to use them as baselines for comparison.
    """

    added_at: typing.Optional[dt.datetime] = pydantic.Field(default=None)
    """
    When the Evaluatee was added to the Evaluation.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow
