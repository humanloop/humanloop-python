# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from .raw_client import RawAgentsClient
from ..requests.chat_message import ChatMessageParams
from .requests.agent_log_request_tool_choice import AgentLogRequestToolChoiceParams
from ..requests.agent_kernel_request import AgentKernelRequestParams
import datetime as dt
from ..types.log_status import LogStatus
from ..core.request_options import RequestOptions
from ..types.create_agent_log_response import CreateAgentLogResponse
from ..types.agent_log_response import AgentLogResponse
from .requests.agents_call_stream_request_tool_choice import AgentsCallStreamRequestToolChoiceParams
from ..requests.provider_api_keys import ProviderApiKeysParams
from ..types.agent_call_stream_response import AgentCallStreamResponse
from .requests.agents_call_request_tool_choice import AgentsCallRequestToolChoiceParams
from ..types.agent_call_response import AgentCallResponse
from ..types.agent_continue_call_stream_response import AgentContinueCallStreamResponse
from ..types.agent_continue_call_response import AgentContinueCallResponse
from ..types.project_sort_by import ProjectSortBy
from ..types.sort_order import SortOrder
from ..core.pagination import SyncPager
from ..types.agent_response import AgentResponse
from ..types.paginated_data_agent_response import PaginatedDataAgentResponse
from ..core.unchecked_base_model import construct_type
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from ..types.model_endpoints import ModelEndpoints
from .requests.agent_request_template import AgentRequestTemplateParams
from ..types.template_language import TemplateLanguage
from ..types.model_providers import ModelProviders
from .requests.agent_request_stop import AgentRequestStopParams
from ..requests.response_format import ResponseFormatParams
from .requests.agent_request_reasoning_effort import AgentRequestReasoningEffortParams
from .requests.agent_request_tools_item import AgentRequestToolsItemParams
from ..types.list_agents import ListAgents
from ..types.file_environment_response import FileEnvironmentResponse
from ..requests.evaluator_activation_deactivation_request_activate_item import (
    EvaluatorActivationDeactivationRequestActivateItemParams,
)
from ..requests.evaluator_activation_deactivation_request_deactivate_item import (
    EvaluatorActivationDeactivationRequestDeactivateItemParams,
)
from ..types.agent_kernel_request import AgentKernelRequest
from ..core.client_wrapper import AsyncClientWrapper
from .raw_client import AsyncRawAgentsClient
from ..core.pagination import AsyncPager

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class AgentsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawAgentsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawAgentsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawAgentsClient
        """
        return self._raw_client

    def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        run_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentLogRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agent_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreateAgentLogResponse:
        """
        Create an Agent Log.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise, the default deployed version will be chosen.

        If you create the Agent Log with a `log_status` of `incomplete`, you should later update it to `complete`
        in order to trigger Evaluators.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        run_id : typing.Optional[str]
            Unique identifier for the Run to associate the Log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agent_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreateAgentLogResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.log(
            path="Banking/Teller Agent",
            agent={
                "provider": "anthropic",
                "endpoint": "chat",
                "model": "claude-3-7-sonnet-latest",
                "reasoning_effort": 1024,
                "template": [
                    {
                        "role": "system",
                        "content": "You are a helpful digital assistant, helping users navigate our digital banking platform.",
                    }
                ],
                "max_iterations": 3,
                "tools": [
                    {
                        "type": "file",
                        "link": {
                            "file_id": "pr_1234567890",
                            "version_id": "prv_1234567890",
                        },
                        "on_agent_call": "continue",
                    },
                    {
                        "type": "inline",
                        "json_schema": {
                            "name": "stop",
                            "description": "Call this tool when you have finished your task.",
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    "output": {
                                        "type": "string",
                                        "description": "The final output to return to the user.",
                                    }
                                },
                                "additionalProperties": False,
                                "required": ["output"],
                            },
                            "strict": True,
                        },
                        "on_agent_call": "stop",
                    },
                ],
            },
        )
        """
        response = self._raw_client.log(
            version_id=version_id,
            environment=environment,
            run_id=run_id,
            path=path,
            id=id,
            output_message=output_message,
            prompt_tokens=prompt_tokens,
            reasoning_tokens=reasoning_tokens,
            output_tokens=output_tokens,
            prompt_cost=prompt_cost,
            output_cost=output_cost,
            finish_reason=finish_reason,
            messages=messages,
            tool_choice=tool_choice,
            agent=agent,
            start_time=start_time,
            end_time=end_time,
            output=output,
            created_at=created_at,
            error=error,
            provider_latency=provider_latency,
            stdout=stdout,
            provider_request=provider_request,
            provider_response=provider_response,
            inputs=inputs,
            source=source,
            metadata=metadata,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            agent_log_request_environment=agent_log_request_environment,
            save=save,
            log_id=log_id,
            request_options=request_options,
        )
        return response.data

    def update_log(
        self,
        id: str,
        log_id: str,
        *,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        output: typing.Optional[str] = OMIT,
        error: typing.Optional[str] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentLogResponse:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        log_id : str
            Unique identifier for the Log.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            List of chat messages that were used as an input to the Flow.

        output_message : typing.Optional[ChatMessageParams]
            The output message returned by this Flow.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the Flow Log.

        output : typing.Optional[str]
            The output of the Flow Log. Provide None to unset existing `output` value. Provide either this, `output_message` or `error`.

        error : typing.Optional[str]
            The error message of the Flow Log. Provide None to unset existing `error` value. Provide either this, `output_message` or `output`.

        log_status : typing.Optional[LogStatus]
            Status of the Flow Log. When a Flow Log is updated to `complete`, no more Logs can be added to it. Monitoring Evaluators will only run on `complete` Flow Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentLogResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.update_log(
            id="ag_1234567890",
            log_id="log_1234567890",
            messages=[
                {"role": "user", "content": "I need to withdraw $1000"},
                {
                    "role": "assistant",
                    "content": "Of course! Would you like to use your savings or checking account?",
                },
            ],
            output_message={
                "role": "assistant",
                "content": "I'm sorry, I can't help with that.",
            },
            log_status="complete",
        )
        """
        response = self._raw_client.update_log(
            id,
            log_id,
            messages=messages,
            output_message=output_message,
            inputs=inputs,
            output=output,
            error=error,
            log_status=log_status,
            request_options=request_options,
        )
        return response.data

    def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentsCallStreamRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agents_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[AgentCallStreamResponse]:
        """
        Call an Agent. The Agent will run on the Humanloop runtime and return a completed Agent Log.

        If the Agent requires a tool call that cannot be ran by Humanloop, execution will halt. To continue,
        pass the ID of the incomplete Log and the required tool call to the /agents/continue endpoint.

        The agent will run for the maximum number of iterations, or until it encounters a stop condition,
        according to its configuration.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Agent details in the request body. A new version is created if it does not match
        any existing ones. This is helpful in the case where you are storing or deriving
        your Agent details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agents_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Only applies when not streaming. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[AgentCallStreamResponse]


        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.agents.call_stream()
        for chunk in response:
            yield chunk
        """
        with self._raw_client.call_stream(
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            messages=messages,
            tool_choice=tool_choice,
            agent=agent,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            agents_call_stream_request_environment=agents_call_stream_request_environment,
            save=save,
            log_id=log_id,
            provider_api_keys=provider_api_keys,
            return_inputs=return_inputs,
            include_trace_children=include_trace_children,
            request_options=request_options,
        ) as r:
            yield from r.data

    def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentsCallRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agents_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentCallResponse:
        """
        Call an Agent. The Agent will run on the Humanloop runtime and return a completed Agent Log.

        If the Agent requires a tool call that cannot be ran by Humanloop, execution will halt. To continue,
        pass the ID of the incomplete Log and the required tool call to the /agents/continue endpoint.

        The agent will run for the maximum number of iterations, or until it encounters a stop condition,
        according to its configuration.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Agent details in the request body. A new version is created if it does not match
        any existing ones. This is helpful in the case where you are storing or deriving
        your Agent details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agents_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Only applies when not streaming. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentCallResponse


        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.call(
            path="Banking/Teller Agent",
            messages=[
                {
                    "role": "user",
                    "content": "I'd like to deposit $1000 to my savings account from my checking account.",
                }
            ],
        )
        """
        response = self._raw_client.call(
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            messages=messages,
            tool_choice=tool_choice,
            agent=agent,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            agents_call_request_environment=agents_call_request_environment,
            save=save,
            log_id=log_id,
            provider_api_keys=provider_api_keys,
            return_inputs=return_inputs,
            include_trace_children=include_trace_children,
            request_options=request_options,
        )
        return response.data

    def continue_call_stream(
        self,
        *,
        log_id: str,
        messages: typing.Sequence[ChatMessageParams],
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[AgentContinueCallStreamResponse]:
        """
        Continue an incomplete Agent call.

        This endpoint allows continuing an existing incomplete Agent call, by passing the tool call
        requested by the Agent. The Agent will resume processing from where it left off.

        The messages in the request will be appended to the original messages in the Log. You do not
        have to provide the previous conversation history.

        The original log must be in an incomplete state to be continued.

        Parameters
        ----------
        log_id : str
            This identifies the Agent Log to continue.

        messages : typing.Sequence[ChatMessageParams]
            The additional messages with which to continue the Agent Log. Often, these should start with the Tool messages with results for the previous Assistant message's tool calls.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[AgentContinueCallStreamResponse]


        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.agents.continue_call_stream(
            log_id="log_id",
            messages=[{"role": "user"}],
        )
        for chunk in response:
            yield chunk
        """
        with self._raw_client.continue_call_stream(
            log_id=log_id,
            messages=messages,
            provider_api_keys=provider_api_keys,
            include_trace_children=include_trace_children,
            request_options=request_options,
        ) as r:
            yield from r.data

    def continue_call(
        self,
        *,
        log_id: str,
        messages: typing.Sequence[ChatMessageParams],
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentContinueCallResponse:
        """
        Continue an incomplete Agent call.

        This endpoint allows continuing an existing incomplete Agent call, by passing the tool call
        requested by the Agent. The Agent will resume processing from where it left off.

        The messages in the request will be appended to the original messages in the Log. You do not
        have to provide the previous conversation history.

        The original log must be in an incomplete state to be continued.

        Parameters
        ----------
        log_id : str
            This identifies the Agent Log to continue.

        messages : typing.Sequence[ChatMessageParams]
            The additional messages with which to continue the Agent Log. Often, these should start with the Tool messages with results for the previous Assistant message's tool calls.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentContinueCallResponse


        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.continue_call(
            log_id="log_1234567890",
            messages=[
                {
                    "role": "tool",
                    "content": '{"type": "checking", "balance": 5200}',
                    "tool_call_id": "tc_1234567890",
                }
            ],
        )
        """
        response = self._raw_client.continue_call(
            log_id=log_id,
            messages=messages,
            provider_api_keys=provider_api_keys,
            include_trace_children=include_trace_children,
            request_options=request_options,
        )
        return response.data

    def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[ProjectSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> SyncPager[AgentResponse]:
        """
        Get a list of all Agents.

        Parameters
        ----------
        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Agents to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Agent name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Agent. This filter matches against both email address and name of users.

        sort_by : typing.Optional[ProjectSortBy]
            Field to sort Agents by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        SyncPager[AgentResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.agents.list(
            size=1,
        )
        for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        for page in response.iter_pages():
            yield page
        """
        page = page if page is not None else 1
        _response = self._raw_client._client_wrapper.httpx_client.request(
            "agents",
            method="GET",
            params={
                "page": page,
                "size": size,
                "name": name,
                "user_filter": user_filter,
                "sort_by": sort_by,
                "order": order,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(
                    PaginatedDataAgentResponse,
                    construct_type(
                        type_=PaginatedDataAgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                _has_next = True
                _get_next = lambda: self.list(
                    page=page + 1,
                    size=size,
                    name=name,
                    user_filter=user_filter,
                    sort_by=sort_by,
                    order=order,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return SyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[AgentRequestTemplateParams] = OMIT,
        template_language: typing.Optional[TemplateLanguage] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[AgentRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        reasoning_effort: typing.Optional[AgentRequestReasoningEffortParams] = OMIT,
        tools: typing.Optional[typing.Sequence[AgentRequestToolsItemParams]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        max_iterations: typing.Optional[int] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        tags: typing.Optional[typing.Sequence[str]] = OMIT,
        readme: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Create an Agent or update it with a new version if it already exists.

        Agents are identified by the `ID` or their `path`. The parameters (i.e. the template, temperature, model etc.) and
        tools determine the versions of the Agent.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within an Agent - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[AgentRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        template_language : typing.Optional[TemplateLanguage]
            The template language to use for rendering the template.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[AgentRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        reasoning_effort : typing.Optional[AgentRequestReasoningEffortParams]
            Guidance on how many reasoning tokens it should generate before creating a response to the prompt. OpenAI reasoning models (o1, o3-mini) expect a OpenAIReasoningEffort enum. Anthropic reasoning models expect an integer, which signifies the maximum token budget.

        tools : typing.Optional[typing.Sequence[AgentRequestToolsItemParams]]

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        max_iterations : typing.Optional[int]
            The maximum number of iterations the Agent can run. This is used to limit the number of times the Agent model is called.

        version_name : typing.Optional[str]
            Unique name for the Prompt version. Each Prompt can only have one version with a given name.

        version_description : typing.Optional[str]
            Description of the Version.

        description : typing.Optional[str]
            Description of the Prompt.

        tags : typing.Optional[typing.Sequence[str]]
            List of tags associated with this prompt.

        readme : typing.Optional[str]
            Long description of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.upsert(
            path="Banking/Teller Agent",
            provider="anthropic",
            endpoint="chat",
            model="claude-3-7-sonnet-latest",
            reasoning_effort=1024,
            template=[
                {
                    "role": "system",
                    "content": "You are a helpful digital assistant, helping users navigate our digital banking platform.",
                }
            ],
            max_iterations=3,
            tools=[
                {
                    "type": "inline",
                    "json_schema": {
                        "name": "stop",
                        "description": "Call this tool when you have finished your task.",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "output": {
                                    "type": "string",
                                    "description": "The final output to return to the user.",
                                }
                            },
                            "additionalProperties": False,
                            "required": ["output"],
                        },
                        "strict": True,
                    },
                    "on_agent_call": "stop",
                }
            ],
            version_name="teller-agent-v1",
            version_description="Initial version",
        )
        """
        response = self._raw_client.upsert(
            model=model,
            path=path,
            id=id,
            endpoint=endpoint,
            template=template,
            template_language=template_language,
            provider=provider,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            stop=stop,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            other=other,
            seed=seed,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            tools=tools,
            attributes=attributes,
            max_iterations=max_iterations,
            version_name=version_name,
            version_description=version_description,
            description=description,
            tags=tags,
            readme=readme,
            request_options=request_options,
        )
        return response.data

    def delete_agent_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : str
            Unique identifier for the specific version of the Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.delete_agent_version(
            id="ag_1234567890",
            version_id="agv_1234567890",
        )
        """
        response = self._raw_client.delete_agent_version(id, version_id, request_options=request_options)
        return response.data

    def patch_agent_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Update the name or description of the Agent version.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : str
            Unique identifier for the specific version of the Agent.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.patch_agent_version(
            id="ag_1234567890",
            version_id="agv_1234567890",
            name="teller-agent-v2",
            description="Updated version",
        )
        """
        response = self._raw_client.patch_agent_version(
            id, version_id, name=name, description=description, request_options=request_options
        )
        return response.data

    def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Retrieve the Agent with the given ID.

        By default, the deployed version of the Agent is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : typing.Optional[str]
            A specific Version ID of the Agent to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.get(
            id="ag_1234567890",
        )
        """
        response = self._raw_client.get(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Agent with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.delete(
            id="ag_1234567890",
        )
        """
        response = self._raw_client.delete(id, request_options=request_options)
        return response.data

    def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        directory_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Move the Agent to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        path : typing.Optional[str]
            Path of the Flow including the Flow name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Flow.

        directory_id : typing.Optional[str]
            Unique identifier for the Directory to move Flow to. Starts with `dir_`.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.move(
            id="ag_1234567890",
            path="new directory/new name",
        )
        """
        response = self._raw_client.move(
            id, path=path, name=name, directory_id=directory_id, request_options=request_options
        )
        return response.data

    def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListAgents:
        """
        Get a list of all the versions of a Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListAgents
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.list_versions(
            id="ag_1234567890",
        )
        """
        response = self._raw_client.list_versions(
            id, evaluator_aggregates=evaluator_aggregates, request_options=request_options
        )
        return response.data

    def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> AgentResponse:
        """
        Deploy Agent to an Environment.

        Set the deployed version for the specified Environment. This Agent
        will be used for calls made to the Agent in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.set_deployment(
            id="id",
            environment_id="environment_id",
            version_id="version_id",
        )
        """
        response = self._raw_client.set_deployment(
            id, environment_id, version_id=version_id, request_options=request_options
        )
        return response.data

    def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Agent from the Environment.

        Remove the deployed version for the specified Environment. This Agent
        will no longer be used for calls made to the Agent in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.remove_deployment(
            id="id",
            environment_id="environment_id",
        )
        """
        response = self._raw_client.remove_deployment(id, environment_id, request_options=request_options)
        return response.data

    def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.list_environments(
            id="ag_1234567890",
        )
        """
        response = self._raw_client.list_environments(id, request_options=request_options)
        return response.data

    def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Activate and deactivate Evaluators for monitoring the Agent.

        An activated Evaluator will automatically be run on all new Logs
        within the Agent for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.update_monitoring(
            id="ag_1234567890",
            activate=[
                {"evaluator_version_id": "ev_1234567890"},
                {"evaluator_id": "ev_2345678901", "environment_id": "env_1234567890"},
            ],
            deactivate=[{"evaluator_version_id": "ev_0987654321"}],
        )
        """
        response = self._raw_client.update_monitoring(
            id, activate=activate, deactivate=deactivate, request_options=request_options
        )
        return response.data

    def serialize(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Serialize an Agent to the .agent file format.

        Useful for storing the Agent with your code in a version control system,
        or for editing with an AI tool.

        By default, the deployed version of the Agent is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : typing.Optional[str]
            A specific Version ID of the Agent to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.serialize(
            id="id",
        )
        """
        response = self._raw_client.serialize(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    def deserialize(self, *, agent: str, request_options: typing.Optional[RequestOptions] = None) -> AgentKernelRequest:
        """
        Deserialize an Agent from the .agent file format.

        This returns a subset of the attributes required by an Agent.
        This subset is the bit that defines the Agent version (e.g. with `model` and `temperature` etc)

        Parameters
        ----------
        agent : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentKernelRequest
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.agents.deserialize(
            agent="agent",
        )
        """
        response = self._raw_client.deserialize(agent=agent, request_options=request_options)
        return response.data


class AsyncAgentsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawAgentsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawAgentsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawAgentsClient
        """
        return self._raw_client

    async def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        run_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentLogRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agent_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreateAgentLogResponse:
        """
        Create an Agent Log.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise, the default deployed version will be chosen.

        If you create the Agent Log with a `log_status` of `incomplete`, you should later update it to `complete`
        in order to trigger Evaluators.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        run_id : typing.Optional[str]
            Unique identifier for the Run to associate the Log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agent_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreateAgentLogResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.log(
                path="Banking/Teller Agent",
                agent={
                    "provider": "anthropic",
                    "endpoint": "chat",
                    "model": "claude-3-7-sonnet-latest",
                    "reasoning_effort": 1024,
                    "template": [
                        {
                            "role": "system",
                            "content": "You are a helpful digital assistant, helping users navigate our digital banking platform.",
                        }
                    ],
                    "max_iterations": 3,
                    "tools": [
                        {
                            "type": "file",
                            "link": {
                                "file_id": "pr_1234567890",
                                "version_id": "prv_1234567890",
                            },
                            "on_agent_call": "continue",
                        },
                        {
                            "type": "inline",
                            "json_schema": {
                                "name": "stop",
                                "description": "Call this tool when you have finished your task.",
                                "parameters": {
                                    "type": "object",
                                    "properties": {
                                        "output": {
                                            "type": "string",
                                            "description": "The final output to return to the user.",
                                        }
                                    },
                                    "additionalProperties": False,
                                    "required": ["output"],
                                },
                                "strict": True,
                            },
                            "on_agent_call": "stop",
                        },
                    ],
                },
            )


        asyncio.run(main())
        """
        response = await self._raw_client.log(
            version_id=version_id,
            environment=environment,
            run_id=run_id,
            path=path,
            id=id,
            output_message=output_message,
            prompt_tokens=prompt_tokens,
            reasoning_tokens=reasoning_tokens,
            output_tokens=output_tokens,
            prompt_cost=prompt_cost,
            output_cost=output_cost,
            finish_reason=finish_reason,
            messages=messages,
            tool_choice=tool_choice,
            agent=agent,
            start_time=start_time,
            end_time=end_time,
            output=output,
            created_at=created_at,
            error=error,
            provider_latency=provider_latency,
            stdout=stdout,
            provider_request=provider_request,
            provider_response=provider_response,
            inputs=inputs,
            source=source,
            metadata=metadata,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            agent_log_request_environment=agent_log_request_environment,
            save=save,
            log_id=log_id,
            request_options=request_options,
        )
        return response.data

    async def update_log(
        self,
        id: str,
        log_id: str,
        *,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        output: typing.Optional[str] = OMIT,
        error: typing.Optional[str] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentLogResponse:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        log_id : str
            Unique identifier for the Log.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            List of chat messages that were used as an input to the Flow.

        output_message : typing.Optional[ChatMessageParams]
            The output message returned by this Flow.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the Flow Log.

        output : typing.Optional[str]
            The output of the Flow Log. Provide None to unset existing `output` value. Provide either this, `output_message` or `error`.

        error : typing.Optional[str]
            The error message of the Flow Log. Provide None to unset existing `error` value. Provide either this, `output_message` or `output`.

        log_status : typing.Optional[LogStatus]
            Status of the Flow Log. When a Flow Log is updated to `complete`, no more Logs can be added to it. Monitoring Evaluators will only run on `complete` Flow Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentLogResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.update_log(
                id="ag_1234567890",
                log_id="log_1234567890",
                messages=[
                    {"role": "user", "content": "I need to withdraw $1000"},
                    {
                        "role": "assistant",
                        "content": "Of course! Would you like to use your savings or checking account?",
                    },
                ],
                output_message={
                    "role": "assistant",
                    "content": "I'm sorry, I can't help with that.",
                },
                log_status="complete",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.update_log(
            id,
            log_id,
            messages=messages,
            output_message=output_message,
            inputs=inputs,
            output=output,
            error=error,
            log_status=log_status,
            request_options=request_options,
        )
        return response.data

    async def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentsCallStreamRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agents_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[AgentCallStreamResponse]:
        """
        Call an Agent. The Agent will run on the Humanloop runtime and return a completed Agent Log.

        If the Agent requires a tool call that cannot be ran by Humanloop, execution will halt. To continue,
        pass the ID of the incomplete Log and the required tool call to the /agents/continue endpoint.

        The agent will run for the maximum number of iterations, or until it encounters a stop condition,
        according to its configuration.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Agent details in the request body. A new version is created if it does not match
        any existing ones. This is helpful in the case where you are storing or deriving
        your Agent details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agents_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Only applies when not streaming. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[AgentCallStreamResponse]


        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.agents.call_stream()
            async for chunk in response:
                yield chunk


        asyncio.run(main())
        """
        async with self._raw_client.call_stream(
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            messages=messages,
            tool_choice=tool_choice,
            agent=agent,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            agents_call_stream_request_environment=agents_call_stream_request_environment,
            save=save,
            log_id=log_id,
            provider_api_keys=provider_api_keys,
            return_inputs=return_inputs,
            include_trace_children=include_trace_children,
            request_options=request_options,
        ) as r:
            async for data in r.data:
                yield data

    async def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentsCallRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agents_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentCallResponse:
        """
        Call an Agent. The Agent will run on the Humanloop runtime and return a completed Agent Log.

        If the Agent requires a tool call that cannot be ran by Humanloop, execution will halt. To continue,
        pass the ID of the incomplete Log and the required tool call to the /agents/continue endpoint.

        The agent will run for the maximum number of iterations, or until it encounters a stop condition,
        according to its configuration.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Agent details in the request body. A new version is created if it does not match
        any existing ones. This is helpful in the case where you are storing or deriving
        your Agent details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agents_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Only applies when not streaming. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentCallResponse


        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.call(
                path="Banking/Teller Agent",
                messages=[
                    {
                        "role": "user",
                        "content": "I'd like to deposit $1000 to my savings account from my checking account.",
                    }
                ],
            )


        asyncio.run(main())
        """
        response = await self._raw_client.call(
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            messages=messages,
            tool_choice=tool_choice,
            agent=agent,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            agents_call_request_environment=agents_call_request_environment,
            save=save,
            log_id=log_id,
            provider_api_keys=provider_api_keys,
            return_inputs=return_inputs,
            include_trace_children=include_trace_children,
            request_options=request_options,
        )
        return response.data

    async def continue_call_stream(
        self,
        *,
        log_id: str,
        messages: typing.Sequence[ChatMessageParams],
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[AgentContinueCallStreamResponse]:
        """
        Continue an incomplete Agent call.

        This endpoint allows continuing an existing incomplete Agent call, by passing the tool call
        requested by the Agent. The Agent will resume processing from where it left off.

        The messages in the request will be appended to the original messages in the Log. You do not
        have to provide the previous conversation history.

        The original log must be in an incomplete state to be continued.

        Parameters
        ----------
        log_id : str
            This identifies the Agent Log to continue.

        messages : typing.Sequence[ChatMessageParams]
            The additional messages with which to continue the Agent Log. Often, these should start with the Tool messages with results for the previous Assistant message's tool calls.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[AgentContinueCallStreamResponse]


        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.agents.continue_call_stream(
                log_id="log_id",
                messages=[{"role": "user"}],
            )
            async for chunk in response:
                yield chunk


        asyncio.run(main())
        """
        async with self._raw_client.continue_call_stream(
            log_id=log_id,
            messages=messages,
            provider_api_keys=provider_api_keys,
            include_trace_children=include_trace_children,
            request_options=request_options,
        ) as r:
            async for data in r.data:
                yield data

    async def continue_call(
        self,
        *,
        log_id: str,
        messages: typing.Sequence[ChatMessageParams],
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentContinueCallResponse:
        """
        Continue an incomplete Agent call.

        This endpoint allows continuing an existing incomplete Agent call, by passing the tool call
        requested by the Agent. The Agent will resume processing from where it left off.

        The messages in the request will be appended to the original messages in the Log. You do not
        have to provide the previous conversation history.

        The original log must be in an incomplete state to be continued.

        Parameters
        ----------
        log_id : str
            This identifies the Agent Log to continue.

        messages : typing.Sequence[ChatMessageParams]
            The additional messages with which to continue the Agent Log. Often, these should start with the Tool messages with results for the previous Assistant message's tool calls.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentContinueCallResponse


        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.continue_call(
                log_id="log_1234567890",
                messages=[
                    {
                        "role": "tool",
                        "content": '{"type": "checking", "balance": 5200}',
                        "tool_call_id": "tc_1234567890",
                    }
                ],
            )


        asyncio.run(main())
        """
        response = await self._raw_client.continue_call(
            log_id=log_id,
            messages=messages,
            provider_api_keys=provider_api_keys,
            include_trace_children=include_trace_children,
            request_options=request_options,
        )
        return response.data

    async def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[ProjectSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncPager[AgentResponse]:
        """
        Get a list of all Agents.

        Parameters
        ----------
        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Agents to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Agent name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Agent. This filter matches against both email address and name of users.

        sort_by : typing.Optional[ProjectSortBy]
            Field to sort Agents by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncPager[AgentResponse]
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.agents.list(
                size=1,
            )
            async for item in response:
                yield item
            # alternatively, you can paginate page-by-page
            async for page in response.iter_pages():
                yield page


        asyncio.run(main())
        """
        page = page if page is not None else 1
        _response = await self._raw_client._client_wrapper.httpx_client.request(
            "agents",
            method="GET",
            params={
                "page": page,
                "size": size,
                "name": name,
                "user_filter": user_filter,
                "sort_by": sort_by,
                "order": order,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(
                    PaginatedDataAgentResponse,
                    construct_type(
                        type_=PaginatedDataAgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                _has_next = True
                _get_next = lambda: self.list(
                    page=page + 1,
                    size=size,
                    name=name,
                    user_filter=user_filter,
                    sort_by=sort_by,
                    order=order,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return AsyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[AgentRequestTemplateParams] = OMIT,
        template_language: typing.Optional[TemplateLanguage] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[AgentRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        reasoning_effort: typing.Optional[AgentRequestReasoningEffortParams] = OMIT,
        tools: typing.Optional[typing.Sequence[AgentRequestToolsItemParams]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        max_iterations: typing.Optional[int] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        tags: typing.Optional[typing.Sequence[str]] = OMIT,
        readme: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Create an Agent or update it with a new version if it already exists.

        Agents are identified by the `ID` or their `path`. The parameters (i.e. the template, temperature, model etc.) and
        tools determine the versions of the Agent.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within an Agent - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[AgentRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        template_language : typing.Optional[TemplateLanguage]
            The template language to use for rendering the template.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[AgentRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        reasoning_effort : typing.Optional[AgentRequestReasoningEffortParams]
            Guidance on how many reasoning tokens it should generate before creating a response to the prompt. OpenAI reasoning models (o1, o3-mini) expect a OpenAIReasoningEffort enum. Anthropic reasoning models expect an integer, which signifies the maximum token budget.

        tools : typing.Optional[typing.Sequence[AgentRequestToolsItemParams]]

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        max_iterations : typing.Optional[int]
            The maximum number of iterations the Agent can run. This is used to limit the number of times the Agent model is called.

        version_name : typing.Optional[str]
            Unique name for the Prompt version. Each Prompt can only have one version with a given name.

        version_description : typing.Optional[str]
            Description of the Version.

        description : typing.Optional[str]
            Description of the Prompt.

        tags : typing.Optional[typing.Sequence[str]]
            List of tags associated with this prompt.

        readme : typing.Optional[str]
            Long description of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.upsert(
                path="Banking/Teller Agent",
                provider="anthropic",
                endpoint="chat",
                model="claude-3-7-sonnet-latest",
                reasoning_effort=1024,
                template=[
                    {
                        "role": "system",
                        "content": "You are a helpful digital assistant, helping users navigate our digital banking platform.",
                    }
                ],
                max_iterations=3,
                tools=[
                    {
                        "type": "inline",
                        "json_schema": {
                            "name": "stop",
                            "description": "Call this tool when you have finished your task.",
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    "output": {
                                        "type": "string",
                                        "description": "The final output to return to the user.",
                                    }
                                },
                                "additionalProperties": False,
                                "required": ["output"],
                            },
                            "strict": True,
                        },
                        "on_agent_call": "stop",
                    }
                ],
                version_name="teller-agent-v1",
                version_description="Initial version",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.upsert(
            model=model,
            path=path,
            id=id,
            endpoint=endpoint,
            template=template,
            template_language=template_language,
            provider=provider,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            stop=stop,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            other=other,
            seed=seed,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            tools=tools,
            attributes=attributes,
            max_iterations=max_iterations,
            version_name=version_name,
            version_description=version_description,
            description=description,
            tags=tags,
            readme=readme,
            request_options=request_options,
        )
        return response.data

    async def delete_agent_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : str
            Unique identifier for the specific version of the Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.delete_agent_version(
                id="ag_1234567890",
                version_id="agv_1234567890",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.delete_agent_version(id, version_id, request_options=request_options)
        return response.data

    async def patch_agent_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Update the name or description of the Agent version.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : str
            Unique identifier for the specific version of the Agent.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.patch_agent_version(
                id="ag_1234567890",
                version_id="agv_1234567890",
                name="teller-agent-v2",
                description="Updated version",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.patch_agent_version(
            id, version_id, name=name, description=description, request_options=request_options
        )
        return response.data

    async def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Retrieve the Agent with the given ID.

        By default, the deployed version of the Agent is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : typing.Optional[str]
            A specific Version ID of the Agent to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.get(
                id="ag_1234567890",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.get(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    async def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Agent with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.delete(
                id="ag_1234567890",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.delete(id, request_options=request_options)
        return response.data

    async def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        directory_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Move the Agent to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        path : typing.Optional[str]
            Path of the Flow including the Flow name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Flow.

        directory_id : typing.Optional[str]
            Unique identifier for the Directory to move Flow to. Starts with `dir_`.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.move(
                id="ag_1234567890",
                path="new directory/new name",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.move(
            id, path=path, name=name, directory_id=directory_id, request_options=request_options
        )
        return response.data

    async def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListAgents:
        """
        Get a list of all the versions of a Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListAgents
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.list_versions(
                id="ag_1234567890",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.list_versions(
            id, evaluator_aggregates=evaluator_aggregates, request_options=request_options
        )
        return response.data

    async def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> AgentResponse:
        """
        Deploy Agent to an Environment.

        Set the deployed version for the specified Environment. This Agent
        will be used for calls made to the Agent in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.set_deployment(
                id="id",
                environment_id="environment_id",
                version_id="version_id",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.set_deployment(
            id, environment_id, version_id=version_id, request_options=request_options
        )
        return response.data

    async def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Agent from the Environment.

        Remove the deployed version for the specified Environment. This Agent
        will no longer be used for calls made to the Agent in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.remove_deployment(
                id="id",
                environment_id="environment_id",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.remove_deployment(id, environment_id, request_options=request_options)
        return response.data

    async def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.list_environments(
                id="ag_1234567890",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.list_environments(id, request_options=request_options)
        return response.data

    async def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AgentResponse:
        """
        Activate and deactivate Evaluators for monitoring the Agent.

        An activated Evaluator will automatically be run on all new Logs
        within the Agent for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.update_monitoring(
                id="ag_1234567890",
                activate=[
                    {"evaluator_version_id": "ev_1234567890"},
                    {
                        "evaluator_id": "ev_2345678901",
                        "environment_id": "env_1234567890",
                    },
                ],
                deactivate=[{"evaluator_version_id": "ev_0987654321"}],
            )


        asyncio.run(main())
        """
        response = await self._raw_client.update_monitoring(
            id, activate=activate, deactivate=deactivate, request_options=request_options
        )
        return response.data

    async def serialize(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Serialize an Agent to the .agent file format.

        Useful for storing the Agent with your code in a version control system,
        or for editing with an AI tool.

        By default, the deployed version of the Agent is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : typing.Optional[str]
            A specific Version ID of the Agent to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.serialize(
                id="id",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.serialize(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    async def deserialize(
        self, *, agent: str, request_options: typing.Optional[RequestOptions] = None
    ) -> AgentKernelRequest:
        """
        Deserialize an Agent from the .agent file format.

        This returns a subset of the attributes required by an Agent.
        This subset is the bit that defines the Agent version (e.g. with `model` and `temperature` etc)

        Parameters
        ----------
        agent : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AgentKernelRequest
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.agents.deserialize(
                agent="agent",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.deserialize(agent=agent, request_options=request_options)
        return response.data
