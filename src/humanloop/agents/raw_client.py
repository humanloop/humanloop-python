# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from ..requests.chat_message import ChatMessageParams
from .requests.agent_log_request_tool_choice import AgentLogRequestToolChoiceParams
from ..requests.agent_kernel_request import AgentKernelRequestParams
import datetime as dt
from ..types.log_status import LogStatus
from ..core.request_options import RequestOptions
from ..core.http_response import HttpResponse
from ..types.create_agent_log_response import CreateAgentLogResponse
from ..core.serialization import convert_and_respect_annotation_metadata
from ..core.unchecked_base_model import construct_type
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from ..types.agent_log_response import AgentLogResponse
from ..core.jsonable_encoder import jsonable_encoder
from .requests.agents_call_stream_request_tool_choice import AgentsCallStreamRequestToolChoiceParams
from ..requests.provider_api_keys import ProviderApiKeysParams
from ..types.agent_call_stream_response import AgentCallStreamResponse
import httpx_sse
import contextlib
from .requests.agents_call_request_tool_choice import AgentsCallRequestToolChoiceParams
from ..types.agent_call_response import AgentCallResponse
from ..types.agent_continue_call_stream_response import AgentContinueCallStreamResponse
from ..types.agent_continue_call_response import AgentContinueCallResponse
from ..types.model_endpoints import ModelEndpoints
from .requests.agent_request_template import AgentRequestTemplateParams
from ..types.template_language import TemplateLanguage
from ..types.model_providers import ModelProviders
from .requests.agent_request_stop import AgentRequestStopParams
from ..requests.response_format import ResponseFormatParams
from .requests.agent_request_reasoning_effort import AgentRequestReasoningEffortParams
from .requests.agent_request_tools_item import AgentRequestToolsItemParams
from ..types.agent_response import AgentResponse
from ..types.list_agents import ListAgents
from ..types.file_environment_response import FileEnvironmentResponse
from ..requests.evaluator_activation_deactivation_request_activate_item import (
    EvaluatorActivationDeactivationRequestActivateItemParams,
)
from ..requests.evaluator_activation_deactivation_request_deactivate_item import (
    EvaluatorActivationDeactivationRequestDeactivateItemParams,
)
from ..types.agent_kernel_request import AgentKernelRequest
from ..core.client_wrapper import AsyncClientWrapper
from ..core.http_response import AsyncHttpResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RawAgentsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        run_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentLogRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agent_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[CreateAgentLogResponse]:
        """
        Create an Agent Log.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise, the default deployed version will be chosen.

        If you create the Agent Log with a `log_status` of `incomplete`, you should later update it to `complete`
        in order to trigger Evaluators.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        run_id : typing.Optional[str]
            Unique identifier for the Run to associate the Log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agent_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[CreateAgentLogResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            "agents/log",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "run_id": run_id,
                "path": path,
                "id": id,
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "reasoning_tokens": reasoning_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=AgentLogRequestToolChoiceParams, direction="write"
                ),
                "agent": convert_and_respect_annotation_metadata(
                    object_=agent, annotation=AgentKernelRequestParams, direction="write"
                ),
                "start_time": start_time,
                "end_time": end_time,
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": agent_log_request_environment,
                "save": save,
                "log_id": log_id,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    CreateAgentLogResponse,
                    construct_type(
                        type_=CreateAgentLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_log(
        self,
        id: str,
        log_id: str,
        *,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        output: typing.Optional[str] = OMIT,
        error: typing.Optional[str] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[AgentLogResponse]:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        log_id : str
            Unique identifier for the Log.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            List of chat messages that were used as an input to the Flow.

        output_message : typing.Optional[ChatMessageParams]
            The output message returned by this Flow.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the Flow Log.

        output : typing.Optional[str]
            The output of the Flow Log. Provide None to unset existing `output` value. Provide either this, `output_message` or `error`.

        error : typing.Optional[str]
            The error message of the Flow Log. Provide None to unset existing `error` value. Provide either this, `output_message` or `output`.

        log_status : typing.Optional[LogStatus]
            Status of the Flow Log. When a Flow Log is updated to `complete`, no more Logs can be added to it. Monitoring Evaluators will only run on `complete` Flow Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentLogResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/log/{jsonable_encoder(log_id)}",
            method="PATCH",
            json={
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "inputs": inputs,
                "output": output,
                "error": error,
                "log_status": log_status,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentLogResponse,
                    construct_type(
                        type_=AgentLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    @contextlib.contextmanager
    def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentsCallStreamRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agents_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[HttpResponse[typing.Iterator[AgentCallStreamResponse]]]:
        """
        Call an Agent. The Agent will run on the Humanloop runtime and return a completed Agent Log.

        If the Agent requires a tool call that cannot be ran by Humanloop, execution will halt. To continue,
        pass the ID of the incomplete Log and the required tool call to the /agents/continue endpoint.

        The agent will run for the maximum number of iterations, or until it encounters a stop condition,
        according to its configuration.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Agent details in the request body. A new version is created if it does not match
        any existing ones. This is helpful in the case where you are storing or deriving
        your Agent details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agents_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Only applies when not streaming. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[HttpResponse[typing.Iterator[AgentCallStreamResponse]]]

        """
        with self._client_wrapper.httpx_client.stream(
            "agents/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=AgentsCallStreamRequestToolChoiceParams, direction="write"
                ),
                "agent": convert_and_respect_annotation_metadata(
                    object_=agent, annotation=AgentKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": agents_call_stream_request_environment,
                "save": save,
                "log_id": log_id,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "return_inputs": return_inputs,
                "include_trace_children": include_trace_children,
                "stream": True,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:

            def stream() -> HttpResponse[typing.Iterator[AgentCallStreamResponse]]:
                try:
                    if 200 <= _response.status_code < 300:

                        def _iter():
                            _event_source = httpx_sse.EventSource(_response)
                            for _sse in _event_source.iter_sse():
                                if _sse.data == None:
                                    return
                                try:
                                    yield _sse.data
                                except Exception:
                                    pass
                            return

                        return HttpResponse(response=_response, data=_iter())
                    _response.read()
                    if _response.status_code == 422:
                        raise UnprocessableEntityError(
                            typing.cast(
                                HttpValidationError,
                                construct_type(
                                    type_=HttpValidationError,  # type: ignore
                                    object_=_response.json(),
                                ),
                            )
                        )
                    _response_json = _response.json()
                except JSONDecodeError:
                    raise ApiError(status_code=_response.status_code, body=_response.text)
                raise ApiError(status_code=_response.status_code, body=_response_json)

            yield stream()

    def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentsCallRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agents_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[AgentCallResponse]:
        """
        Call an Agent. The Agent will run on the Humanloop runtime and return a completed Agent Log.

        If the Agent requires a tool call that cannot be ran by Humanloop, execution will halt. To continue,
        pass the ID of the incomplete Log and the required tool call to the /agents/continue endpoint.

        The agent will run for the maximum number of iterations, or until it encounters a stop condition,
        according to its configuration.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Agent details in the request body. A new version is created if it does not match
        any existing ones. This is helpful in the case where you are storing or deriving
        your Agent details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agents_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Only applies when not streaming. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentCallResponse]

        """
        _response = self._client_wrapper.httpx_client.request(
            "agents/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=AgentsCallRequestToolChoiceParams, direction="write"
                ),
                "agent": convert_and_respect_annotation_metadata(
                    object_=agent, annotation=AgentKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": agents_call_request_environment,
                "save": save,
                "log_id": log_id,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "return_inputs": return_inputs,
                "include_trace_children": include_trace_children,
                "stream": False,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentCallResponse,
                    construct_type(
                        type_=AgentCallResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    @contextlib.contextmanager
    def continue_call_stream(
        self,
        *,
        log_id: str,
        messages: typing.Sequence[ChatMessageParams],
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[HttpResponse[typing.Iterator[AgentContinueCallStreamResponse]]]:
        """
        Continue an incomplete Agent call.

        This endpoint allows continuing an existing incomplete Agent call, by passing the tool call
        requested by the Agent. The Agent will resume processing from where it left off.

        The messages in the request will be appended to the original messages in the Log. You do not
        have to provide the previous conversation history.

        The original log must be in an incomplete state to be continued.

        Parameters
        ----------
        log_id : str
            This identifies the Agent Log to continue.

        messages : typing.Sequence[ChatMessageParams]
            The additional messages with which to continue the Agent Log. Often, these should start with the Tool messages with results for the previous Assistant message's tool calls.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[HttpResponse[typing.Iterator[AgentContinueCallStreamResponse]]]

        """
        with self._client_wrapper.httpx_client.stream(
            "agents/continue",
            method="POST",
            json={
                "log_id": log_id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "include_trace_children": include_trace_children,
                "stream": True,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:

            def stream() -> HttpResponse[typing.Iterator[AgentContinueCallStreamResponse]]:
                try:
                    if 200 <= _response.status_code < 300:

                        def _iter():
                            _event_source = httpx_sse.EventSource(_response)
                            for _sse in _event_source.iter_sse():
                                if _sse.data == None:
                                    return
                                try:
                                    yield _sse.data
                                except Exception:
                                    pass
                            return

                        return HttpResponse(response=_response, data=_iter())
                    _response.read()
                    if _response.status_code == 422:
                        raise UnprocessableEntityError(
                            typing.cast(
                                HttpValidationError,
                                construct_type(
                                    type_=HttpValidationError,  # type: ignore
                                    object_=_response.json(),
                                ),
                            )
                        )
                    _response_json = _response.json()
                except JSONDecodeError:
                    raise ApiError(status_code=_response.status_code, body=_response.text)
                raise ApiError(status_code=_response.status_code, body=_response_json)

            yield stream()

    def continue_call(
        self,
        *,
        log_id: str,
        messages: typing.Sequence[ChatMessageParams],
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[AgentContinueCallResponse]:
        """
        Continue an incomplete Agent call.

        This endpoint allows continuing an existing incomplete Agent call, by passing the tool call
        requested by the Agent. The Agent will resume processing from where it left off.

        The messages in the request will be appended to the original messages in the Log. You do not
        have to provide the previous conversation history.

        The original log must be in an incomplete state to be continued.

        Parameters
        ----------
        log_id : str
            This identifies the Agent Log to continue.

        messages : typing.Sequence[ChatMessageParams]
            The additional messages with which to continue the Agent Log. Often, these should start with the Tool messages with results for the previous Assistant message's tool calls.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentContinueCallResponse]

        """
        _response = self._client_wrapper.httpx_client.request(
            "agents/continue",
            method="POST",
            json={
                "log_id": log_id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "include_trace_children": include_trace_children,
                "stream": False,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentContinueCallResponse,
                    construct_type(
                        type_=AgentContinueCallResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[AgentRequestTemplateParams] = OMIT,
        template_language: typing.Optional[TemplateLanguage] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[AgentRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        reasoning_effort: typing.Optional[AgentRequestReasoningEffortParams] = OMIT,
        tools: typing.Optional[typing.Sequence[AgentRequestToolsItemParams]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        max_iterations: typing.Optional[int] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        tags: typing.Optional[typing.Sequence[str]] = OMIT,
        readme: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[AgentResponse]:
        """
        Create an Agent or update it with a new version if it already exists.

        Agents are identified by the `ID` or their `path`. The parameters (i.e. the template, temperature, model etc.) and
        tools determine the versions of the Agent.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within an Agent - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[AgentRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        template_language : typing.Optional[TemplateLanguage]
            The template language to use for rendering the template.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[AgentRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        reasoning_effort : typing.Optional[AgentRequestReasoningEffortParams]
            Guidance on how many reasoning tokens it should generate before creating a response to the prompt. OpenAI reasoning models (o1, o3-mini) expect a OpenAIReasoningEffort enum. Anthropic reasoning models expect an integer, which signifies the maximum token budget.

        tools : typing.Optional[typing.Sequence[AgentRequestToolsItemParams]]

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        max_iterations : typing.Optional[int]
            The maximum number of iterations the Agent can run. This is used to limit the number of times the Agent model is called.

        version_name : typing.Optional[str]
            Unique name for the Prompt version. Each Prompt can only have one version with a given name.

        version_description : typing.Optional[str]
            Description of the Version.

        description : typing.Optional[str]
            Description of the Prompt.

        tags : typing.Optional[typing.Sequence[str]]
            List of tags associated with this prompt.

        readme : typing.Optional[str]
            Long description of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            "agents",
            method="POST",
            json={
                "path": path,
                "id": id,
                "model": model,
                "endpoint": endpoint,
                "template": convert_and_respect_annotation_metadata(
                    object_=template, annotation=AgentRequestTemplateParams, direction="write"
                ),
                "template_language": template_language,
                "provider": provider,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": convert_and_respect_annotation_metadata(
                    object_=stop, annotation=AgentRequestStopParams, direction="write"
                ),
                "presence_penalty": presence_penalty,
                "frequency_penalty": frequency_penalty,
                "other": other,
                "seed": seed,
                "response_format": convert_and_respect_annotation_metadata(
                    object_=response_format, annotation=ResponseFormatParams, direction="write"
                ),
                "reasoning_effort": convert_and_respect_annotation_metadata(
                    object_=reasoning_effort, annotation=AgentRequestReasoningEffortParams, direction="write"
                ),
                "tools": convert_and_respect_annotation_metadata(
                    object_=tools, annotation=typing.Sequence[AgentRequestToolsItemParams], direction="write"
                ),
                "attributes": attributes,
                "max_iterations": max_iterations,
                "version_name": version_name,
                "version_description": version_description,
                "description": description,
                "tags": tags,
                "readme": readme,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_agent_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[None]:
        """
        Delete a version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : str
            Unique identifier for the specific version of the Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def patch_agent_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[AgentResponse]:
        """
        Update the name or description of the Agent version.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : str
            Unique identifier for the specific version of the Agent.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}",
            method="PATCH",
            json={
                "name": name,
                "description": description,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[AgentResponse]:
        """
        Retrieve the Agent with the given ID.

        By default, the deployed version of the Agent is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : typing.Optional[str]
            A specific Version ID of the Agent to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> HttpResponse[None]:
        """
        Delete the Agent with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        directory_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[AgentResponse]:
        """
        Move the Agent to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        path : typing.Optional[str]
            Path of the Flow including the Flow name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Flow.

        directory_id : typing.Optional[str]
            Unique identifier for the Directory to move Flow to. Starts with `dir_`.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "path": path,
                "name": name,
                "directory_id": directory_id,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[ListAgents]:
        """
        Get a list of all the versions of a Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[ListAgents]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/versions",
            method="GET",
            params={
                "evaluator_aggregates": evaluator_aggregates,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    ListAgents,
                    construct_type(
                        type_=ListAgents,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[AgentResponse]:
        """
        Deploy Agent to an Environment.

        Set the deployed version for the specified Environment. This Agent
        will be used for calls made to the Agent in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="POST",
            params={
                "version_id": version_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[None]:
        """
        Remove deployed Agent from the Environment.

        Remove the deployed version for the specified Environment. This Agent
        will no longer be used for calls made to the Agent in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[typing.List[FileEnvironmentResponse]]:
        """
        List all Environments and their deployed versions for the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[typing.List[FileEnvironmentResponse]]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/environments",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    typing.List[FileEnvironmentResponse],
                    construct_type(
                        type_=typing.List[FileEnvironmentResponse],  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[AgentResponse]:
        """
        Activate and deactivate Evaluators for monitoring the Agent.

        An activated Evaluator will automatically be run on all new Logs
        within the Agent for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/evaluators",
            method="POST",
            json={
                "activate": convert_and_respect_annotation_metadata(
                    object_=activate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams],
                    direction="write",
                ),
                "deactivate": convert_and_respect_annotation_metadata(
                    object_=deactivate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams],
                    direction="write",
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def serialize(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[None]:
        """
        Serialize an Agent to the .agent file format.

        Useful for storing the Agent with your code in a version control system,
        or for editing with an AI tool.

        By default, the deployed version of the Agent is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : typing.Optional[str]
            A specific Version ID of the Agent to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/serialize",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def deserialize(
        self, *, agent: str, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[AgentKernelRequest]:
        """
        Deserialize an Agent from the .agent file format.

        This returns a subset of the attributes required by an Agent.
        This subset is the bit that defines the Agent version (e.g. with `model` and `temperature` etc)

        Parameters
        ----------
        agent : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[AgentKernelRequest]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            "agents/deserialize",
            method="POST",
            json={
                "agent": agent,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentKernelRequest,
                    construct_type(
                        type_=AgentKernelRequest,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncRawAgentsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        run_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentLogRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agent_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[CreateAgentLogResponse]:
        """
        Create an Agent Log.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise, the default deployed version will be chosen.

        If you create the Agent Log with a `log_status` of `incomplete`, you should later update it to `complete`
        in order to trigger Evaluators.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        run_id : typing.Optional[str]
            Unique identifier for the Run to associate the Log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agent_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[CreateAgentLogResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            "agents/log",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "run_id": run_id,
                "path": path,
                "id": id,
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "reasoning_tokens": reasoning_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=AgentLogRequestToolChoiceParams, direction="write"
                ),
                "agent": convert_and_respect_annotation_metadata(
                    object_=agent, annotation=AgentKernelRequestParams, direction="write"
                ),
                "start_time": start_time,
                "end_time": end_time,
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": agent_log_request_environment,
                "save": save,
                "log_id": log_id,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    CreateAgentLogResponse,
                    construct_type(
                        type_=CreateAgentLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_log(
        self,
        id: str,
        log_id: str,
        *,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        output: typing.Optional[str] = OMIT,
        error: typing.Optional[str] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[AgentLogResponse]:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        log_id : str
            Unique identifier for the Log.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            List of chat messages that were used as an input to the Flow.

        output_message : typing.Optional[ChatMessageParams]
            The output message returned by this Flow.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the Flow Log.

        output : typing.Optional[str]
            The output of the Flow Log. Provide None to unset existing `output` value. Provide either this, `output_message` or `error`.

        error : typing.Optional[str]
            The error message of the Flow Log. Provide None to unset existing `error` value. Provide either this, `output_message` or `output`.

        log_status : typing.Optional[LogStatus]
            Status of the Flow Log. When a Flow Log is updated to `complete`, no more Logs can be added to it. Monitoring Evaluators will only run on `complete` Flow Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentLogResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/log/{jsonable_encoder(log_id)}",
            method="PATCH",
            json={
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "inputs": inputs,
                "output": output,
                "error": error,
                "log_status": log_status,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentLogResponse,
                    construct_type(
                        type_=AgentLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    @contextlib.asynccontextmanager
    async def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentsCallStreamRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agents_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[AsyncHttpResponse[typing.AsyncIterator[AgentCallStreamResponse]]]:
        """
        Call an Agent. The Agent will run on the Humanloop runtime and return a completed Agent Log.

        If the Agent requires a tool call that cannot be ran by Humanloop, execution will halt. To continue,
        pass the ID of the incomplete Log and the required tool call to the /agents/continue endpoint.

        The agent will run for the maximum number of iterations, or until it encounters a stop condition,
        according to its configuration.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Agent details in the request body. A new version is created if it does not match
        any existing ones. This is helpful in the case where you are storing or deriving
        your Agent details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agents_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Only applies when not streaming. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[AsyncHttpResponse[typing.AsyncIterator[AgentCallStreamResponse]]]

        """
        async with self._client_wrapper.httpx_client.stream(
            "agents/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=AgentsCallStreamRequestToolChoiceParams, direction="write"
                ),
                "agent": convert_and_respect_annotation_metadata(
                    object_=agent, annotation=AgentKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": agents_call_stream_request_environment,
                "save": save,
                "log_id": log_id,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "return_inputs": return_inputs,
                "include_trace_children": include_trace_children,
                "stream": True,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:

            async def stream() -> AsyncHttpResponse[typing.AsyncIterator[AgentCallStreamResponse]]:
                try:
                    if 200 <= _response.status_code < 300:

                        async def _iter():
                            _event_source = httpx_sse.EventSource(_response)
                            async for _sse in _event_source.aiter_sse():
                                if _sse.data == None:
                                    return
                                try:
                                    yield _sse.data
                                except Exception:
                                    pass
                            return

                        return AsyncHttpResponse(response=_response, data=_iter())
                    await _response.aread()
                    if _response.status_code == 422:
                        raise UnprocessableEntityError(
                            typing.cast(
                                HttpValidationError,
                                construct_type(
                                    type_=HttpValidationError,  # type: ignore
                                    object_=_response.json(),
                                ),
                            )
                        )
                    _response_json = _response.json()
                except JSONDecodeError:
                    raise ApiError(status_code=_response.status_code, body=_response.text)
                raise ApiError(status_code=_response.status_code, body=_response_json)

            yield await stream()

    async def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[AgentsCallRequestToolChoiceParams] = OMIT,
        agent: typing.Optional[AgentKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        agents_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[AgentCallResponse]:
        """
        Call an Agent. The Agent will run on the Humanloop runtime and return a completed Agent Log.

        If the Agent requires a tool call that cannot be ran by Humanloop, execution will halt. To continue,
        pass the ID of the incomplete Log and the required tool call to the /agents/continue endpoint.

        The agent will run for the maximum number of iterations, or until it encounters a stop condition,
        according to its configuration.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Agent. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Agent details in the request body. A new version is created if it does not match
        any existing ones. This is helpful in the case where you are storing or deriving
        your Agent details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Agent to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[AgentsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        agent : typing.Optional[AgentKernelRequestParams]
            Details of your Agent. A new Agent version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        agents_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Only applies when not streaming. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentCallResponse]

        """
        _response = await self._client_wrapper.httpx_client.request(
            "agents/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=AgentsCallRequestToolChoiceParams, direction="write"
                ),
                "agent": convert_and_respect_annotation_metadata(
                    object_=agent, annotation=AgentKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": agents_call_request_environment,
                "save": save,
                "log_id": log_id,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "return_inputs": return_inputs,
                "include_trace_children": include_trace_children,
                "stream": False,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentCallResponse,
                    construct_type(
                        type_=AgentCallResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    @contextlib.asynccontextmanager
    async def continue_call_stream(
        self,
        *,
        log_id: str,
        messages: typing.Sequence[ChatMessageParams],
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[AsyncHttpResponse[typing.AsyncIterator[AgentContinueCallStreamResponse]]]:
        """
        Continue an incomplete Agent call.

        This endpoint allows continuing an existing incomplete Agent call, by passing the tool call
        requested by the Agent. The Agent will resume processing from where it left off.

        The messages in the request will be appended to the original messages in the Log. You do not
        have to provide the previous conversation history.

        The original log must be in an incomplete state to be continued.

        Parameters
        ----------
        log_id : str
            This identifies the Agent Log to continue.

        messages : typing.Sequence[ChatMessageParams]
            The additional messages with which to continue the Agent Log. Often, these should start with the Tool messages with results for the previous Assistant message's tool calls.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[AsyncHttpResponse[typing.AsyncIterator[AgentContinueCallStreamResponse]]]

        """
        async with self._client_wrapper.httpx_client.stream(
            "agents/continue",
            method="POST",
            json={
                "log_id": log_id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "include_trace_children": include_trace_children,
                "stream": True,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:

            async def stream() -> AsyncHttpResponse[typing.AsyncIterator[AgentContinueCallStreamResponse]]:
                try:
                    if 200 <= _response.status_code < 300:

                        async def _iter():
                            _event_source = httpx_sse.EventSource(_response)
                            async for _sse in _event_source.aiter_sse():
                                if _sse.data == None:
                                    return
                                try:
                                    yield _sse.data
                                except Exception:
                                    pass
                            return

                        return AsyncHttpResponse(response=_response, data=_iter())
                    await _response.aread()
                    if _response.status_code == 422:
                        raise UnprocessableEntityError(
                            typing.cast(
                                HttpValidationError,
                                construct_type(
                                    type_=HttpValidationError,  # type: ignore
                                    object_=_response.json(),
                                ),
                            )
                        )
                    _response_json = _response.json()
                except JSONDecodeError:
                    raise ApiError(status_code=_response.status_code, body=_response.text)
                raise ApiError(status_code=_response.status_code, body=_response_json)

            yield await stream()

    async def continue_call(
        self,
        *,
        log_id: str,
        messages: typing.Sequence[ChatMessageParams],
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        include_trace_children: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[AgentContinueCallResponse]:
        """
        Continue an incomplete Agent call.

        This endpoint allows continuing an existing incomplete Agent call, by passing the tool call
        requested by the Agent. The Agent will resume processing from where it left off.

        The messages in the request will be appended to the original messages in the Log. You do not
        have to provide the previous conversation history.

        The original log must be in an incomplete state to be continued.

        Parameters
        ----------
        log_id : str
            This identifies the Agent Log to continue.

        messages : typing.Sequence[ChatMessageParams]
            The additional messages with which to continue the Agent Log. Often, these should start with the Tool messages with results for the previous Assistant message's tool calls.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        include_trace_children : typing.Optional[bool]
            If true, populate `trace_children` for the returned Agent Log. Defaults to false.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentContinueCallResponse]

        """
        _response = await self._client_wrapper.httpx_client.request(
            "agents/continue",
            method="POST",
            json={
                "log_id": log_id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "include_trace_children": include_trace_children,
                "stream": False,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentContinueCallResponse,
                    construct_type(
                        type_=AgentContinueCallResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[AgentRequestTemplateParams] = OMIT,
        template_language: typing.Optional[TemplateLanguage] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[AgentRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        reasoning_effort: typing.Optional[AgentRequestReasoningEffortParams] = OMIT,
        tools: typing.Optional[typing.Sequence[AgentRequestToolsItemParams]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        max_iterations: typing.Optional[int] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        tags: typing.Optional[typing.Sequence[str]] = OMIT,
        readme: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[AgentResponse]:
        """
        Create an Agent or update it with a new version if it already exists.

        Agents are identified by the `ID` or their `path`. The parameters (i.e. the template, temperature, model etc.) and
        tools determine the versions of the Agent.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within an Agent - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Agent.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[AgentRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        template_language : typing.Optional[TemplateLanguage]
            The template language to use for rendering the template.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[AgentRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        reasoning_effort : typing.Optional[AgentRequestReasoningEffortParams]
            Guidance on how many reasoning tokens it should generate before creating a response to the prompt. OpenAI reasoning models (o1, o3-mini) expect a OpenAIReasoningEffort enum. Anthropic reasoning models expect an integer, which signifies the maximum token budget.

        tools : typing.Optional[typing.Sequence[AgentRequestToolsItemParams]]

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        max_iterations : typing.Optional[int]
            The maximum number of iterations the Agent can run. This is used to limit the number of times the Agent model is called.

        version_name : typing.Optional[str]
            Unique name for the Prompt version. Each Prompt can only have one version with a given name.

        version_description : typing.Optional[str]
            Description of the Version.

        description : typing.Optional[str]
            Description of the Prompt.

        tags : typing.Optional[typing.Sequence[str]]
            List of tags associated with this prompt.

        readme : typing.Optional[str]
            Long description of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            "agents",
            method="POST",
            json={
                "path": path,
                "id": id,
                "model": model,
                "endpoint": endpoint,
                "template": convert_and_respect_annotation_metadata(
                    object_=template, annotation=AgentRequestTemplateParams, direction="write"
                ),
                "template_language": template_language,
                "provider": provider,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": convert_and_respect_annotation_metadata(
                    object_=stop, annotation=AgentRequestStopParams, direction="write"
                ),
                "presence_penalty": presence_penalty,
                "frequency_penalty": frequency_penalty,
                "other": other,
                "seed": seed,
                "response_format": convert_and_respect_annotation_metadata(
                    object_=response_format, annotation=ResponseFormatParams, direction="write"
                ),
                "reasoning_effort": convert_and_respect_annotation_metadata(
                    object_=reasoning_effort, annotation=AgentRequestReasoningEffortParams, direction="write"
                ),
                "tools": convert_and_respect_annotation_metadata(
                    object_=tools, annotation=typing.Sequence[AgentRequestToolsItemParams], direction="write"
                ),
                "attributes": attributes,
                "max_iterations": max_iterations,
                "version_name": version_name,
                "version_description": version_description,
                "description": description,
                "tags": tags,
                "readme": readme,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_agent_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[None]:
        """
        Delete a version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : str
            Unique identifier for the specific version of the Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def patch_agent_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[AgentResponse]:
        """
        Update the name or description of the Agent version.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : str
            Unique identifier for the specific version of the Agent.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}",
            method="PATCH",
            json={
                "name": name,
                "description": description,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[AgentResponse]:
        """
        Retrieve the Agent with the given ID.

        By default, the deployed version of the Agent is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : typing.Optional[str]
            A specific Version ID of the Agent to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[None]:
        """
        Delete the Agent with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        directory_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[AgentResponse]:
        """
        Move the Agent to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        path : typing.Optional[str]
            Path of the Flow including the Flow name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Flow.

        directory_id : typing.Optional[str]
            Unique identifier for the Directory to move Flow to. Starts with `dir_`.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "path": path,
                "name": name,
                "directory_id": directory_id,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[ListAgents]:
        """
        Get a list of all the versions of a Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[ListAgents]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/versions",
            method="GET",
            params={
                "evaluator_aggregates": evaluator_aggregates,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    ListAgents,
                    construct_type(
                        type_=ListAgents,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[AgentResponse]:
        """
        Deploy Agent to an Environment.

        Set the deployed version for the specified Environment. This Agent
        will be used for calls made to the Agent in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="POST",
            params={
                "version_id": version_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[None]:
        """
        Remove deployed Agent from the Environment.

        Remove the deployed version for the specified Environment. This Agent
        will no longer be used for calls made to the Agent in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[typing.List[FileEnvironmentResponse]]:
        """
        List all Environments and their deployed versions for the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[typing.List[FileEnvironmentResponse]]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/environments",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    typing.List[FileEnvironmentResponse],
                    construct_type(
                        type_=typing.List[FileEnvironmentResponse],  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[AgentResponse]:
        """
        Activate and deactivate Evaluators for monitoring the Agent.

        An activated Evaluator will automatically be run on all new Logs
        within the Agent for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/evaluators",
            method="POST",
            json={
                "activate": convert_and_respect_annotation_metadata(
                    object_=activate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams],
                    direction="write",
                ),
                "deactivate": convert_and_respect_annotation_metadata(
                    object_=deactivate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams],
                    direction="write",
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentResponse,
                    construct_type(
                        type_=AgentResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def serialize(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[None]:
        """
        Serialize an Agent to the .agent file format.

        Useful for storing the Agent with your code in a version control system,
        or for editing with an AI tool.

        By default, the deployed version of the Agent is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Agent.

        Parameters
        ----------
        id : str
            Unique identifier for Agent.

        version_id : typing.Optional[str]
            A specific Version ID of the Agent to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"agents/{jsonable_encoder(id)}/serialize",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def deserialize(
        self, *, agent: str, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[AgentKernelRequest]:
        """
        Deserialize an Agent from the .agent file format.

        This returns a subset of the attributes required by an Agent.
        This subset is the bit that defines the Agent version (e.g. with `model` and `temperature` etc)

        Parameters
        ----------
        agent : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[AgentKernelRequest]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            "agents/deserialize",
            method="POST",
            json={
                "agent": agent,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    AgentKernelRequest,
                    construct_type(
                        type_=AgentKernelRequest,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
