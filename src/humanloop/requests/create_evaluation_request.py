# This file was auto-generated by Fern from our API Definition.

import typing

import typing_extensions

from .evaluatee_request import EvaluateeRequestParams
from .evaluations_dataset_request import EvaluationsDatasetRequestParams
from .evaluations_request import EvaluationsRequestParams


class CreateEvaluationRequestParams(typing_extensions.TypedDict):
    """
    Request model for creating an Evaluation.

    Evaluation benchmark your Prompt/Tool Versions. With the Datapoints in a Dataset Version,
    Logs corresponding to the Datapoint and each Evaluated Version are evaluated by the specified Evaluator Versions.
    Aggregated statistics are then calculated and presented in the Evaluation.
    """

    dataset: EvaluationsDatasetRequestParams
    """
    The Dataset Version to use in this Evaluation.
    """

    evaluatees: typing.Sequence[EvaluateeRequestParams]
    """
    Unique identifiers for the Prompt/Tool Versions to include in the Evaluation Report.
    """

    evaluators: typing.Sequence[EvaluationsRequestParams]
    """
    The Evaluators used to evaluate.
    """
