# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from ..core.request_options import RequestOptions
from ..core.pagination import SyncPager
from ..types.evaluation_response import EvaluationResponse
from ..types.paginated_evaluation_response import PaginatedEvaluationResponse
from ..core.unchecked_base_model import construct_type
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from ..requests.evaluations_dataset_request import EvaluationsDatasetRequestParams
from ..requests.evaluations_request import EvaluationsRequestParams
from ..requests.evaluatee_request import EvaluateeRequestParams
from ..requests.file_request import FileRequestParams
from ..core.serialization import convert_and_respect_annotation_metadata
from ..core.jsonable_encoder import jsonable_encoder
from ..types.evaluation_status import EvaluationStatus
from ..types.evaluation_stats import EvaluationStats
from ..types.paginated_data_evaluation_report_log_response import PaginatedDataEvaluationReportLogResponse
from ..core.client_wrapper import AsyncClientWrapper
from ..core.pagination import AsyncPager

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class EvaluationsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list(
        self,
        *,
        file_id: str,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> SyncPager[EvaluationResponse]:
        """
        List all Evaluations for the specified `file_id`.

        Retrieve a list of Evaluations that evaluate versions of the specified File.

        Parameters
        ----------
        file_id : str
            Filter by File ID. Only Evaluations for the specified File will be returned.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Evaluations to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        SyncPager[EvaluationResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.evaluations.list(
            file_id="pr_30gco7dx6JDq4200GVOHa",
            size=1,
        )
        for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        for page in response.iter_pages():
            yield page
        """
        page = page if page is not None else 1
        _response = self._client_wrapper.httpx_client.request(
            "evaluations",
            method="GET",
            params={
                "file_id": file_id,
                "page": page,
                "size": size,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(
                    PaginatedEvaluationResponse,
                    construct_type(
                        type_=PaginatedEvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                _has_next = True
                _get_next = lambda: self.list(
                    file_id=file_id,
                    page=page + 1,
                    size=size,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return SyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create(
        self,
        *,
        dataset: EvaluationsDatasetRequestParams,
        evaluators: typing.Sequence[EvaluationsRequestParams],
        evaluatees: typing.Optional[typing.Sequence[EvaluateeRequestParams]] = OMIT,
        name: typing.Optional[str] = OMIT,
        file: typing.Optional[FileRequestParams] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Create an Evaluation.

        Create a new Evaluation by specifying the Dataset, versions to be
        evaluated (Evaluatees), and which Evaluators to provide judgments.

        Humanloop will automatically start generating Logs and running Evaluators where
        `orchestrated=true`. If you own the runtime for the Evaluatee or Evaluator, you
        can set `orchestrated=false` and then generate and submit the required logs using
        your runtime.

        To keep updated on the progress of the Evaluation, you can poll the Evaluation using
        the `GET /evaluations/:id` endpoint and check its status.

        Parameters
        ----------
        dataset : EvaluationsDatasetRequestParams
            Dataset to use in this Evaluation.

        evaluators : typing.Sequence[EvaluationsRequestParams]
            The Evaluators used to evaluate.

        evaluatees : typing.Optional[typing.Sequence[EvaluateeRequestParams]]
            Unique identifiers for the Prompt/Tool Versions to include in the Evaluation. Can be left unpopulated if you wish to add Evaluatees to this Evaluation by specifying `evaluation_id` in Log calls.

        name : typing.Optional[str]
            Name of the Evaluation to help identify it. Must be unique within the associated File.

        file : typing.Optional[FileRequestParams]
            The File to associate with the Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.create(
            dataset={"version_id": "dsv_6L78pqrdFi2xa"},
            evaluatees=[
                {"version_id": "prv_7ZlQREDScH0xkhUwtXruN", "orchestrated": False}
            ],
            evaluators=[{"version_id": "evv_012def", "orchestrated": False}],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "evaluations",
            method="POST",
            json={
                "dataset": convert_and_respect_annotation_metadata(
                    object_=dataset, annotation=EvaluationsDatasetRequestParams, direction="write"
                ),
                "evaluatees": convert_and_respect_annotation_metadata(
                    object_=evaluatees, annotation=typing.Sequence[EvaluateeRequestParams], direction="write"
                ),
                "evaluators": convert_and_respect_annotation_metadata(
                    object_=evaluators, annotation=typing.Sequence[EvaluationsRequestParams], direction="write"
                ),
                "name": name,
                "file": convert_and_respect_annotation_metadata(
                    object_=file, annotation=FileRequestParams, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationResponse:
        """
        Get an Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.get(
            id="ev_567yza",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete an Evaluation.

        Remove an Evaluation from Humanloop. The Logs and Versions used in the Evaluation
        will not be deleted.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.delete(
            id="ev_567yza",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_setup(
        self,
        id: str,
        *,
        dataset: typing.Optional[EvaluationsDatasetRequestParams] = OMIT,
        evaluatees: typing.Optional[typing.Sequence[EvaluateeRequestParams]] = OMIT,
        evaluators: typing.Optional[typing.Sequence[EvaluationsRequestParams]] = OMIT,
        name: typing.Optional[str] = OMIT,
        file: typing.Optional[FileRequestParams] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Update an Evaluation.

        Update the setup of an Evaluation by specifying the Dataset, versions to be
        evaluated (Evaluatees), and which Evaluators to provide judgments.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        dataset : typing.Optional[EvaluationsDatasetRequestParams]
            Dataset to use in this Evaluation.

        evaluatees : typing.Optional[typing.Sequence[EvaluateeRequestParams]]
            Unique identifiers for the Prompt/Tool Versions to include in the Evaluation. Can be left unpopulated if you wish to add evaluatees to this Evaluation by specifying `evaluation_id` in Log calls.

        evaluators : typing.Optional[typing.Sequence[EvaluationsRequestParams]]
            The Evaluators used to evaluate.

        name : typing.Optional[str]
            Name of the Evaluation to help identify it. Must be unique within the associated File.

        file : typing.Optional[FileRequestParams]
            The File to associate with the Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.update_setup(
            id="ev_567yza",
            dataset={"version_id": "dsv_6L78pqrdFi2xa"},
            evaluatees=[
                {"version_id": "prv_7ZlQREDScH0xkhUwtXruN", "orchestrated": False}
            ],
            evaluators=[{"version_id": "evv_012def", "orchestrated": False}],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "dataset": convert_and_respect_annotation_metadata(
                    object_=dataset, annotation=EvaluationsDatasetRequestParams, direction="write"
                ),
                "evaluatees": convert_and_respect_annotation_metadata(
                    object_=evaluatees, annotation=typing.Sequence[EvaluateeRequestParams], direction="write"
                ),
                "evaluators": convert_and_respect_annotation_metadata(
                    object_=evaluators, annotation=typing.Sequence[EvaluationsRequestParams], direction="write"
                ),
                "name": name,
                "file": convert_and_respect_annotation_metadata(
                    object_=file, annotation=FileRequestParams, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_status(
        self, id: str, *, status: EvaluationStatus, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationResponse:
        """
        Update the status of an Evaluation.

        Can be used to cancel a running Evaluation, or mark an Evaluation that uses
        external or human evaluators as completed.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        status : EvaluationStatus

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.update_status(
            id="id",
            status="pending",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/status",
            method="PATCH",
            json={
                "status": status,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_stats(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationStats:
        """
        Get Evaluation Stats.

        Retrieve aggregate stats for the specified Evaluation.
        This includes the number of generated Logs for each evaluated version and the
        corresponding Evaluator statistics (such as the mean and percentiles).

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationStats
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.get_stats(
            id="id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/stats",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationStats,
                    construct_type(
                        type_=EvaluationStats,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_logs(
        self,
        id: str,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedDataEvaluationReportLogResponse:
        """
        Get the Logs associated to a specific Evaluation.

        Each Datapoint in your Dataset will have a corresponding Log for each File version evaluated.
        e.g. If you have 50 Datapoints and are evaluating 2 Prompts, there will be 100 Logs associated with the Evaluation.

        Parameters
        ----------
        id : str
            String ID of evaluation. Starts with `ev_` or `evr_`.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Logs to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedDataEvaluationReportLogResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.get_logs(
            id="id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/logs",
            method="GET",
            params={
                "page": page,
                "size": size,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PaginatedDataEvaluationReportLogResponse,
                    construct_type(
                        type_=PaginatedDataEvaluationReportLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def pin_evaluatee(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        file_id: typing.Optional[str] = OMIT,
        environment: typing.Optional[str] = OMIT,
        batch_id: typing.Optional[str] = OMIT,
        orchestrated: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Pin the specified Evaluatee.

        Pinned Evaluatees are always displayed in the Evaluation Overview,
        and serve as the baseline for comparison with other Evaluatees.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        version_id : typing.Optional[str]
            Unique identifier for the File Version. If provided, none of the other fields should be specified.

        path : typing.Optional[str]
            Path identifying a File. Provide either this or `file_id` if you want to specify a File.

        file_id : typing.Optional[str]
            Unique identifier for the File. Provide either this or `path` if you want to specify a File.

        environment : typing.Optional[str]
            Name of the Environment a Version is deployed to. Only provide this when specifying a File. If not provided (and a File is specified), the default Environment is used.

        batch_id : typing.Optional[str]
            Unique identifier for the batch of Logs to include in the Evaluation Report.

        orchestrated : typing.Optional[bool]
            Whether the Prompt/Tool is orchestrated by Humanloop. Default is `True`. If `False`, a log for the Prompt/Tool should be submitted by the user via the API.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.pin_evaluatee(
            id="id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/pin-evaluatee",
            method="POST",
            json={
                "version_id": version_id,
                "path": path,
                "file_id": file_id,
                "environment": environment,
                "batch_id": batch_id,
                "orchestrated": orchestrated,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncEvaluationsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list(
        self,
        *,
        file_id: str,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncPager[EvaluationResponse]:
        """
        List all Evaluations for the specified `file_id`.

        Retrieve a list of Evaluations that evaluate versions of the specified File.

        Parameters
        ----------
        file_id : str
            Filter by File ID. Only Evaluations for the specified File will be returned.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Evaluations to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncPager[EvaluationResponse]
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.evaluations.list(
                file_id="pr_30gco7dx6JDq4200GVOHa",
                size=1,
            )
            async for item in response:
                yield item
            # alternatively, you can paginate page-by-page
            async for page in response.iter_pages():
                yield page


        asyncio.run(main())
        """
        page = page if page is not None else 1
        _response = await self._client_wrapper.httpx_client.request(
            "evaluations",
            method="GET",
            params={
                "file_id": file_id,
                "page": page,
                "size": size,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(
                    PaginatedEvaluationResponse,
                    construct_type(
                        type_=PaginatedEvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                _has_next = True
                _get_next = lambda: self.list(
                    file_id=file_id,
                    page=page + 1,
                    size=size,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return AsyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create(
        self,
        *,
        dataset: EvaluationsDatasetRequestParams,
        evaluators: typing.Sequence[EvaluationsRequestParams],
        evaluatees: typing.Optional[typing.Sequence[EvaluateeRequestParams]] = OMIT,
        name: typing.Optional[str] = OMIT,
        file: typing.Optional[FileRequestParams] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Create an Evaluation.

        Create a new Evaluation by specifying the Dataset, versions to be
        evaluated (Evaluatees), and which Evaluators to provide judgments.

        Humanloop will automatically start generating Logs and running Evaluators where
        `orchestrated=true`. If you own the runtime for the Evaluatee or Evaluator, you
        can set `orchestrated=false` and then generate and submit the required logs using
        your runtime.

        To keep updated on the progress of the Evaluation, you can poll the Evaluation using
        the `GET /evaluations/:id` endpoint and check its status.

        Parameters
        ----------
        dataset : EvaluationsDatasetRequestParams
            Dataset to use in this Evaluation.

        evaluators : typing.Sequence[EvaluationsRequestParams]
            The Evaluators used to evaluate.

        evaluatees : typing.Optional[typing.Sequence[EvaluateeRequestParams]]
            Unique identifiers for the Prompt/Tool Versions to include in the Evaluation. Can be left unpopulated if you wish to add Evaluatees to this Evaluation by specifying `evaluation_id` in Log calls.

        name : typing.Optional[str]
            Name of the Evaluation to help identify it. Must be unique within the associated File.

        file : typing.Optional[FileRequestParams]
            The File to associate with the Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.evaluations.create(
                dataset={"version_id": "dsv_6L78pqrdFi2xa"},
                evaluatees=[
                    {"version_id": "prv_7ZlQREDScH0xkhUwtXruN", "orchestrated": False}
                ],
                evaluators=[{"version_id": "evv_012def", "orchestrated": False}],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "evaluations",
            method="POST",
            json={
                "dataset": convert_and_respect_annotation_metadata(
                    object_=dataset, annotation=EvaluationsDatasetRequestParams, direction="write"
                ),
                "evaluatees": convert_and_respect_annotation_metadata(
                    object_=evaluatees, annotation=typing.Sequence[EvaluateeRequestParams], direction="write"
                ),
                "evaluators": convert_and_respect_annotation_metadata(
                    object_=evaluators, annotation=typing.Sequence[EvaluationsRequestParams], direction="write"
                ),
                "name": name,
                "file": convert_and_respect_annotation_metadata(
                    object_=file, annotation=FileRequestParams, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationResponse:
        """
        Get an Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.evaluations.get(
                id="ev_567yza",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete an Evaluation.

        Remove an Evaluation from Humanloop. The Logs and Versions used in the Evaluation
        will not be deleted.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.evaluations.delete(
                id="ev_567yza",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_setup(
        self,
        id: str,
        *,
        dataset: typing.Optional[EvaluationsDatasetRequestParams] = OMIT,
        evaluatees: typing.Optional[typing.Sequence[EvaluateeRequestParams]] = OMIT,
        evaluators: typing.Optional[typing.Sequence[EvaluationsRequestParams]] = OMIT,
        name: typing.Optional[str] = OMIT,
        file: typing.Optional[FileRequestParams] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Update an Evaluation.

        Update the setup of an Evaluation by specifying the Dataset, versions to be
        evaluated (Evaluatees), and which Evaluators to provide judgments.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        dataset : typing.Optional[EvaluationsDatasetRequestParams]
            Dataset to use in this Evaluation.

        evaluatees : typing.Optional[typing.Sequence[EvaluateeRequestParams]]
            Unique identifiers for the Prompt/Tool Versions to include in the Evaluation. Can be left unpopulated if you wish to add evaluatees to this Evaluation by specifying `evaluation_id` in Log calls.

        evaluators : typing.Optional[typing.Sequence[EvaluationsRequestParams]]
            The Evaluators used to evaluate.

        name : typing.Optional[str]
            Name of the Evaluation to help identify it. Must be unique within the associated File.

        file : typing.Optional[FileRequestParams]
            The File to associate with the Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.evaluations.update_setup(
                id="ev_567yza",
                dataset={"version_id": "dsv_6L78pqrdFi2xa"},
                evaluatees=[
                    {"version_id": "prv_7ZlQREDScH0xkhUwtXruN", "orchestrated": False}
                ],
                evaluators=[{"version_id": "evv_012def", "orchestrated": False}],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "dataset": convert_and_respect_annotation_metadata(
                    object_=dataset, annotation=EvaluationsDatasetRequestParams, direction="write"
                ),
                "evaluatees": convert_and_respect_annotation_metadata(
                    object_=evaluatees, annotation=typing.Sequence[EvaluateeRequestParams], direction="write"
                ),
                "evaluators": convert_and_respect_annotation_metadata(
                    object_=evaluators, annotation=typing.Sequence[EvaluationsRequestParams], direction="write"
                ),
                "name": name,
                "file": convert_and_respect_annotation_metadata(
                    object_=file, annotation=FileRequestParams, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_status(
        self, id: str, *, status: EvaluationStatus, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationResponse:
        """
        Update the status of an Evaluation.

        Can be used to cancel a running Evaluation, or mark an Evaluation that uses
        external or human evaluators as completed.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        status : EvaluationStatus

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.evaluations.update_status(
                id="id",
                status="pending",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/status",
            method="PATCH",
            json={
                "status": status,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_stats(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationStats:
        """
        Get Evaluation Stats.

        Retrieve aggregate stats for the specified Evaluation.
        This includes the number of generated Logs for each evaluated version and the
        corresponding Evaluator statistics (such as the mean and percentiles).

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationStats
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.evaluations.get_stats(
                id="id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/stats",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationStats,
                    construct_type(
                        type_=EvaluationStats,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_logs(
        self,
        id: str,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedDataEvaluationReportLogResponse:
        """
        Get the Logs associated to a specific Evaluation.

        Each Datapoint in your Dataset will have a corresponding Log for each File version evaluated.
        e.g. If you have 50 Datapoints and are evaluating 2 Prompts, there will be 100 Logs associated with the Evaluation.

        Parameters
        ----------
        id : str
            String ID of evaluation. Starts with `ev_` or `evr_`.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Logs to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedDataEvaluationReportLogResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.evaluations.get_logs(
                id="id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/logs",
            method="GET",
            params={
                "page": page,
                "size": size,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PaginatedDataEvaluationReportLogResponse,
                    construct_type(
                        type_=PaginatedDataEvaluationReportLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def pin_evaluatee(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        file_id: typing.Optional[str] = OMIT,
        environment: typing.Optional[str] = OMIT,
        batch_id: typing.Optional[str] = OMIT,
        orchestrated: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Pin the specified Evaluatee.

        Pinned Evaluatees are always displayed in the Evaluation Overview,
        and serve as the baseline for comparison with other Evaluatees.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        version_id : typing.Optional[str]
            Unique identifier for the File Version. If provided, none of the other fields should be specified.

        path : typing.Optional[str]
            Path identifying a File. Provide either this or `file_id` if you want to specify a File.

        file_id : typing.Optional[str]
            Unique identifier for the File. Provide either this or `path` if you want to specify a File.

        environment : typing.Optional[str]
            Name of the Environment a Version is deployed to. Only provide this when specifying a File. If not provided (and a File is specified), the default Environment is used.

        batch_id : typing.Optional[str]
            Unique identifier for the batch of Logs to include in the Evaluation Report.

        orchestrated : typing.Optional[bool]
            Whether the Prompt/Tool is orchestrated by Humanloop. Default is `True`. If `False`, a log for the Prompt/Tool should be submitted by the user via the API.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.evaluations.pin_evaluatee(
                id="id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/pin-evaluatee",
            method="POST",
            json={
                "version_id": version_id,
                "path": path,
                "file_id": file_id,
                "environment": environment,
                "batch_id": batch_id,
                "orchestrated": orchestrated,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvaluationResponse,
                    construct_type(
                        type_=EvaluationResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
