# This file was auto-generated by Fern from our API Definition.

import typing
from json.decoder import JSONDecodeError

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.jsonable_encoder import jsonable_encoder
from ..core.pagination import AsyncPager, SyncPager
from ..core.request_options import RequestOptions
from ..core.unchecked_base_model import construct_type
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.evaluatee_request import EvaluateeRequest
from ..types.evaluation_response import EvaluationResponse
from ..types.evaluation_stats import EvaluationStats
from ..types.evaluation_status import EvaluationStatus
from ..types.evaluations_dataset_request import EvaluationsDatasetRequest
from ..types.evaluations_request import EvaluationsRequest
from ..types.http_validation_error import HttpValidationError
from ..types.paginated_data_evaluation_report_log_response import PaginatedDataEvaluationReportLogResponse
from ..types.paginated_evaluation_response import PaginatedEvaluationResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class EvaluationsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list(
        self,
        *,
        file_id: str,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> SyncPager[EvaluationResponse]:
        """
        List all Evaluations for the specified `file_id`.

        Retrieve a list of Evaluations that evaluate versions of the specified File.

        Parameters
        ----------
        file_id : str
            Filter by File ID. Only Evaluations for the specified File will be returned.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Evaluations to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        SyncPager[EvaluationResponse]
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.evaluations.list(
            file_id="pr_30gco7dx6JDq4200GVOHa",
            size=1,
        )
        for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        for page in response.iter_pages():
            yield page
        """
        page = page or 1
        _response = self._client_wrapper.httpx_client.request(
            "evaluations",
            method="GET",
            params={"file_id": file_id, "page": page, "size": size},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(PaginatedEvaluationResponse, construct_type(type_=PaginatedEvaluationResponse, object_=_response.json()))  # type: ignore
                _has_next = True
                _get_next = lambda: self.list(
                    file_id=file_id, page=page + 1, size=size, request_options=request_options
                )
                _items = _parsed_response.records
                return SyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create(
        self,
        *,
        dataset: EvaluationsDatasetRequest,
        evaluatees: typing.Sequence[EvaluateeRequest],
        evaluators: typing.Sequence[EvaluationsRequest],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Create an Evaluation.

        Create a new Evaluation by specifying the Dataset, versions to be
        evaluated (Evaluatees), and which Evaluators to provide judgments.

        Humanloop will automatically start generating Logs and running Evaluators where
        `orchestrated=true`. If you own the runtime for the Evaluatee or Evaluator, you
        can set `orchestrated=false` and then generate and submit the required logs using
        your runtime.

        To keep updated on the progress of the Evaluation, you can poll the Evaluation using
        the GET /evaluations/{id} endpoint and check its status.

        Parameters
        ----------
        dataset : EvaluationsDatasetRequest
            The Dataset Version to use in this Evaluation.

        evaluatees : typing.Sequence[EvaluateeRequest]
            Unique identifiers for the Prompt/Tool Versions to include in the Evaluation Report.

        evaluators : typing.Sequence[EvaluationsRequest]
            The Evaluators used to evaluate.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import (
            EvaluateeRequest,
            EvaluationsDatasetRequest,
            EvaluationsRequest,
        )
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.create(
            dataset=EvaluationsDatasetRequest(
                version_id="dsv_6L78pqrdFi2xa",
            ),
            evaluatees=[
                EvaluateeRequest(
                    version_id="prv_7ZlQREDScH0xkhUwtXruN",
                    orchestrated=False,
                )
            ],
            evaluators=[
                EvaluationsRequest(
                    version_id="evv_012def",
                    orchestrated=False,
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "evaluations",
            method="POST",
            json={"dataset": dataset, "evaluatees": evaluatees, "evaluators": evaluators},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationResponse, construct_type(type_=EvaluationResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationResponse:
        """
        Get an Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.get(
            id="ev_567yza",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}", method="GET", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationResponse, construct_type(type_=EvaluationResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete an Evaluation.

        Remove an Evaluation from Humanloop. The Logs and Versions used in the Evaluation
        will not be deleted.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.delete(
            id="ev_567yza",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}", method="DELETE", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_setup(
        self,
        id: str,
        *,
        dataset: EvaluationsDatasetRequest,
        evaluatees: typing.Sequence[EvaluateeRequest],
        evaluators: typing.Sequence[EvaluationsRequest],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Update an Evaluation.

        Update the setup of an Evaluation by specifying the Dataset, versions to be
        evaluated (Evaluatees), and which Evaluators to provide judgments.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        dataset : EvaluationsDatasetRequest
            The Dataset Version to use in this Evaluation.

        evaluatees : typing.Sequence[EvaluateeRequest]
            Unique identifiers for the Prompt/Tool Versions to include in the Evaluation Report.

        evaluators : typing.Sequence[EvaluationsRequest]
            The Evaluators used to evaluate.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import (
            EvaluateeRequest,
            EvaluationsDatasetRequest,
            EvaluationsRequest,
        )
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.update_setup(
            id="ev_567yza",
            dataset=EvaluationsDatasetRequest(
                version_id="dsv_6L78pqrdFi2xa",
            ),
            evaluatees=[
                EvaluateeRequest(
                    version_id="prv_7ZlQREDScH0xkhUwtXruN",
                    orchestrated=False,
                )
            ],
            evaluators=[
                EvaluationsRequest(
                    version_id="evv_012def",
                    orchestrated=False,
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}",
            method="PATCH",
            json={"dataset": dataset, "evaluatees": evaluatees, "evaluators": evaluators},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationResponse, construct_type(type_=EvaluationResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_status(
        self, id: str, *, status: EvaluationStatus, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationResponse:
        """
        Update the status of an Evaluation.

        Can be used to cancel a running Evaluation, or mark an Evaluation that uses
        external or human evaluators as completed.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        status : EvaluationStatus

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.update_status(
            id="id",
            status="pending",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/status",
            method="PATCH",
            json={"status": status},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationResponse, construct_type(type_=EvaluationResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_stats(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationStats:
        """
        Get Evaluation Stats.

        Retrieve aggregate stats for the specified Evaluation.
        This includes the number of generated Logs for each evaluated version and the
        corresponding Evaluator statistics (such as the mean and percentiles).

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationStats
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.get_stats(
            id="id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/stats", method="GET", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationStats, construct_type(type_=EvaluationStats, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_logs(
        self,
        id: str,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedDataEvaluationReportLogResponse:
        """
        Get the Logs associated to a specific Evaluation.

        Each Datapoint in your Dataset will have a corresponding Log for each File version evaluated.
        e.g. If you have 50 Datapoints and are evaluating 2 Prompts, there will be 100 Logs associated with the Evaluation.

        Parameters
        ----------
        id : str
            String ID of evaluation. Starts with `ev_` or `evr_`.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Logs to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedDataEvaluationReportLogResponse
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.evaluations.get_logs(
            id="id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/logs",
            method="GET",
            params={"page": page, "size": size},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PaginatedDataEvaluationReportLogResponse, construct_type(type_=PaginatedDataEvaluationReportLogResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncEvaluationsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list(
        self,
        *,
        file_id: str,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncPager[EvaluationResponse]:
        """
        List all Evaluations for the specified `file_id`.

        Retrieve a list of Evaluations that evaluate versions of the specified File.

        Parameters
        ----------
        file_id : str
            Filter by File ID. Only Evaluations for the specified File will be returned.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Evaluations to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncPager[EvaluationResponse]
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        response = await client.evaluations.list(
            file_id="pr_30gco7dx6JDq4200GVOHa",
            size=1,
        )
        async for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        async for page in response.iter_pages():
            yield page
        """
        page = page or 1
        _response = await self._client_wrapper.httpx_client.request(
            "evaluations",
            method="GET",
            params={"file_id": file_id, "page": page, "size": size},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(PaginatedEvaluationResponse, construct_type(type_=PaginatedEvaluationResponse, object_=_response.json()))  # type: ignore
                _has_next = True
                _get_next = lambda: self.list(
                    file_id=file_id, page=page + 1, size=size, request_options=request_options
                )
                _items = _parsed_response.records
                return AsyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create(
        self,
        *,
        dataset: EvaluationsDatasetRequest,
        evaluatees: typing.Sequence[EvaluateeRequest],
        evaluators: typing.Sequence[EvaluationsRequest],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Create an Evaluation.

        Create a new Evaluation by specifying the Dataset, versions to be
        evaluated (Evaluatees), and which Evaluators to provide judgments.

        Humanloop will automatically start generating Logs and running Evaluators where
        `orchestrated=true`. If you own the runtime for the Evaluatee or Evaluator, you
        can set `orchestrated=false` and then generate and submit the required logs using
        your runtime.

        To keep updated on the progress of the Evaluation, you can poll the Evaluation using
        the GET /evaluations/{id} endpoint and check its status.

        Parameters
        ----------
        dataset : EvaluationsDatasetRequest
            The Dataset Version to use in this Evaluation.

        evaluatees : typing.Sequence[EvaluateeRequest]
            Unique identifiers for the Prompt/Tool Versions to include in the Evaluation Report.

        evaluators : typing.Sequence[EvaluationsRequest]
            The Evaluators used to evaluate.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import (
            EvaluateeRequest,
            EvaluationsDatasetRequest,
            EvaluationsRequest,
        )
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.evaluations.create(
            dataset=EvaluationsDatasetRequest(
                version_id="dsv_6L78pqrdFi2xa",
            ),
            evaluatees=[
                EvaluateeRequest(
                    version_id="prv_7ZlQREDScH0xkhUwtXruN",
                    orchestrated=False,
                )
            ],
            evaluators=[
                EvaluationsRequest(
                    version_id="evv_012def",
                    orchestrated=False,
                )
            ],
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "evaluations",
            method="POST",
            json={"dataset": dataset, "evaluatees": evaluatees, "evaluators": evaluators},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationResponse, construct_type(type_=EvaluationResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationResponse:
        """
        Get an Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.evaluations.get(
            id="ev_567yza",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}", method="GET", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationResponse, construct_type(type_=EvaluationResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete an Evaluation.

        Remove an Evaluation from Humanloop. The Logs and Versions used in the Evaluation
        will not be deleted.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.evaluations.delete(
            id="ev_567yza",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}", method="DELETE", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_setup(
        self,
        id: str,
        *,
        dataset: EvaluationsDatasetRequest,
        evaluatees: typing.Sequence[EvaluateeRequest],
        evaluators: typing.Sequence[EvaluationsRequest],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Update an Evaluation.

        Update the setup of an Evaluation by specifying the Dataset, versions to be
        evaluated (Evaluatees), and which Evaluators to provide judgments.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        dataset : EvaluationsDatasetRequest
            The Dataset Version to use in this Evaluation.

        evaluatees : typing.Sequence[EvaluateeRequest]
            Unique identifiers for the Prompt/Tool Versions to include in the Evaluation Report.

        evaluators : typing.Sequence[EvaluationsRequest]
            The Evaluators used to evaluate.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import (
            EvaluateeRequest,
            EvaluationsDatasetRequest,
            EvaluationsRequest,
        )
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.evaluations.update_setup(
            id="ev_567yza",
            dataset=EvaluationsDatasetRequest(
                version_id="dsv_6L78pqrdFi2xa",
            ),
            evaluatees=[
                EvaluateeRequest(
                    version_id="prv_7ZlQREDScH0xkhUwtXruN",
                    orchestrated=False,
                )
            ],
            evaluators=[
                EvaluationsRequest(
                    version_id="evv_012def",
                    orchestrated=False,
                )
            ],
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}",
            method="PATCH",
            json={"dataset": dataset, "evaluatees": evaluatees, "evaluators": evaluators},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationResponse, construct_type(type_=EvaluationResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_status(
        self, id: str, *, status: EvaluationStatus, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationResponse:
        """
        Update the status of an Evaluation.

        Can be used to cancel a running Evaluation, or mark an Evaluation that uses
        external or human evaluators as completed.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        status : EvaluationStatus

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.evaluations.update_status(
            id="id",
            status="pending",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/status",
            method="PATCH",
            json={"status": status},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationResponse, construct_type(type_=EvaluationResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_stats(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationStats:
        """
        Get Evaluation Stats.

        Retrieve aggregate stats for the specified Evaluation.
        This includes the number of generated Logs for each evaluated version and the
        corresponding Evaluator statistics (such as the mean and percentiles).

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationStats
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.evaluations.get_stats(
            id="id",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/stats", method="GET", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(EvaluationStats, construct_type(type_=EvaluationStats, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_logs(
        self,
        id: str,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedDataEvaluationReportLogResponse:
        """
        Get the Logs associated to a specific Evaluation.

        Each Datapoint in your Dataset will have a corresponding Log for each File version evaluated.
        e.g. If you have 50 Datapoints and are evaluating 2 Prompts, there will be 100 Logs associated with the Evaluation.

        Parameters
        ----------
        id : str
            String ID of evaluation. Starts with `ev_` or `evr_`.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Logs to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedDataEvaluationReportLogResponse
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.evaluations.get_logs(
            id="id",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"evaluations/{jsonable_encoder(id)}/logs",
            method="GET",
            params={"page": page, "size": size},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PaginatedDataEvaluationReportLogResponse, construct_type(type_=PaginatedDataEvaluationReportLogResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
