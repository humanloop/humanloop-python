# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.pagination import AsyncPager, SyncPager
from ..core.request_options import RequestOptions
from ..requests.file_request import FileRequestParams
from ..types.evaluation_response import EvaluationResponse
from ..types.evaluation_run_response import EvaluationRunResponse
from ..types.evaluation_runs_response import EvaluationRunsResponse
from ..types.evaluation_stats import EvaluationStats
from ..types.evaluation_status import EvaluationStatus
from ..types.paginated_data_evaluation_log_response import PaginatedDataEvaluationLogResponse
from .raw_client import AsyncRawEvaluationsClient, RawEvaluationsClient
from .requests.add_evaluators_request_evaluators_item import AddEvaluatorsRequestEvaluatorsItemParams
from .requests.create_evaluation_request_evaluators_item import CreateEvaluationRequestEvaluatorsItemParams
from .requests.create_run_request_dataset import CreateRunRequestDatasetParams
from .requests.create_run_request_version import CreateRunRequestVersionParams

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class EvaluationsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawEvaluationsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawEvaluationsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawEvaluationsClient
        """
        return self._raw_client

    def list(
        self,
        *,
        file_id: str,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> SyncPager[EvaluationResponse]:
        """
        Retrieve a list of Evaluations for the specified File.

        Parameters
        ----------
        file_id : str
            Filter by File ID. Only Evaluations for the specified File will be returned.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Evaluations to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        SyncPager[EvaluationResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        response = client.evaluations.list(file_id='pr_30gco7dx6JDq4200GVOHa', size=1, )
        for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        for page in response.iter_pages():
            yield page
        """
        response = self._raw_client.list(file_id=file_id, page=page, size=size, request_options=request_options)
        return response.data

    def create(
        self,
        *,
        evaluators: typing.Sequence[CreateEvaluationRequestEvaluatorsItemParams],
        file: typing.Optional[FileRequestParams] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Create an Evaluation.

        Create a new Evaluation by specifying the File to evaluate, and a name
        for the Evaluation.
        You can then add Runs to this Evaluation using the `POST /evaluations/{id}/runs` endpoint.

        Parameters
        ----------
        evaluators : typing.Sequence[CreateEvaluationRequestEvaluatorsItemParams]
            The Evaluators used to evaluate.

        file : typing.Optional[FileRequestParams]
            The File to associate with the Evaluation. This File contains the Logs you're evaluating.

        name : typing.Optional[str]
            Name of the Evaluation to help identify it. Must be unique within the associated File.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.create(evaluators=[{'version_id': 'version_id'}], )
        """
        response = self._raw_client.create(evaluators=evaluators, file=file, name=name, request_options=request_options)
        return response.data

    def add_evaluators(
        self,
        id: str,
        *,
        evaluators: typing.Sequence[AddEvaluatorsRequestEvaluatorsItemParams],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Add Evaluators to an Evaluation.

        The Evaluators will be run on the Logs generated for the Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        evaluators : typing.Sequence[AddEvaluatorsRequestEvaluatorsItemParams]
            The Evaluators to add to this Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.add_evaluators(id='id', evaluators=[{'version_id': 'version_id'}], )
        """
        response = self._raw_client.add_evaluators(id, evaluators=evaluators, request_options=request_options)
        return response.data

    def remove_evaluator(
        self, id: str, evaluator_version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationResponse:
        """
        Remove an Evaluator from an Evaluation.

        The Evaluator will no longer be run on the Logs in the Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        evaluator_version_id : str
            Unique identifier for Evaluator Version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.remove_evaluator(id='id', evaluator_version_id='evaluator_version_id', )
        """
        response = self._raw_client.remove_evaluator(id, evaluator_version_id, request_options=request_options)
        return response.data

    def get(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationResponse:
        """
        Get an Evaluation.

        This includes the Evaluators associated with the Evaluation and metadata about the Evaluation,
        such as its name.

        To get the Runs associated with the Evaluation, use the `GET /evaluations/{id}/runs` endpoint.
        To retrieve stats for the Evaluation, use the `GET /evaluations/{id}/stats` endpoint.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.get(id='ev_567yza', )
        """
        response = self._raw_client.get(id, request_options=request_options)
        return response.data

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete an Evaluation.

        The Runs and Evaluators in the Evaluation will not be deleted.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.delete(id='ev_567yza', )
        """
        response = self._raw_client.delete(id, request_options=request_options)
        return response.data

    def list_runs_for_evaluation(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationRunsResponse:
        """
        List all Runs for an Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationRunsResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.list_runs_for_evaluation(id='id', )
        """
        response = self._raw_client.list_runs_for_evaluation(id, request_options=request_options)
        return response.data

    def create_run(
        self,
        id: str,
        *,
        dataset: typing.Optional[CreateRunRequestDatasetParams] = OMIT,
        version: typing.Optional[CreateRunRequestVersionParams] = OMIT,
        orchestrated: typing.Optional[bool] = OMIT,
        use_existing_logs: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationRunResponse:
        """
        Create an Evaluation Run.

        Optionally specify the Dataset and version to be evaluated.

        Humanloop will automatically start generating Logs and running Evaluators where
        `orchestrated=true`. If you are generating Logs yourself, you can set `orchestrated=false`
        and then generate and submit the required Logs via the API.

        If `dataset` and `version` are provided, you can set `use_existing_logs=True` to reuse existing Logs,
        avoiding generating new Logs unnecessarily. Logs that are associated with the specified Version and have `source_datapoint_id`
        referencing a datapoint in the specified Dataset will be associated with the Run.

        To keep updated on the progress of the Run, you can poll the Run using
        the `GET /evaluations/{id}/runs` endpoint and check its status.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        dataset : typing.Optional[CreateRunRequestDatasetParams]
            Dataset to use in this Run.

        version : typing.Optional[CreateRunRequestVersionParams]
            Version to use in this Run.

        orchestrated : typing.Optional[bool]
            Whether the Run is orchestrated by Humanloop. If `True`, Humanloop will generate Logs for the Run; `dataset` and `version` must be provided. If `False`, a log for the Prompt/Tool should be submitted by the user via the API.

        use_existing_logs : typing.Optional[bool]
            If `True`, the Run will be initialized with existing Logs associated with the Dataset and Version. If `False`, the Run will be initialized with no Logs. Can only be set to `True` when both `dataset` and `version` are provided.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationRunResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.create_run(id='id', )
        """
        response = self._raw_client.create_run(
            id,
            dataset=dataset,
            version=version,
            orchestrated=orchestrated,
            use_existing_logs=use_existing_logs,
            request_options=request_options,
        )
        return response.data

    def add_existing_run(
        self, id: str, run_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[typing.Any]:
        """
        Add an existing Run to the specified Evaluation.

        This is useful if you want to compare the Runs in this Evaluation with an existing Run
        that exists within another Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        run_id : str
            Unique identifier for Run.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.add_existing_run(id='id', run_id='run_id', )
        """
        response = self._raw_client.add_existing_run(id, run_id, request_options=request_options)
        return response.data

    def remove_run(self, id: str, run_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Remove a Run from an Evaluation.

        The Logs and Versions used in the Run will not be deleted.
        If this Run is used in any other Evaluations, it will still be available in those Evaluations.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        run_id : str
            Unique identifier for Run.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.remove_run(id='id', run_id='run_id', )
        """
        response = self._raw_client.remove_run(id, run_id, request_options=request_options)
        return response.data

    def update_evaluation_run(
        self,
        id: str,
        run_id: str,
        *,
        control: typing.Optional[bool] = OMIT,
        status: typing.Optional[EvaluationStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationRunResponse:
        """
        Update an Evaluation Run.

        Specify `control=true` to use this Run as the control Run for the Evaluation.
        You can cancel a running/pending Run, or mark a Run that uses external or human Evaluators as completed.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        run_id : str
            Unique identifier for Run.

        control : typing.Optional[bool]
            If `True`, this Run will be used as the control in the Evaluation. Stats for other Runs will be compared to this Run. This will replace any existing control Run.

        status : typing.Optional[EvaluationStatus]
            Used to set the Run to `cancelled` or `completed`. Can only be used if the Run is currently `pending` or `running`.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationRunResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.update_evaluation_run(id='id', run_id='run_id', )
        """
        response = self._raw_client.update_evaluation_run(
            id, run_id, control=control, status=status, request_options=request_options
        )
        return response.data

    def add_logs_to_run(
        self,
        id: str,
        run_id: str,
        *,
        log_ids: typing.Sequence[str],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationRunResponse:
        """
        Add the specified Logs to a Run.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        run_id : str
            Unique identifier for Run.

        log_ids : typing.Sequence[str]
            The IDs of the Logs to add to the Run.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationRunResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.add_logs_to_run(id='id', run_id='run_id', log_ids=['log_ids'], )
        """
        response = self._raw_client.add_logs_to_run(id, run_id, log_ids=log_ids, request_options=request_options)
        return response.data

    def get_stats(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationStats:
        """
        Get Evaluation Stats.

        Retrieve aggregate stats for the specified Evaluation. This includes the number of generated Logs for each Run and the
        corresponding Evaluator statistics (such as the mean and percentiles).

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationStats
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.get_stats(id='id', )
        """
        response = self._raw_client.get_stats(id, request_options=request_options)
        return response.data

    def get_logs(
        self,
        id: str,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        run_id: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedDataEvaluationLogResponse:
        """
        Get the Logs associated to a specific Evaluation.

        This returns the Logs associated to all Runs within with the Evaluation.

        Parameters
        ----------
        id : str
            String ID of evaluation. Starts with `ev_` or `evr_`.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Logs to fetch.

        run_id : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Filter by Run IDs. Only Logs for the specified Runs will be returned.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedDataEvaluationLogResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop
        client = Humanloop(api_key="YOUR_API_KEY", )
        client.evaluations.get_logs(id='id', )
        """
        response = self._raw_client.get_logs(id, page=page, size=size, run_id=run_id, request_options=request_options)
        return response.data


class AsyncEvaluationsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawEvaluationsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawEvaluationsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawEvaluationsClient
        """
        return self._raw_client

    async def list(
        self,
        *,
        file_id: str,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncPager[EvaluationResponse]:
        """
        Retrieve a list of Evaluations for the specified File.

        Parameters
        ----------
        file_id : str
            Filter by File ID. Only Evaluations for the specified File will be returned.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Evaluations to fetch.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncPager[EvaluationResponse]
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            response = await client.evaluations.list(file_id='pr_30gco7dx6JDq4200GVOHa', size=1, )
            async for item in response:
                yield item

            # alternatively, you can paginate page-by-page
            async for page in response.iter_pages():
                yield page
        asyncio.run(main())
        """
        response = await self._raw_client.list(file_id=file_id, page=page, size=size, request_options=request_options)
        return response.data

    async def create(
        self,
        *,
        evaluators: typing.Sequence[CreateEvaluationRequestEvaluatorsItemParams],
        file: typing.Optional[FileRequestParams] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Create an Evaluation.

        Create a new Evaluation by specifying the File to evaluate, and a name
        for the Evaluation.
        You can then add Runs to this Evaluation using the `POST /evaluations/{id}/runs` endpoint.

        Parameters
        ----------
        evaluators : typing.Sequence[CreateEvaluationRequestEvaluatorsItemParams]
            The Evaluators used to evaluate.

        file : typing.Optional[FileRequestParams]
            The File to associate with the Evaluation. This File contains the Logs you're evaluating.

        name : typing.Optional[str]
            Name of the Evaluation to help identify it. Must be unique within the associated File.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.create(evaluators=[{'version_id': 'version_id'}], )
        asyncio.run(main())
        """
        response = await self._raw_client.create(
            evaluators=evaluators, file=file, name=name, request_options=request_options
        )
        return response.data

    async def add_evaluators(
        self,
        id: str,
        *,
        evaluators: typing.Sequence[AddEvaluatorsRequestEvaluatorsItemParams],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationResponse:
        """
        Add Evaluators to an Evaluation.

        The Evaluators will be run on the Logs generated for the Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        evaluators : typing.Sequence[AddEvaluatorsRequestEvaluatorsItemParams]
            The Evaluators to add to this Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.add_evaluators(id='id', evaluators=[{'version_id': 'version_id'}], )
        asyncio.run(main())
        """
        response = await self._raw_client.add_evaluators(id, evaluators=evaluators, request_options=request_options)
        return response.data

    async def remove_evaluator(
        self, id: str, evaluator_version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationResponse:
        """
        Remove an Evaluator from an Evaluation.

        The Evaluator will no longer be run on the Logs in the Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        evaluator_version_id : str
            Unique identifier for Evaluator Version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.remove_evaluator(id='id', evaluator_version_id='evaluator_version_id', )
        asyncio.run(main())
        """
        response = await self._raw_client.remove_evaluator(id, evaluator_version_id, request_options=request_options)
        return response.data

    async def get(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationResponse:
        """
        Get an Evaluation.

        This includes the Evaluators associated with the Evaluation and metadata about the Evaluation,
        such as its name.

        To get the Runs associated with the Evaluation, use the `GET /evaluations/{id}/runs` endpoint.
        To retrieve stats for the Evaluation, use the `GET /evaluations/{id}/stats` endpoint.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.get(id='ev_567yza', )
        asyncio.run(main())
        """
        response = await self._raw_client.get(id, request_options=request_options)
        return response.data

    async def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete an Evaluation.

        The Runs and Evaluators in the Evaluation will not be deleted.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.delete(id='ev_567yza', )
        asyncio.run(main())
        """
        response = await self._raw_client.delete(id, request_options=request_options)
        return response.data

    async def list_runs_for_evaluation(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> EvaluationRunsResponse:
        """
        List all Runs for an Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationRunsResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.list_runs_for_evaluation(id='id', )
        asyncio.run(main())
        """
        response = await self._raw_client.list_runs_for_evaluation(id, request_options=request_options)
        return response.data

    async def create_run(
        self,
        id: str,
        *,
        dataset: typing.Optional[CreateRunRequestDatasetParams] = OMIT,
        version: typing.Optional[CreateRunRequestVersionParams] = OMIT,
        orchestrated: typing.Optional[bool] = OMIT,
        use_existing_logs: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationRunResponse:
        """
        Create an Evaluation Run.

        Optionally specify the Dataset and version to be evaluated.

        Humanloop will automatically start generating Logs and running Evaluators where
        `orchestrated=true`. If you are generating Logs yourself, you can set `orchestrated=false`
        and then generate and submit the required Logs via the API.

        If `dataset` and `version` are provided, you can set `use_existing_logs=True` to reuse existing Logs,
        avoiding generating new Logs unnecessarily. Logs that are associated with the specified Version and have `source_datapoint_id`
        referencing a datapoint in the specified Dataset will be associated with the Run.

        To keep updated on the progress of the Run, you can poll the Run using
        the `GET /evaluations/{id}/runs` endpoint and check its status.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        dataset : typing.Optional[CreateRunRequestDatasetParams]
            Dataset to use in this Run.

        version : typing.Optional[CreateRunRequestVersionParams]
            Version to use in this Run.

        orchestrated : typing.Optional[bool]
            Whether the Run is orchestrated by Humanloop. If `True`, Humanloop will generate Logs for the Run; `dataset` and `version` must be provided. If `False`, a log for the Prompt/Tool should be submitted by the user via the API.

        use_existing_logs : typing.Optional[bool]
            If `True`, the Run will be initialized with existing Logs associated with the Dataset and Version. If `False`, the Run will be initialized with no Logs. Can only be set to `True` when both `dataset` and `version` are provided.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationRunResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.create_run(id='id', )
        asyncio.run(main())
        """
        response = await self._raw_client.create_run(
            id,
            dataset=dataset,
            version=version,
            orchestrated=orchestrated,
            use_existing_logs=use_existing_logs,
            request_options=request_options,
        )
        return response.data

    async def add_existing_run(
        self, id: str, run_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[typing.Any]:
        """
        Add an existing Run to the specified Evaluation.

        This is useful if you want to compare the Runs in this Evaluation with an existing Run
        that exists within another Evaluation.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        run_id : str
            Unique identifier for Run.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.add_existing_run(id='id', run_id='run_id', )
        asyncio.run(main())
        """
        response = await self._raw_client.add_existing_run(id, run_id, request_options=request_options)
        return response.data

    async def remove_run(
        self, id: str, run_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove a Run from an Evaluation.

        The Logs and Versions used in the Run will not be deleted.
        If this Run is used in any other Evaluations, it will still be available in those Evaluations.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        run_id : str
            Unique identifier for Run.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.remove_run(id='id', run_id='run_id', )
        asyncio.run(main())
        """
        response = await self._raw_client.remove_run(id, run_id, request_options=request_options)
        return response.data

    async def update_evaluation_run(
        self,
        id: str,
        run_id: str,
        *,
        control: typing.Optional[bool] = OMIT,
        status: typing.Optional[EvaluationStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationRunResponse:
        """
        Update an Evaluation Run.

        Specify `control=true` to use this Run as the control Run for the Evaluation.
        You can cancel a running/pending Run, or mark a Run that uses external or human Evaluators as completed.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        run_id : str
            Unique identifier for Run.

        control : typing.Optional[bool]
            If `True`, this Run will be used as the control in the Evaluation. Stats for other Runs will be compared to this Run. This will replace any existing control Run.

        status : typing.Optional[EvaluationStatus]
            Used to set the Run to `cancelled` or `completed`. Can only be used if the Run is currently `pending` or `running`.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationRunResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.update_evaluation_run(id='id', run_id='run_id', )
        asyncio.run(main())
        """
        response = await self._raw_client.update_evaluation_run(
            id, run_id, control=control, status=status, request_options=request_options
        )
        return response.data

    async def add_logs_to_run(
        self,
        id: str,
        run_id: str,
        *,
        log_ids: typing.Sequence[str],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvaluationRunResponse:
        """
        Add the specified Logs to a Run.

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        run_id : str
            Unique identifier for Run.

        log_ids : typing.Sequence[str]
            The IDs of the Logs to add to the Run.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationRunResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.add_logs_to_run(id='id', run_id='run_id', log_ids=['log_ids'], )
        asyncio.run(main())
        """
        response = await self._raw_client.add_logs_to_run(id, run_id, log_ids=log_ids, request_options=request_options)
        return response.data

    async def get_stats(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> EvaluationStats:
        """
        Get Evaluation Stats.

        Retrieve aggregate stats for the specified Evaluation. This includes the number of generated Logs for each Run and the
        corresponding Evaluator statistics (such as the mean and percentiles).

        Parameters
        ----------
        id : str
            Unique identifier for Evaluation.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvaluationStats
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.get_stats(id='id', )
        asyncio.run(main())
        """
        response = await self._raw_client.get_stats(id, request_options=request_options)
        return response.data

    async def get_logs(
        self,
        id: str,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        run_id: typing.Optional[typing.Union[str, typing.Sequence[str]]] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedDataEvaluationLogResponse:
        """
        Get the Logs associated to a specific Evaluation.

        This returns the Logs associated to all Runs within with the Evaluation.

        Parameters
        ----------
        id : str
            String ID of evaluation. Starts with `ev_` or `evr_`.

        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Logs to fetch.

        run_id : typing.Optional[typing.Union[str, typing.Sequence[str]]]
            Filter by Run IDs. Only Logs for the specified Runs will be returned.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedDataEvaluationLogResponse
            Successful Response

        Examples
        --------
        from humanloop import AsyncHumanloop
        import asyncio
        client = AsyncHumanloop(api_key="YOUR_API_KEY", )
        async def main() -> None:
            await client.evaluations.get_logs(id='id', )
        asyncio.run(main())
        """
        response = await self._raw_client.get_logs(
            id, page=page, size=size, run_id=run_id, request_options=request_options
        )
        return response.data
