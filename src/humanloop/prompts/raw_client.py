# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from ..requests.chat_message import ChatMessageParams
from .requests.prompt_log_request_tool_choice import PromptLogRequestToolChoiceParams
from ..requests.prompt_kernel_request import PromptKernelRequestParams
import datetime as dt
from ..types.log_status import LogStatus
from ..core.request_options import RequestOptions
from ..core.http_response import HttpResponse
from ..types.create_prompt_log_response import CreatePromptLogResponse
from ..core.serialization import convert_and_respect_annotation_metadata
from ..core.unchecked_base_model import construct_type
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from .requests.prompt_log_update_request_tool_choice import PromptLogUpdateRequestToolChoiceParams
from ..types.log_response import LogResponse
from ..core.jsonable_encoder import jsonable_encoder
from .requests.prompts_call_stream_request_tool_choice import PromptsCallStreamRequestToolChoiceParams
from ..requests.provider_api_keys import ProviderApiKeysParams
from ..types.prompt_call_stream_response import PromptCallStreamResponse
import httpx_sse
import contextlib
from .requests.prompts_call_request_tool_choice import PromptsCallRequestToolChoiceParams
from ..types.prompt_call_response import PromptCallResponse
from ..types.model_endpoints import ModelEndpoints
from .requests.prompt_request_template import PromptRequestTemplateParams
from ..types.template_language import TemplateLanguage
from ..types.model_providers import ModelProviders
from .requests.prompt_request_stop import PromptRequestStopParams
from ..requests.response_format import ResponseFormatParams
from .requests.prompt_request_reasoning_effort import PromptRequestReasoningEffortParams
from ..requests.tool_function import ToolFunctionParams
from ..types.prompt_response import PromptResponse
from ..types.populate_template_response import PopulateTemplateResponse
from ..types.list_prompts import ListPrompts
from ..types.file_environment_response import FileEnvironmentResponse
from ..requests.evaluator_activation_deactivation_request_activate_item import (
    EvaluatorActivationDeactivationRequestActivateItemParams,
)
from ..requests.evaluator_activation_deactivation_request_deactivate_item import (
    EvaluatorActivationDeactivationRequestDeactivateItemParams,
)
from ..types.prompt_kernel_request import PromptKernelRequest
from ..core.client_wrapper import AsyncClientWrapper
from ..core.http_response import AsyncHttpResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RawPromptsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        run_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[CreatePromptLogResponse]:
        """
        Log to a Prompt.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise, the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        run_id : typing.Optional[str]
            Unique identifier for the Run to associate the Log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[CreatePromptLogResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts/log",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "run_id": run_id,
                "path": path,
                "id": id,
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "reasoning_tokens": reasoning_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptLogRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "start_time": start_time,
                "end_time": end_time,
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": prompt_log_request_environment,
                "save": save,
                "log_id": log_id,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    CreatePromptLogResponse,
                    construct_type(
                        type_=CreatePromptLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_log(
        self,
        id: str,
        log_id: str,
        *,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogUpdateRequestToolChoiceParams] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[LogResponse]:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        log_id : str
            Unique identifier for the Log.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogUpdateRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[LogResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/log/{jsonable_encoder(log_id)}",
            method="PATCH",
            json={
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "reasoning_tokens": reasoning_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptLogUpdateRequestToolChoiceParams, direction="write"
                ),
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    LogResponse,
                    construct_type(
                        type_=LogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    @contextlib.contextmanager
    def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallStreamRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[HttpResponse[typing.Iterator[PromptCallStreamResponse]]]:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[HttpResponse[typing.Iterator[PromptCallStreamResponse]]]

        """
        with self._client_wrapper.httpx_client.stream(
            "prompts/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptsCallStreamRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": prompts_call_stream_request_environment,
                "save": save,
                "log_id": log_id,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "num_samples": num_samples,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
                "stream": True,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:

            def stream() -> HttpResponse[typing.Iterator[PromptCallStreamResponse]]:
                try:
                    if 200 <= _response.status_code < 300:

                        def _iter():
                            _event_source = httpx_sse.EventSource(_response)
                            for _sse in _event_source.iter_sse():
                                if _sse.data == None:
                                    return
                                try:
                                    yield _sse.data()
                                except Exception:
                                    pass
                            return

                        return HttpResponse(response=_response, data=_iter())
                    _response.read()
                    if _response.status_code == 422:
                        raise UnprocessableEntityError(
                            typing.cast(
                                HttpValidationError,
                                construct_type(
                                    type_=HttpValidationError,  # type: ignore
                                    object_=_response.json(),
                                ),
                            )
                        )
                    _response_json = _response.json()
                except JSONDecodeError:
                    raise ApiError(status_code=_response.status_code, body=_response.text)
                raise ApiError(status_code=_response.status_code, body=_response_json)

            yield stream()

    def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[PromptCallResponse]:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PromptCallResponse]

        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptsCallRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": prompts_call_request_environment,
                "save": save,
                "log_id": log_id,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "num_samples": num_samples,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
                "stream": False,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptCallResponse,
                    construct_type(
                        type_=PromptCallResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[PromptRequestTemplateParams] = OMIT,
        template_language: typing.Optional[TemplateLanguage] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[PromptRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        reasoning_effort: typing.Optional[PromptRequestReasoningEffortParams] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolFunctionParams]] = OMIT,
        linked_tools: typing.Optional[typing.Sequence[str]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        tags: typing.Optional[typing.Sequence[str]] = OMIT,
        readme: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[PromptResponse]:
        """
        Create a Prompt or update it with a new version if it already exists.

        Prompts are identified by the `ID` or their `path`. The parameters (i.e. the prompt template, temperature, model etc.) determine the versions of the Prompt.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within a Prompt - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[PromptRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        template_language : typing.Optional[TemplateLanguage]
            The template language to use for rendering the template.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[PromptRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        reasoning_effort : typing.Optional[PromptRequestReasoningEffortParams]
            Guidance on how many reasoning tokens it should generate before creating a response to the prompt. OpenAI reasoning models (o1, o3-mini) expect a OpenAIReasoningEffort enum. Anthropic reasoning models expect an integer, which signifies the maximum token budget.

        tools : typing.Optional[typing.Sequence[ToolFunctionParams]]
            The tool specification that the model can choose to call if Tool calling is supported.

        linked_tools : typing.Optional[typing.Sequence[str]]
            The IDs of the Tools in your organization that the model can choose to call if Tool calling is supported. The default deployed version of that tool is called.

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        version_name : typing.Optional[str]
            Unique name for the Prompt version. Version names must be unique for a given Prompt.

        version_description : typing.Optional[str]
            Description of the version, e.g., the changes made in this version.

        description : typing.Optional[str]
            Description of the Prompt.

        tags : typing.Optional[typing.Sequence[str]]
            List of tags associated with this prompt.

        readme : typing.Optional[str]
            Long description of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PromptResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts",
            method="POST",
            json={
                "path": path,
                "id": id,
                "model": model,
                "endpoint": endpoint,
                "template": convert_and_respect_annotation_metadata(
                    object_=template, annotation=PromptRequestTemplateParams, direction="write"
                ),
                "template_language": template_language,
                "provider": provider,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": convert_and_respect_annotation_metadata(
                    object_=stop, annotation=PromptRequestStopParams, direction="write"
                ),
                "presence_penalty": presence_penalty,
                "frequency_penalty": frequency_penalty,
                "other": other,
                "seed": seed,
                "response_format": convert_and_respect_annotation_metadata(
                    object_=response_format, annotation=ResponseFormatParams, direction="write"
                ),
                "reasoning_effort": convert_and_respect_annotation_metadata(
                    object_=reasoning_effort, annotation=PromptRequestReasoningEffortParams, direction="write"
                ),
                "tools": convert_and_respect_annotation_metadata(
                    object_=tools, annotation=typing.Sequence[ToolFunctionParams], direction="write"
                ),
                "linked_tools": linked_tools,
                "attributes": attributes,
                "version_name": version_name,
                "version_description": version_description,
                "description": description,
                "tags": tags,
                "readme": readme,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[PromptResponse]:
        """
        Retrieve the Prompt with the given ID.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PromptResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> HttpResponse[None]:
        """
        Delete the Prompt with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[PromptResponse]:
        """
        Move the Prompt to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        path : typing.Optional[str]
            Path of the Prompt including the Prompt name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PromptResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "path": path,
                "name": name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def populate(
        self,
        id: str,
        *,
        request: typing.Dict[str, typing.Optional[typing.Any]],
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[PopulateTemplateResponse]:
        """
        Retrieve the Prompt with the given ID, including the populated template.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request : typing.Dict[str, typing.Optional[typing.Any]]

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve to populate the template.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from to populate the template.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PopulateTemplateResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/populate",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json=request,
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PopulateTemplateResponse,
                    construct_type(
                        type_=PopulateTemplateResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[ListPrompts]:
        """
        Get a list of all the versions of a Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[ListPrompts]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions",
            method="GET",
            params={
                "evaluator_aggregates": evaluator_aggregates,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    ListPrompts,
                    construct_type(
                        type_=ListPrompts,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_prompt_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[None]:
        """
        Delete a version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def patch_prompt_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[PromptResponse]:
        """
        Update the name or description of the Prompt version.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PromptResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}",
            method="PATCH",
            json={
                "name": name,
                "description": description,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[PromptResponse]:
        """
        Deploy Prompt to an Environment.

        Set the deployed version for the specified Environment. This Prompt
        will be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PromptResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="POST",
            params={
                "version_id": version_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[None]:
        """
        Remove deployed Prompt from the Environment.

        Remove the deployed version for the specified Environment. This Prompt
        will no longer be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[typing.List[FileEnvironmentResponse]]:
        """
        List all Environments and their deployed versions for the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[typing.List[FileEnvironmentResponse]]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    typing.List[FileEnvironmentResponse],
                    construct_type(
                        type_=typing.List[FileEnvironmentResponse],  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[PromptResponse]:
        """
        Activate and deactivate Evaluators for monitoring the Prompt.

        An activated Evaluator will automatically be run on all new Logs
        within the Prompt for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PromptResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/evaluators",
            method="POST",
            json={
                "activate": convert_and_respect_annotation_metadata(
                    object_=activate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams],
                    direction="write",
                ),
                "deactivate": convert_and_respect_annotation_metadata(
                    object_=deactivate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams],
                    direction="write",
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def serialize(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[None]:
        """
        Serialize a Prompt to the .prompt file format.

        Useful for storing the Prompt with your code in a version control system,
        or for editing with an AI tool.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/serialize",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def deserialize(
        self, *, prompt: str, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[PromptKernelRequest]:
        """
        Deserialize a Prompt from the .prompt file format.

        This returns a subset of the attributes required by a Prompt.
        This subset is the bit that defines the Prompt version (e.g. with `model` and `temperature` etc)

        Parameters
        ----------
        prompt : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PromptKernelRequest]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts/deserialize",
            method="POST",
            json={
                "prompt": prompt,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptKernelRequest,
                    construct_type(
                        type_=PromptKernelRequest,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncRawPromptsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        run_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[CreatePromptLogResponse]:
        """
        Log to a Prompt.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise, the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        run_id : typing.Optional[str]
            Unique identifier for the Run to associate the Log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[CreatePromptLogResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts/log",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "run_id": run_id,
                "path": path,
                "id": id,
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "reasoning_tokens": reasoning_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptLogRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "start_time": start_time,
                "end_time": end_time,
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": prompt_log_request_environment,
                "save": save,
                "log_id": log_id,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    CreatePromptLogResponse,
                    construct_type(
                        type_=CreatePromptLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_log(
        self,
        id: str,
        log_id: str,
        *,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogUpdateRequestToolChoiceParams] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[LogResponse]:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        log_id : str
            Unique identifier for the Log.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogUpdateRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[LogResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/log/{jsonable_encoder(log_id)}",
            method="PATCH",
            json={
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "reasoning_tokens": reasoning_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptLogUpdateRequestToolChoiceParams, direction="write"
                ),
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    LogResponse,
                    construct_type(
                        type_=LogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    @contextlib.asynccontextmanager
    async def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallStreamRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[AsyncHttpResponse[typing.AsyncIterator[PromptCallStreamResponse]]]:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[AsyncHttpResponse[typing.AsyncIterator[PromptCallStreamResponse]]]

        """
        async with self._client_wrapper.httpx_client.stream(
            "prompts/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptsCallStreamRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": prompts_call_stream_request_environment,
                "save": save,
                "log_id": log_id,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "num_samples": num_samples,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
                "stream": True,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:

            async def stream() -> AsyncHttpResponse[typing.AsyncIterator[PromptCallStreamResponse]]:
                try:
                    if 200 <= _response.status_code < 300:

                        async def _iter():
                            _event_source = httpx_sse.EventSource(_response)
                            async for _sse in _event_source.aiter_sse():
                                if _sse.data == None:
                                    return
                                try:
                                    yield _sse.data()
                                except Exception:
                                    pass
                            return

                        return AsyncHttpResponse(response=_response, data=_iter())
                    await _response.aread()
                    if _response.status_code == 422:
                        raise UnprocessableEntityError(
                            typing.cast(
                                HttpValidationError,
                                construct_type(
                                    type_=HttpValidationError,  # type: ignore
                                    object_=_response.json(),
                                ),
                            )
                        )
                    _response_json = _response.json()
                except JSONDecodeError:
                    raise ApiError(status_code=_response.status_code, body=_response.text)
                raise ApiError(status_code=_response.status_code, body=_response_json)

            yield await stream()

    async def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[PromptCallResponse]:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PromptCallResponse]

        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptsCallRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "log_status": log_status,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "user": user,
                "environment": prompts_call_request_environment,
                "save": save,
                "log_id": log_id,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "num_samples": num_samples,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
                "stream": False,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptCallResponse,
                    construct_type(
                        type_=PromptCallResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[PromptRequestTemplateParams] = OMIT,
        template_language: typing.Optional[TemplateLanguage] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[PromptRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        reasoning_effort: typing.Optional[PromptRequestReasoningEffortParams] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolFunctionParams]] = OMIT,
        linked_tools: typing.Optional[typing.Sequence[str]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        tags: typing.Optional[typing.Sequence[str]] = OMIT,
        readme: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[PromptResponse]:
        """
        Create a Prompt or update it with a new version if it already exists.

        Prompts are identified by the `ID` or their `path`. The parameters (i.e. the prompt template, temperature, model etc.) determine the versions of the Prompt.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within a Prompt - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[PromptRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        template_language : typing.Optional[TemplateLanguage]
            The template language to use for rendering the template.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[PromptRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        reasoning_effort : typing.Optional[PromptRequestReasoningEffortParams]
            Guidance on how many reasoning tokens it should generate before creating a response to the prompt. OpenAI reasoning models (o1, o3-mini) expect a OpenAIReasoningEffort enum. Anthropic reasoning models expect an integer, which signifies the maximum token budget.

        tools : typing.Optional[typing.Sequence[ToolFunctionParams]]
            The tool specification that the model can choose to call if Tool calling is supported.

        linked_tools : typing.Optional[typing.Sequence[str]]
            The IDs of the Tools in your organization that the model can choose to call if Tool calling is supported. The default deployed version of that tool is called.

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        version_name : typing.Optional[str]
            Unique name for the Prompt version. Version names must be unique for a given Prompt.

        version_description : typing.Optional[str]
            Description of the version, e.g., the changes made in this version.

        description : typing.Optional[str]
            Description of the Prompt.

        tags : typing.Optional[typing.Sequence[str]]
            List of tags associated with this prompt.

        readme : typing.Optional[str]
            Long description of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PromptResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts",
            method="POST",
            json={
                "path": path,
                "id": id,
                "model": model,
                "endpoint": endpoint,
                "template": convert_and_respect_annotation_metadata(
                    object_=template, annotation=PromptRequestTemplateParams, direction="write"
                ),
                "template_language": template_language,
                "provider": provider,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": convert_and_respect_annotation_metadata(
                    object_=stop, annotation=PromptRequestStopParams, direction="write"
                ),
                "presence_penalty": presence_penalty,
                "frequency_penalty": frequency_penalty,
                "other": other,
                "seed": seed,
                "response_format": convert_and_respect_annotation_metadata(
                    object_=response_format, annotation=ResponseFormatParams, direction="write"
                ),
                "reasoning_effort": convert_and_respect_annotation_metadata(
                    object_=reasoning_effort, annotation=PromptRequestReasoningEffortParams, direction="write"
                ),
                "tools": convert_and_respect_annotation_metadata(
                    object_=tools, annotation=typing.Sequence[ToolFunctionParams], direction="write"
                ),
                "linked_tools": linked_tools,
                "attributes": attributes,
                "version_name": version_name,
                "version_description": version_description,
                "description": description,
                "tags": tags,
                "readme": readme,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[PromptResponse]:
        """
        Retrieve the Prompt with the given ID.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PromptResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[None]:
        """
        Delete the Prompt with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[PromptResponse]:
        """
        Move the Prompt to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        path : typing.Optional[str]
            Path of the Prompt including the Prompt name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PromptResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "path": path,
                "name": name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def populate(
        self,
        id: str,
        *,
        request: typing.Dict[str, typing.Optional[typing.Any]],
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[PopulateTemplateResponse]:
        """
        Retrieve the Prompt with the given ID, including the populated template.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request : typing.Dict[str, typing.Optional[typing.Any]]

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve to populate the template.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from to populate the template.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PopulateTemplateResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/populate",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json=request,
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PopulateTemplateResponse,
                    construct_type(
                        type_=PopulateTemplateResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[ListPrompts]:
        """
        Get a list of all the versions of a Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[ListPrompts]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions",
            method="GET",
            params={
                "evaluator_aggregates": evaluator_aggregates,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    ListPrompts,
                    construct_type(
                        type_=ListPrompts,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_prompt_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[None]:
        """
        Delete a version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def patch_prompt_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[PromptResponse]:
        """
        Update the name or description of the Prompt version.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PromptResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}",
            method="PATCH",
            json={
                "name": name,
                "description": description,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[PromptResponse]:
        """
        Deploy Prompt to an Environment.

        Set the deployed version for the specified Environment. This Prompt
        will be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PromptResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="POST",
            params={
                "version_id": version_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[None]:
        """
        Remove deployed Prompt from the Environment.

        Remove the deployed version for the specified Environment. This Prompt
        will no longer be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[typing.List[FileEnvironmentResponse]]:
        """
        List all Environments and their deployed versions for the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[typing.List[FileEnvironmentResponse]]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    typing.List[FileEnvironmentResponse],
                    construct_type(
                        type_=typing.List[FileEnvironmentResponse],  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[PromptResponse]:
        """
        Activate and deactivate Evaluators for monitoring the Prompt.

        An activated Evaluator will automatically be run on all new Logs
        within the Prompt for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PromptResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/evaluators",
            method="POST",
            json={
                "activate": convert_and_respect_annotation_metadata(
                    object_=activate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams],
                    direction="write",
                ),
                "deactivate": convert_and_respect_annotation_metadata(
                    object_=deactivate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams],
                    direction="write",
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def serialize(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[None]:
        """
        Serialize a Prompt to the .prompt file format.

        Useful for storing the Prompt with your code in a version control system,
        or for editing with an AI tool.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/serialize",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def deserialize(
        self, *, prompt: str, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[PromptKernelRequest]:
        """
        Deserialize a Prompt from the .prompt file format.

        This returns a subset of the attributes required by a Prompt.
        This subset is the bit that defines the Prompt version (e.g. with `model` and `temperature` etc)

        Parameters
        ----------
        prompt : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PromptKernelRequest]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts/deserialize",
            method="POST",
            json={
                "prompt": prompt,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PromptKernelRequest,
                    construct_type(
                        type_=PromptKernelRequest,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
