# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from .raw_client import RawPromptsClient
from ..requests.chat_message import ChatMessageParams
from .requests.prompt_log_request_tool_choice import PromptLogRequestToolChoiceParams
from ..requests.prompt_kernel_request import PromptKernelRequestParams
import datetime as dt
from ..types.log_status import LogStatus
from ..core.request_options import RequestOptions
from ..types.create_prompt_log_response import CreatePromptLogResponse
from .requests.prompt_log_update_request_tool_choice import PromptLogUpdateRequestToolChoiceParams
from ..types.log_response import LogResponse
from .requests.prompts_call_stream_request_tool_choice import PromptsCallStreamRequestToolChoiceParams
from ..requests.provider_api_keys import ProviderApiKeysParams
from ..types.prompt_call_stream_response import PromptCallStreamResponse
from .requests.prompts_call_request_tool_choice import PromptsCallRequestToolChoiceParams
from ..types.prompt_call_response import PromptCallResponse
from ..types.project_sort_by import ProjectSortBy
from ..types.sort_order import SortOrder
from ..core.pagination import SyncPager
from ..types.prompt_response import PromptResponse
from ..types.paginated_data_prompt_response import PaginatedDataPromptResponse
from ..core.unchecked_base_model import construct_type
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from ..types.model_endpoints import ModelEndpoints
from .requests.prompt_request_template import PromptRequestTemplateParams
from ..types.template_language import TemplateLanguage
from ..types.model_providers import ModelProviders
from .requests.prompt_request_stop import PromptRequestStopParams
from ..requests.response_format import ResponseFormatParams
from .requests.prompt_request_reasoning_effort import PromptRequestReasoningEffortParams
from ..requests.tool_function import ToolFunctionParams
from ..types.populate_template_response import PopulateTemplateResponse
from ..types.list_prompts import ListPrompts
from ..types.file_environment_response import FileEnvironmentResponse
from ..requests.evaluator_activation_deactivation_request_activate_item import (
    EvaluatorActivationDeactivationRequestActivateItemParams,
)
from ..requests.evaluator_activation_deactivation_request_deactivate_item import (
    EvaluatorActivationDeactivationRequestDeactivateItemParams,
)
from ..types.prompt_kernel_request import PromptKernelRequest
from ..core.client_wrapper import AsyncClientWrapper
from .raw_client import AsyncRawPromptsClient
from ..core.pagination import AsyncPager

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class PromptsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawPromptsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawPromptsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawPromptsClient
        """
        return self._raw_client

    def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        run_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreatePromptLogResponse:
        """
        Log to a Prompt.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise, the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        run_id : typing.Optional[str]
            Unique identifier for the Run to associate the Log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreatePromptLogResponse
            Successful Response

        Examples
        --------
        import datetime

        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.log(
            path="persona",
            prompt={
                "model": "gpt-4",
                "template": [
                    {
                        "role": "system",
                        "content": "You are {{person}}. Answer questions as this person. Do not break character.",
                    }
                ],
            },
            messages=[{"role": "user", "content": "What really happened at Roswell?"}],
            inputs={"person": "Trump"},
            created_at=datetime.datetime.fromisoformat(
                "2024-07-18 23:29:35.178000+00:00",
            ),
            provider_latency=6.5931549072265625,
            output_message={
                "content": "Well, you know, there is so much secrecy involved in government, folks, it's unbelievable. They don't want to tell you everything. They don't tell me everything! But about Roswell, it's a very popular question. I know, I just know, that something very, very peculiar happened there. Was it a weather balloon? Maybe. Was it something extraterrestrial? Could be. I'd love to go down and open up all the classified documents, believe me, I would. But they don't let that happen. The Deep State, folks, the Deep State. They're unbelievable. They want to keep everything a secret. But whatever the truth is, I can tell you this: it's something big, very very big. Tremendous, in fact.",
                "role": "assistant",
            },
            prompt_tokens=100,
            output_tokens=220,
            prompt_cost=1e-05,
            output_cost=0.0002,
            finish_reason="stop",
        )
        """
        response = self._raw_client.log(
            version_id=version_id,
            environment=environment,
            run_id=run_id,
            path=path,
            id=id,
            output_message=output_message,
            prompt_tokens=prompt_tokens,
            reasoning_tokens=reasoning_tokens,
            output_tokens=output_tokens,
            prompt_cost=prompt_cost,
            output_cost=output_cost,
            finish_reason=finish_reason,
            messages=messages,
            tool_choice=tool_choice,
            prompt=prompt,
            start_time=start_time,
            end_time=end_time,
            output=output,
            created_at=created_at,
            error=error,
            provider_latency=provider_latency,
            stdout=stdout,
            provider_request=provider_request,
            provider_response=provider_response,
            inputs=inputs,
            source=source,
            metadata=metadata,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            prompt_log_request_environment=prompt_log_request_environment,
            save=save,
            log_id=log_id,
            request_options=request_options,
        )
        return response.data

    def update_log(
        self,
        id: str,
        log_id: str,
        *,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogUpdateRequestToolChoiceParams] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> LogResponse:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        log_id : str
            Unique identifier for the Log.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogUpdateRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        LogResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.update_log(
            id="id",
            log_id="log_id",
        )
        """
        response = self._raw_client.update_log(
            id,
            log_id,
            output_message=output_message,
            prompt_tokens=prompt_tokens,
            reasoning_tokens=reasoning_tokens,
            output_tokens=output_tokens,
            prompt_cost=prompt_cost,
            output_cost=output_cost,
            finish_reason=finish_reason,
            messages=messages,
            tool_choice=tool_choice,
            output=output,
            created_at=created_at,
            error=error,
            provider_latency=provider_latency,
            stdout=stdout,
            provider_request=provider_request,
            provider_response=provider_response,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            request_options=request_options,
        )
        return response.data

    def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallStreamRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[PromptCallStreamResponse]:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[PromptCallStreamResponse]


        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.prompts.call_stream()
        for chunk in response:
            yield chunk
        """
        with self._raw_client.call_stream(
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            messages=messages,
            tool_choice=tool_choice,
            prompt=prompt,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            prompts_call_stream_request_environment=prompts_call_stream_request_environment,
            save=save,
            log_id=log_id,
            provider_api_keys=provider_api_keys,
            num_samples=num_samples,
            return_inputs=return_inputs,
            logprobs=logprobs,
            suffix=suffix,
            request_options=request_options,
        ) as r:
            yield from r.data

    def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptCallResponse:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptCallResponse


        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.call(
            path="persona",
            prompt={
                "model": "gpt-4",
                "template": [
                    {
                        "role": "system",
                        "content": "You are stockbot. Return latest prices.",
                    }
                ],
                "tools": [
                    {
                        "name": "get_stock_price",
                        "description": "Get current stock price",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "ticker_symbol": {
                                    "type": "string",
                                    "name": "Ticker Symbol",
                                    "description": "Ticker symbol of the stock",
                                }
                            },
                            "required": [],
                        },
                    }
                ],
            },
            messages=[{"role": "user", "content": "latest apple"}],
        )
        """
        response = self._raw_client.call(
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            messages=messages,
            tool_choice=tool_choice,
            prompt=prompt,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            prompts_call_request_environment=prompts_call_request_environment,
            save=save,
            log_id=log_id,
            provider_api_keys=provider_api_keys,
            num_samples=num_samples,
            return_inputs=return_inputs,
            logprobs=logprobs,
            suffix=suffix,
            request_options=request_options,
        )
        return response.data

    def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[ProjectSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> SyncPager[PromptResponse]:
        """
        Get a list of all Prompts.

        Parameters
        ----------
        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Prompts to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Prompt name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Prompt. This filter matches against both email address and name of users.

        sort_by : typing.Optional[ProjectSortBy]
            Field to sort Prompts by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        SyncPager[PromptResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.prompts.list(
            size=1,
        )
        for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        for page in response.iter_pages():
            yield page
        """
        page = page if page is not None else 1
        _response = self._raw_client._client_wrapper.httpx_client.request(
            "prompts",
            method="GET",
            params={
                "page": page,
                "size": size,
                "name": name,
                "user_filter": user_filter,
                "sort_by": sort_by,
                "order": order,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(
                    PaginatedDataPromptResponse,
                    construct_type(
                        type_=PaginatedDataPromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                _has_next = True
                _get_next = lambda: self.list(
                    page=page + 1,
                    size=size,
                    name=name,
                    user_filter=user_filter,
                    sort_by=sort_by,
                    order=order,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return SyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[PromptRequestTemplateParams] = OMIT,
        template_language: typing.Optional[TemplateLanguage] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[PromptRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        reasoning_effort: typing.Optional[PromptRequestReasoningEffortParams] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolFunctionParams]] = OMIT,
        linked_tools: typing.Optional[typing.Sequence[str]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        tags: typing.Optional[typing.Sequence[str]] = OMIT,
        readme: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Create a Prompt or update it with a new version if it already exists.

        Prompts are identified by the `ID` or their `path`. The parameters (i.e. the prompt template, temperature, model etc.) determine the versions of the Prompt.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within a Prompt - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[PromptRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        template_language : typing.Optional[TemplateLanguage]
            The template language to use for rendering the template.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[PromptRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        reasoning_effort : typing.Optional[PromptRequestReasoningEffortParams]
            Guidance on how many reasoning tokens it should generate before creating a response to the prompt. OpenAI reasoning models (o1, o3-mini) expect a OpenAIReasoningEffort enum. Anthropic reasoning models expect an integer, which signifies the maximum token budget.

        tools : typing.Optional[typing.Sequence[ToolFunctionParams]]
            The tool specification that the model can choose to call if Tool calling is supported.

        linked_tools : typing.Optional[typing.Sequence[str]]
            The IDs of the Tools in your organization that the model can choose to call if Tool calling is supported. The default deployed version of that tool is called.

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        version_name : typing.Optional[str]
            Unique name for the Prompt version. Version names must be unique for a given Prompt.

        version_description : typing.Optional[str]
            Description of the version, e.g., the changes made in this version.

        description : typing.Optional[str]
            Description of the Prompt.

        tags : typing.Optional[typing.Sequence[str]]
            List of tags associated with this prompt.

        readme : typing.Optional[str]
            Long description of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.upsert(
            path="Personal Projects/Coding Assistant",
            model="gpt-4o",
            endpoint="chat",
            template=[
                {
                    "content": "You are a helpful coding assistant specialising in {{language}}",
                    "role": "system",
                }
            ],
            provider="openai",
            max_tokens=-1,
            temperature=0.7,
            version_name="coding-assistant-v1",
            version_description="Initial version",
        )
        """
        response = self._raw_client.upsert(
            model=model,
            path=path,
            id=id,
            endpoint=endpoint,
            template=template,
            template_language=template_language,
            provider=provider,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            stop=stop,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            other=other,
            seed=seed,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            tools=tools,
            linked_tools=linked_tools,
            attributes=attributes,
            version_name=version_name,
            version_description=version_description,
            description=description,
            tags=tags,
            readme=readme,
            request_options=request_options,
        )
        return response.data

    def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Retrieve the Prompt with the given ID.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.get(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        response = self._raw_client.get(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Prompt with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.delete(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        response = self._raw_client.delete(id, request_options=request_options)
        return response.data

    def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Move the Prompt to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        path : typing.Optional[str]
            Path of the Prompt including the Prompt name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.move(
            id="pr_30gco7dx6JDq4200GVOHa",
            path="new directory/new name",
        )
        """
        response = self._raw_client.move(id, path=path, name=name, request_options=request_options)
        return response.data

    def populate(
        self,
        id: str,
        *,
        request: typing.Dict[str, typing.Optional[typing.Any]],
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PopulateTemplateResponse:
        """
        Retrieve the Prompt with the given ID, including the populated template.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request : typing.Dict[str, typing.Optional[typing.Any]]

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve to populate the template.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from to populate the template.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PopulateTemplateResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.populate(
            id="id",
            request={"key": "value"},
        )
        """
        response = self._raw_client.populate(
            id, request=request, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListPrompts:
        """
        Get a list of all the versions of a Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListPrompts
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.list_versions(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        response = self._raw_client.list_versions(
            id, evaluator_aggregates=evaluator_aggregates, request_options=request_options
        )
        return response.data

    def delete_prompt_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.delete_prompt_version(
            id="id",
            version_id="version_id",
        )
        """
        response = self._raw_client.delete_prompt_version(id, version_id, request_options=request_options)
        return response.data

    def patch_prompt_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Update the name or description of the Prompt version.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.patch_prompt_version(
            id="id",
            version_id="version_id",
        )
        """
        response = self._raw_client.patch_prompt_version(
            id, version_id, name=name, description=description, request_options=request_options
        )
        return response.data

    def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Deploy Prompt to an Environment.

        Set the deployed version for the specified Environment. This Prompt
        will be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.set_deployment(
            id="id",
            environment_id="environment_id",
            version_id="version_id",
        )
        """
        response = self._raw_client.set_deployment(
            id, environment_id, version_id=version_id, request_options=request_options
        )
        return response.data

    def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Prompt from the Environment.

        Remove the deployed version for the specified Environment. This Prompt
        will no longer be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.remove_deployment(
            id="id",
            environment_id="environment_id",
        )
        """
        response = self._raw_client.remove_deployment(id, environment_id, request_options=request_options)
        return response.data

    def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.list_environments(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        response = self._raw_client.list_environments(id, request_options=request_options)
        return response.data

    def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Activate and deactivate Evaluators for monitoring the Prompt.

        An activated Evaluator will automatically be run on all new Logs
        within the Prompt for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.update_monitoring(
            id="pr_30gco7dx6JDq4200GVOHa",
            activate=[{"evaluator_version_id": "evv_1abc4308abd"}],
        )
        """
        response = self._raw_client.update_monitoring(
            id, activate=activate, deactivate=deactivate, request_options=request_options
        )
        return response.data

    def serialize(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Serialize a Prompt to the .prompt file format.

        Useful for storing the Prompt with your code in a version control system,
        or for editing with an AI tool.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.serialize(
            id="id",
        )
        """
        response = self._raw_client.serialize(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    def deserialize(
        self, *, prompt: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptKernelRequest:
        """
        Deserialize a Prompt from the .prompt file format.

        This returns a subset of the attributes required by a Prompt.
        This subset is the bit that defines the Prompt version (e.g. with `model` and `temperature` etc)

        Parameters
        ----------
        prompt : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptKernelRequest
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.deserialize(
            prompt="prompt",
        )
        """
        response = self._raw_client.deserialize(prompt=prompt, request_options=request_options)
        return response.data


class AsyncPromptsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawPromptsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawPromptsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawPromptsClient
        """
        return self._raw_client

    async def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        run_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreatePromptLogResponse:
        """
        Log to a Prompt.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise, the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        run_id : typing.Optional[str]
            Unique identifier for the Run to associate the Log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreatePromptLogResponse
            Successful Response

        Examples
        --------
        import asyncio
        import datetime

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.log(
                path="persona",
                prompt={
                    "model": "gpt-4",
                    "template": [
                        {
                            "role": "system",
                            "content": "You are {{person}}. Answer questions as this person. Do not break character.",
                        }
                    ],
                },
                messages=[
                    {"role": "user", "content": "What really happened at Roswell?"}
                ],
                inputs={"person": "Trump"},
                created_at=datetime.datetime.fromisoformat(
                    "2024-07-18 23:29:35.178000+00:00",
                ),
                provider_latency=6.5931549072265625,
                output_message={
                    "content": "Well, you know, there is so much secrecy involved in government, folks, it's unbelievable. They don't want to tell you everything. They don't tell me everything! But about Roswell, it's a very popular question. I know, I just know, that something very, very peculiar happened there. Was it a weather balloon? Maybe. Was it something extraterrestrial? Could be. I'd love to go down and open up all the classified documents, believe me, I would. But they don't let that happen. The Deep State, folks, the Deep State. They're unbelievable. They want to keep everything a secret. But whatever the truth is, I can tell you this: it's something big, very very big. Tremendous, in fact.",
                    "role": "assistant",
                },
                prompt_tokens=100,
                output_tokens=220,
                prompt_cost=1e-05,
                output_cost=0.0002,
                finish_reason="stop",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.log(
            version_id=version_id,
            environment=environment,
            run_id=run_id,
            path=path,
            id=id,
            output_message=output_message,
            prompt_tokens=prompt_tokens,
            reasoning_tokens=reasoning_tokens,
            output_tokens=output_tokens,
            prompt_cost=prompt_cost,
            output_cost=output_cost,
            finish_reason=finish_reason,
            messages=messages,
            tool_choice=tool_choice,
            prompt=prompt,
            start_time=start_time,
            end_time=end_time,
            output=output,
            created_at=created_at,
            error=error,
            provider_latency=provider_latency,
            stdout=stdout,
            provider_request=provider_request,
            provider_response=provider_response,
            inputs=inputs,
            source=source,
            metadata=metadata,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            prompt_log_request_environment=prompt_log_request_environment,
            save=save,
            log_id=log_id,
            request_options=request_options,
        )
        return response.data

    async def update_log(
        self,
        id: str,
        log_id: str,
        *,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        reasoning_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogUpdateRequestToolChoiceParams] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> LogResponse:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        log_id : str
            Unique identifier for the Log.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        reasoning_tokens : typing.Optional[int]
            Number of reasoning tokens used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogUpdateRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        LogResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.update_log(
                id="id",
                log_id="log_id",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.update_log(
            id,
            log_id,
            output_message=output_message,
            prompt_tokens=prompt_tokens,
            reasoning_tokens=reasoning_tokens,
            output_tokens=output_tokens,
            prompt_cost=prompt_cost,
            output_cost=output_cost,
            finish_reason=finish_reason,
            messages=messages,
            tool_choice=tool_choice,
            output=output,
            created_at=created_at,
            error=error,
            provider_latency=provider_latency,
            stdout=stdout,
            provider_request=provider_request,
            provider_response=provider_response,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            request_options=request_options,
        )
        return response.data

    async def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallStreamRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[PromptCallStreamResponse]:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[PromptCallStreamResponse]


        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.prompts.call_stream()
            async for chunk in response:
                yield chunk


        asyncio.run(main())
        """
        async with self._raw_client.call_stream(
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            messages=messages,
            tool_choice=tool_choice,
            prompt=prompt,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            prompts_call_stream_request_environment=prompts_call_stream_request_environment,
            save=save,
            log_id=log_id,
            provider_api_keys=provider_api_keys,
            num_samples=num_samples,
            return_inputs=return_inputs,
            logprobs=logprobs,
            suffix=suffix,
            request_options=request_options,
        ) as r:
            async for data in r.data:
                yield data

    async def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        log_status: typing.Optional[LogStatus] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        log_id: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptCallResponse:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model must call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        log_status : typing.Optional[LogStatus]
            Status of a Log. Set to `incomplete` if you intend to update and eventually complete the Log and want the File's monitoring Evaluators to wait until you mark it as `complete`. If log_status is not provided, observability will pick up the Log as soon as possible. Updating this from specified to unspecified is undefined behavior.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        log_id : typing.Optional[str]
            This will identify a Log. If you don't provide a Log ID, Humanloop will generate one for you.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptCallResponse


        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.call(
                path="persona",
                prompt={
                    "model": "gpt-4",
                    "template": [
                        {
                            "role": "system",
                            "content": "You are stockbot. Return latest prices.",
                        }
                    ],
                    "tools": [
                        {
                            "name": "get_stock_price",
                            "description": "Get current stock price",
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    "ticker_symbol": {
                                        "type": "string",
                                        "name": "Ticker Symbol",
                                        "description": "Ticker symbol of the stock",
                                    }
                                },
                                "required": [],
                            },
                        }
                    ],
                },
                messages=[{"role": "user", "content": "latest apple"}],
            )


        asyncio.run(main())
        """
        response = await self._raw_client.call(
            version_id=version_id,
            environment=environment,
            path=path,
            id=id,
            messages=messages,
            tool_choice=tool_choice,
            prompt=prompt,
            inputs=inputs,
            source=source,
            metadata=metadata,
            start_time=start_time,
            end_time=end_time,
            log_status=log_status,
            source_datapoint_id=source_datapoint_id,
            trace_parent_id=trace_parent_id,
            user=user,
            prompts_call_request_environment=prompts_call_request_environment,
            save=save,
            log_id=log_id,
            provider_api_keys=provider_api_keys,
            num_samples=num_samples,
            return_inputs=return_inputs,
            logprobs=logprobs,
            suffix=suffix,
            request_options=request_options,
        )
        return response.data

    async def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[ProjectSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncPager[PromptResponse]:
        """
        Get a list of all Prompts.

        Parameters
        ----------
        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Prompts to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Prompt name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Prompt. This filter matches against both email address and name of users.

        sort_by : typing.Optional[ProjectSortBy]
            Field to sort Prompts by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncPager[PromptResponse]
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.prompts.list(
                size=1,
            )
            async for item in response:
                yield item
            # alternatively, you can paginate page-by-page
            async for page in response.iter_pages():
                yield page


        asyncio.run(main())
        """
        page = page if page is not None else 1
        _response = await self._raw_client._client_wrapper.httpx_client.request(
            "prompts",
            method="GET",
            params={
                "page": page,
                "size": size,
                "name": name,
                "user_filter": user_filter,
                "sort_by": sort_by,
                "order": order,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(
                    PaginatedDataPromptResponse,
                    construct_type(
                        type_=PaginatedDataPromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                _has_next = True
                _get_next = lambda: self.list(
                    page=page + 1,
                    size=size,
                    name=name,
                    user_filter=user_filter,
                    sort_by=sort_by,
                    order=order,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return AsyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[PromptRequestTemplateParams] = OMIT,
        template_language: typing.Optional[TemplateLanguage] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[PromptRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        reasoning_effort: typing.Optional[PromptRequestReasoningEffortParams] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolFunctionParams]] = OMIT,
        linked_tools: typing.Optional[typing.Sequence[str]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        version_name: typing.Optional[str] = OMIT,
        version_description: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        tags: typing.Optional[typing.Sequence[str]] = OMIT,
        readme: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Create a Prompt or update it with a new version if it already exists.

        Prompts are identified by the `ID` or their `path`. The parameters (i.e. the prompt template, temperature, model etc.) determine the versions of the Prompt.

        You can provide `version_name` and `version_description` to identify and describe your versions.
        Version names must be unique within a Prompt - attempting to create a version with a name
        that already exists will result in a 409 Conflict error.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[PromptRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        template_language : typing.Optional[TemplateLanguage]
            The template language to use for rendering the template.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[PromptRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        reasoning_effort : typing.Optional[PromptRequestReasoningEffortParams]
            Guidance on how many reasoning tokens it should generate before creating a response to the prompt. OpenAI reasoning models (o1, o3-mini) expect a OpenAIReasoningEffort enum. Anthropic reasoning models expect an integer, which signifies the maximum token budget.

        tools : typing.Optional[typing.Sequence[ToolFunctionParams]]
            The tool specification that the model can choose to call if Tool calling is supported.

        linked_tools : typing.Optional[typing.Sequence[str]]
            The IDs of the Tools in your organization that the model can choose to call if Tool calling is supported. The default deployed version of that tool is called.

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        version_name : typing.Optional[str]
            Unique name for the Prompt version. Version names must be unique for a given Prompt.

        version_description : typing.Optional[str]
            Description of the version, e.g., the changes made in this version.

        description : typing.Optional[str]
            Description of the Prompt.

        tags : typing.Optional[typing.Sequence[str]]
            List of tags associated with this prompt.

        readme : typing.Optional[str]
            Long description of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.upsert(
                path="Personal Projects/Coding Assistant",
                model="gpt-4o",
                endpoint="chat",
                template=[
                    {
                        "content": "You are a helpful coding assistant specialising in {{language}}",
                        "role": "system",
                    }
                ],
                provider="openai",
                max_tokens=-1,
                temperature=0.7,
                version_name="coding-assistant-v1",
                version_description="Initial version",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.upsert(
            model=model,
            path=path,
            id=id,
            endpoint=endpoint,
            template=template,
            template_language=template_language,
            provider=provider,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            stop=stop,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            other=other,
            seed=seed,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            tools=tools,
            linked_tools=linked_tools,
            attributes=attributes,
            version_name=version_name,
            version_description=version_description,
            description=description,
            tags=tags,
            readme=readme,
            request_options=request_options,
        )
        return response.data

    async def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Retrieve the Prompt with the given ID.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.get(
                id="pr_30gco7dx6JDq4200GVOHa",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.get(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    async def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Prompt with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.delete(
                id="pr_30gco7dx6JDq4200GVOHa",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.delete(id, request_options=request_options)
        return response.data

    async def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Move the Prompt to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        path : typing.Optional[str]
            Path of the Prompt including the Prompt name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.move(
                id="pr_30gco7dx6JDq4200GVOHa",
                path="new directory/new name",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.move(id, path=path, name=name, request_options=request_options)
        return response.data

    async def populate(
        self,
        id: str,
        *,
        request: typing.Dict[str, typing.Optional[typing.Any]],
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PopulateTemplateResponse:
        """
        Retrieve the Prompt with the given ID, including the populated template.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request : typing.Dict[str, typing.Optional[typing.Any]]

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve to populate the template.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from to populate the template.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PopulateTemplateResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.populate(
                id="id",
                request={"key": "value"},
            )


        asyncio.run(main())
        """
        response = await self._raw_client.populate(
            id, request=request, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    async def list_versions(
        self,
        id: str,
        *,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListPrompts:
        """
        Get a list of all the versions of a Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListPrompts
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.list_versions(
                id="pr_30gco7dx6JDq4200GVOHa",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.list_versions(
            id, evaluator_aggregates=evaluator_aggregates, request_options=request_options
        )
        return response.data

    async def delete_prompt_version(
        self, id: str, version_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.delete_prompt_version(
                id="id",
                version_id="version_id",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.delete_prompt_version(id, version_id, request_options=request_options)
        return response.data

    async def patch_prompt_version(
        self,
        id: str,
        version_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Update the name or description of the Prompt version.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        name : typing.Optional[str]
            Name of the version.

        description : typing.Optional[str]
            Description of the version.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.patch_prompt_version(
                id="id",
                version_id="version_id",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.patch_prompt_version(
            id, version_id, name=name, description=description, request_options=request_options
        )
        return response.data

    async def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Deploy Prompt to an Environment.

        Set the deployed version for the specified Environment. This Prompt
        will be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.set_deployment(
                id="id",
                environment_id="environment_id",
                version_id="version_id",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.set_deployment(
            id, environment_id, version_id=version_id, request_options=request_options
        )
        return response.data

    async def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Prompt from the Environment.

        Remove the deployed version for the specified Environment. This Prompt
        will no longer be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.remove_deployment(
                id="id",
                environment_id="environment_id",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.remove_deployment(id, environment_id, request_options=request_options)
        return response.data

    async def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.list_environments(
                id="pr_30gco7dx6JDq4200GVOHa",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.list_environments(id, request_options=request_options)
        return response.data

    async def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Activate and deactivate Evaluators for monitoring the Prompt.

        An activated Evaluator will automatically be run on all new Logs
        within the Prompt for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.update_monitoring(
                id="pr_30gco7dx6JDq4200GVOHa",
                activate=[{"evaluator_version_id": "evv_1abc4308abd"}],
            )


        asyncio.run(main())
        """
        response = await self._raw_client.update_monitoring(
            id, activate=activate, deactivate=deactivate, request_options=request_options
        )
        return response.data

    async def serialize(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Serialize a Prompt to the .prompt file format.

        Useful for storing the Prompt with your code in a version control system,
        or for editing with an AI tool.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.serialize(
                id="id",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.serialize(
            id, version_id=version_id, environment=environment, request_options=request_options
        )
        return response.data

    async def deserialize(
        self, *, prompt: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptKernelRequest:
        """
        Deserialize a Prompt from the .prompt file format.

        This returns a subset of the attributes required by a Prompt.
        This subset is the bit that defines the Prompt version (e.g. with `model` and `temperature` etc)

        Parameters
        ----------
        prompt : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptKernelRequest
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.deserialize(
                prompt="prompt",
            )


        asyncio.run(main())
        """
        response = await self._raw_client.deserialize(prompt=prompt, request_options=request_options)
        return response.data
