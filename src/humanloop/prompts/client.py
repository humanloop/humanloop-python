# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing
from json.decoder import JSONDecodeError

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.jsonable_encoder import jsonable_encoder
from ..core.pagination import AsyncPager, SyncPager
from ..core.request_options import RequestOptions
from ..core.unchecked_base_model import construct_type
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.chat_message import ChatMessage
from ..types.create_prompt_log_response import CreatePromptLogResponse
from ..types.evaluator_activation_deactivation_request_activate_item import (
    EvaluatorActivationDeactivationRequestActivateItem,
)
from ..types.evaluator_activation_deactivation_request_deactivate_item import (
    EvaluatorActivationDeactivationRequestDeactivateItem,
)
from ..types.file_environment_response import FileEnvironmentResponse
from ..types.http_validation_error import HttpValidationError
from ..types.list_prompts import ListPrompts
from ..types.model_endpoints import ModelEndpoints
from ..types.model_providers import ModelProviders
from ..types.paginated_data_prompt_response import PaginatedDataPromptResponse
from ..types.project_sort_by import ProjectSortBy
from ..types.prompt_kernel_request import PromptKernelRequest
from ..types.prompt_response import PromptResponse
from ..types.provider_api_keys import ProviderApiKeys
from ..types.response_format import ResponseFormat
from ..types.sort_order import SortOrder
from ..types.tool_function import ToolFunction
from ..types.version_status import VersionStatus
from .types.call_prompts_call_post_response import CallPromptsCallPostResponse
from .types.prompt_call_request_tool_choice import PromptCallRequestToolChoice
from .types.prompt_log_request_tool_choice import PromptLogRequestToolChoice
from .types.prompt_request_stop import PromptRequestStop
from .types.prompt_request_template import PromptRequestTemplate

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class PromptsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessage] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        prompt: typing.Optional[PromptKernelRequest] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessage]] = OMIT,
        tool_choice: typing.Optional[PromptLogRequestToolChoice] = OMIT,
        output: typing.Optional[str] = OMIT,
        raw_output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        session_id: typing.Optional[str] = OMIT,
        parent_id: typing.Optional[str] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        save: typing.Optional[bool] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        batches: typing.Optional[typing.Sequence[str]] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_log_request_environment: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreatePromptLogResponse:
        """
        Log to a Prompt.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt, if not we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name, which is used as a unique identifier.

        id : typing.Optional[str]
            ID for an existing Prompt to update.

        output_message : typing.Optional[ChatMessage]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        prompt : typing.Optional[PromptKernelRequest]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        messages : typing.Optional[typing.Sequence[ChatMessage]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogRequestToolChoice]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        raw_output : typing.Optional[str]
            Raw output from the provider.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        provider_request : typing.Optional[typing.Dict[str, typing.Any]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Any]]
            Raw response received the provider.

        session_id : typing.Optional[str]
            Unique identifier for the Session to associate the Log to. Allows you to record multiple Logs to a Session (using an ID kept by your internal systems) by passing the same `session_id` in subsequent log requests.

        parent_id : typing.Optional[str]
            Unique identifier for the parent Log in a Session. Should only be provided if `session_id` is provided. If provided, the Log will be nested under the parent Log within the Session.

        inputs : typing.Optional[typing.Dict[str, typing.Any]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Any]]
            Any additional metadata to record.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        batches : typing.Optional[typing.Sequence[str]]
            Array of Batch Ids that this log is part of. Batches are used to group Logs together for offline Evaluations

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreatePromptLogResponse
            Successful Response

        Examples
        --------
        from humanloop import ChatMessage, PromptKernelRequest
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.log(
            path="persona",
            prompt=PromptKernelRequest(
                model="gpt-4",
                template=[
                    ChatMessage(
                        role="system",
                        content="You are {{person}}. Answer questions as this person. Do not break character.",
                    )
                ],
            ),
            messages=[
                ChatMessage(
                    role="user",
                    content="What really happened at Roswell?",
                )
            ],
            inputs={"person": "Trump"},
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts/log",
            method="POST",
            params={"version_id": version_id, "environment": environment},
            json={
                "path": path,
                "id": id,
                "output_message": output_message,
                "prompt_tokens": prompt_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "prompt": prompt,
                "messages": messages,
                "tool_choice": tool_choice,
                "output": output,
                "raw_output": raw_output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "session_id": session_id,
                "parent_id": parent_id,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "save": save,
                "source_datapoint_id": source_datapoint_id,
                "batches": batches,
                "user": user,
                "environment": prompt_log_request_environment,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(CreatePromptLogResponse, construct_type(type_=CreatePromptLogResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        prompt: typing.Optional[PromptKernelRequest] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessage]] = OMIT,
        tool_choice: typing.Optional[PromptCallRequestToolChoice] = OMIT,
        session_id: typing.Optional[str] = OMIT,
        parent_id: typing.Optional[str] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        save: typing.Optional[bool] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        batches: typing.Optional[typing.Sequence[str]] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_call_request_environment: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeys] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CallPromptsCallPostResponse:
        """
        Call a Prompt.

        Calling a Prompt subsequently calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt, if not we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name, which is used as a unique identifier.

        id : typing.Optional[str]
            ID for an existing Prompt to update.

        prompt : typing.Optional[PromptKernelRequest]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        messages : typing.Optional[typing.Sequence[ChatMessage]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptCallRequestToolChoice]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        session_id : typing.Optional[str]
            Unique identifier for the Session to associate the Log to. Allows you to record multiple Logs to a Session (using an ID kept by your internal systems) by passing the same `session_id` in subsequent log requests.

        parent_id : typing.Optional[str]
            Unique identifier for the parent Log in a Session. Should only be provided if `session_id` is provided. If provided, the Log will be nested under the parent Log within the Session.

        inputs : typing.Optional[typing.Dict[str, typing.Any]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Any]]
            Any additional metadata to record.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        batches : typing.Optional[typing.Sequence[str]]
            Array of Batch Ids that this log is part of. Batches are used to group Logs together for offline Evaluations

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        provider_api_keys : typing.Optional[ProviderApiKeys]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        stream : typing.Optional[bool]
            If true, tokens will be sent as data-only server-sent events. If num_samples > 1, samples are streamed back independently.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CallPromptsCallPostResponse
            Successful Response

        Examples
        --------
        from humanloop import ChatMessage, PromptKernelRequest
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.call(
            path="persona",
            prompt=PromptKernelRequest(
                model="gpt-4",
                template=[
                    ChatMessage(
                        role="system",
                        content="You are {{person}}. Answer any questions as this person. Do not break character.",
                    )
                ],
            ),
            messages=[
                ChatMessage(
                    role="user",
                    content="What really happened at Roswell?",
                )
            ],
            inputs={"person": "Trump"},
            stream=False,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts/call",
            method="POST",
            params={"version_id": version_id, "environment": environment},
            json={
                "path": path,
                "id": id,
                "prompt": prompt,
                "messages": messages,
                "tool_choice": tool_choice,
                "session_id": session_id,
                "parent_id": parent_id,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "save": save,
                "source_datapoint_id": source_datapoint_id,
                "batches": batches,
                "user": user,
                "environment": prompt_call_request_environment,
                "provider_api_keys": provider_api_keys,
                "num_samples": num_samples,
                "stream": stream,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(CallPromptsCallPostResponse, construct_type(type_=CallPromptsCallPostResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[ProjectSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> SyncPager[PromptResponse]:
        """
        Get a list of all Prompts.

        Parameters
        ----------
        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Prompts to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Prompt name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Prompt. This filter matches against both email address and name of users.

        sort_by : typing.Optional[ProjectSortBy]
            Field to sort Prompts by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        SyncPager[PromptResponse]
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.prompts.list(
            size=1,
        )
        for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        for page in response.iter_pages():
            yield page
        """
        page = page or 1
        _response = self._client_wrapper.httpx_client.request(
            "prompts",
            method="GET",
            params={
                "page": page,
                "size": size,
                "name": name,
                "user_filter": user_filter,
                "sort_by": sort_by,
                "order": order,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(PaginatedDataPromptResponse, construct_type(type_=PaginatedDataPromptResponse, object_=_response.json()))  # type: ignore
                _has_next = True
                _get_next = lambda: self.list(
                    page=page + 1,
                    size=size,
                    name=name,
                    user_filter=user_filter,
                    sort_by=sort_by,
                    order=order,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return SyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[PromptRequestTemplate] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[PromptRequestStop] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormat] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolFunction]] = OMIT,
        linked_tools: typing.Optional[typing.Sequence[str]] = OMIT,
        commit_message: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Create a Prompt or update it with a new version if it already exists.

        Prompts are identified by the `ID` or their `path`. The parameters (i.e. the prompt template, temperature, model etc.) determine the versions of the Prompt.

        If you provide a commit message, then the new version will be committed;
        otherwise it will be uncommitted. If you try to commit an already committed version,
        an exception will be raised.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/supported-models)

        path : typing.Optional[str]
            Path of the Prompt, including the name, which is used as a unique identifier.

        id : typing.Optional[str]
            ID for an existing Prompt to update.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[PromptRequestTemplate]
            For chat endpoint, provide a Chat template. For completion endpoint, provide a Prompt template. Input variables within the template should be specified with double curly bracket syntax: {{INPUT_NAME}}.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[PromptRequestStop]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Any]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormat]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        tools : typing.Optional[typing.Sequence[ToolFunction]]
            The tool specification that the model can choose to call if Tool calling is supported.

        linked_tools : typing.Optional[typing.Sequence[str]]
            The IDs of the Tools in your organization that the model can choose to call if Tool calling is supported. The default deployed version of that tool is called.

        commit_message : typing.Optional[str]
            Message describing the changes made.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import ChatMessage
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.upsert(
            path="Personal Projects/Coding Assistant",
            model="gpt-4o",
            endpoint="chat",
            template=[
                ChatMessage(
                    content="You are a helpful coding assistant specialising in {{language}}",
                    role="system",
                )
            ],
            provider="openai",
            max_tokens=-1,
            temperature=0.7,
            top_p=1.0,
            presence_penalty=0.0,
            frequency_penalty=0.0,
            other={},
            tools=[],
            linked_tools=[],
            commit_message="Initial commit",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts",
            method="POST",
            json={
                "path": path,
                "id": id,
                "model": model,
                "endpoint": endpoint,
                "template": template,
                "provider": provider,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": stop,
                "presence_penalty": presence_penalty,
                "frequency_penalty": frequency_penalty,
                "other": other,
                "seed": seed,
                "response_format": response_format,
                "tools": tools,
                "linked_tools": linked_tools,
                "commit_message": commit_message,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Retrieve the Prompt with the given ID.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.get(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="GET",
            params={"version_id": version_id, "environment": environment},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Prompt with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.delete(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}", method="DELETE", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Move the Prompt to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        path : typing.Optional[str]
            Path of the Prompt including the Prompt name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.move(
            id="pr_30gco7dx6JDq4200GVOHa",
            path="new directory/new name",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="PATCH",
            json={"path": path, "name": name},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_versions(
        self,
        id: str,
        *,
        status: typing.Optional[VersionStatus] = None,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListPrompts:
        """
        Get a list of all the versions of a Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        status : typing.Optional[VersionStatus]
            Filter versions by status: 'uncommitted', 'committed'. If no status is provided, all versions are returned.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListPrompts
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.list_versions(
            id="pr_30gco7dx6JDq4200GVOHa",
            status="committed",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions",
            method="GET",
            params={"status": status, "evaluator_aggregates": evaluator_aggregates},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(ListPrompts, construct_type(type_=ListPrompts, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def commit(
        self, id: str, version_id: str, *, commit_message: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Commit a version of the Prompt with a commit message.

        If the version is already committed, an exception will be raised.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        commit_message : str
            Message describing the changes made.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.commit(
            id="pr_30gco7dx6JDq4200GVOHa",
            version_id="prv_F34aba5f3asp0",
            commit_message="Reiterated point about not discussing sentience",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}/commit",
            method="POST",
            json={"commit_message": commit_message},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItem]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItem]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Activate and deactivate Evaluators for monitoring the Prompt.

        An activated Evaluator will automatically be run on all new Logs
        within the Prompt for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItem]]
            Evaluators to activate on Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItem]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import MonitoringEvaluatorVersionRequest
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.update_monitoring(
            id="pr_30gco7dx6JDq4200GVOHa",
            activate=[
                MonitoringEvaluatorVersionRequest(
                    evaluator_version_id="evv_1abc4308abd",
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/evaluators",
            method="POST",
            json={"activate": activate, "deactivate": deactivate},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Deploy Prompt to an Environment.

        Set the deployed version for the specified Environment. This Prompt
        will be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.set_deployment(
            id="id",
            environment_id="environment_id",
            version_id="version_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="POST",
            params={"version_id": version_id},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Prompt from the Environment.

        Remove the deployed version for the specified Environment. This Prompt
        will no longer be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.remove_deployment(
            id="id",
            environment_id="environment_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        from humanloop.client import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.list_environments(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments", method="GET", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(typing.List[FileEnvironmentResponse], construct_type(type_=typing.List[FileEnvironmentResponse], object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncPromptsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessage] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        prompt: typing.Optional[PromptKernelRequest] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessage]] = OMIT,
        tool_choice: typing.Optional[PromptLogRequestToolChoice] = OMIT,
        output: typing.Optional[str] = OMIT,
        raw_output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        session_id: typing.Optional[str] = OMIT,
        parent_id: typing.Optional[str] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        save: typing.Optional[bool] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        batches: typing.Optional[typing.Sequence[str]] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_log_request_environment: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreatePromptLogResponse:
        """
        Log to a Prompt.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt, if not we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name, which is used as a unique identifier.

        id : typing.Optional[str]
            ID for an existing Prompt to update.

        output_message : typing.Optional[ChatMessage]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        prompt : typing.Optional[PromptKernelRequest]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        messages : typing.Optional[typing.Sequence[ChatMessage]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogRequestToolChoice]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        raw_output : typing.Optional[str]
            Raw output from the provider.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        provider_request : typing.Optional[typing.Dict[str, typing.Any]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Any]]
            Raw response received the provider.

        session_id : typing.Optional[str]
            Unique identifier for the Session to associate the Log to. Allows you to record multiple Logs to a Session (using an ID kept by your internal systems) by passing the same `session_id` in subsequent log requests.

        parent_id : typing.Optional[str]
            Unique identifier for the parent Log in a Session. Should only be provided if `session_id` is provided. If provided, the Log will be nested under the parent Log within the Session.

        inputs : typing.Optional[typing.Dict[str, typing.Any]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Any]]
            Any additional metadata to record.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        batches : typing.Optional[typing.Sequence[str]]
            Array of Batch Ids that this log is part of. Batches are used to group Logs together for offline Evaluations

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreatePromptLogResponse
            Successful Response

        Examples
        --------
        from humanloop import ChatMessage, PromptKernelRequest
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.log(
            path="persona",
            prompt=PromptKernelRequest(
                model="gpt-4",
                template=[
                    ChatMessage(
                        role="system",
                        content="You are {{person}}. Answer questions as this person. Do not break character.",
                    )
                ],
            ),
            messages=[
                ChatMessage(
                    role="user",
                    content="What really happened at Roswell?",
                )
            ],
            inputs={"person": "Trump"},
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts/log",
            method="POST",
            params={"version_id": version_id, "environment": environment},
            json={
                "path": path,
                "id": id,
                "output_message": output_message,
                "prompt_tokens": prompt_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "prompt": prompt,
                "messages": messages,
                "tool_choice": tool_choice,
                "output": output,
                "raw_output": raw_output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "session_id": session_id,
                "parent_id": parent_id,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "save": save,
                "source_datapoint_id": source_datapoint_id,
                "batches": batches,
                "user": user,
                "environment": prompt_log_request_environment,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(CreatePromptLogResponse, construct_type(type_=CreatePromptLogResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        prompt: typing.Optional[PromptKernelRequest] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessage]] = OMIT,
        tool_choice: typing.Optional[PromptCallRequestToolChoice] = OMIT,
        session_id: typing.Optional[str] = OMIT,
        parent_id: typing.Optional[str] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        save: typing.Optional[bool] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        batches: typing.Optional[typing.Sequence[str]] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_call_request_environment: typing.Optional[str] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeys] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CallPromptsCallPostResponse:
        """
        Call a Prompt.

        Calling a Prompt subsequently calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt, if not we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name, which is used as a unique identifier.

        id : typing.Optional[str]
            ID for an existing Prompt to update.

        prompt : typing.Optional[PromptKernelRequest]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        messages : typing.Optional[typing.Sequence[ChatMessage]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptCallRequestToolChoice]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        session_id : typing.Optional[str]
            Unique identifier for the Session to associate the Log to. Allows you to record multiple Logs to a Session (using an ID kept by your internal systems) by passing the same `session_id` in subsequent log requests.

        parent_id : typing.Optional[str]
            Unique identifier for the parent Log in a Session. Should only be provided if `session_id` is provided. If provided, the Log will be nested under the parent Log within the Session.

        inputs : typing.Optional[typing.Dict[str, typing.Any]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Any]]
            Any additional metadata to record.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        batches : typing.Optional[typing.Sequence[str]]
            Array of Batch Ids that this log is part of. Batches are used to group Logs together for offline Evaluations

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        provider_api_keys : typing.Optional[ProviderApiKeys]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        stream : typing.Optional[bool]
            If true, tokens will be sent as data-only server-sent events. If num_samples > 1, samples are streamed back independently.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CallPromptsCallPostResponse
            Successful Response

        Examples
        --------
        from humanloop import ChatMessage, PromptKernelRequest
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.call(
            path="persona",
            prompt=PromptKernelRequest(
                model="gpt-4",
                template=[
                    ChatMessage(
                        role="system",
                        content="You are {{person}}. Answer any questions as this person. Do not break character.",
                    )
                ],
            ),
            messages=[
                ChatMessage(
                    role="user",
                    content="What really happened at Roswell?",
                )
            ],
            inputs={"person": "Trump"},
            stream=False,
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts/call",
            method="POST",
            params={"version_id": version_id, "environment": environment},
            json={
                "path": path,
                "id": id,
                "prompt": prompt,
                "messages": messages,
                "tool_choice": tool_choice,
                "session_id": session_id,
                "parent_id": parent_id,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "save": save,
                "source_datapoint_id": source_datapoint_id,
                "batches": batches,
                "user": user,
                "environment": prompt_call_request_environment,
                "provider_api_keys": provider_api_keys,
                "num_samples": num_samples,
                "stream": stream,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(CallPromptsCallPostResponse, construct_type(type_=CallPromptsCallPostResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[ProjectSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncPager[PromptResponse]:
        """
        Get a list of all Prompts.

        Parameters
        ----------
        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Prompts to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Prompt name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Prompt. This filter matches against both email address and name of users.

        sort_by : typing.Optional[ProjectSortBy]
            Field to sort Prompts by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncPager[PromptResponse]
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        response = await client.prompts.list(
            size=1,
        )
        async for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        async for page in response.iter_pages():
            yield page
        """
        page = page or 1
        _response = await self._client_wrapper.httpx_client.request(
            "prompts",
            method="GET",
            params={
                "page": page,
                "size": size,
                "name": name,
                "user_filter": user_filter,
                "sort_by": sort_by,
                "order": order,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(PaginatedDataPromptResponse, construct_type(type_=PaginatedDataPromptResponse, object_=_response.json()))  # type: ignore
                _has_next = True
                _get_next = lambda: self.list(
                    page=page + 1,
                    size=size,
                    name=name,
                    user_filter=user_filter,
                    sort_by=sort_by,
                    order=order,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return AsyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[PromptRequestTemplate] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[PromptRequestStop] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormat] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolFunction]] = OMIT,
        linked_tools: typing.Optional[typing.Sequence[str]] = OMIT,
        commit_message: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Create a Prompt or update it with a new version if it already exists.

        Prompts are identified by the `ID` or their `path`. The parameters (i.e. the prompt template, temperature, model etc.) determine the versions of the Prompt.

        If you provide a commit message, then the new version will be committed;
        otherwise it will be uncommitted. If you try to commit an already committed version,
        an exception will be raised.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/supported-models)

        path : typing.Optional[str]
            Path of the Prompt, including the name, which is used as a unique identifier.

        id : typing.Optional[str]
            ID for an existing Prompt to update.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[PromptRequestTemplate]
            For chat endpoint, provide a Chat template. For completion endpoint, provide a Prompt template. Input variables within the template should be specified with double curly bracket syntax: {{INPUT_NAME}}.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[PromptRequestStop]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Any]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormat]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        tools : typing.Optional[typing.Sequence[ToolFunction]]
            The tool specification that the model can choose to call if Tool calling is supported.

        linked_tools : typing.Optional[typing.Sequence[str]]
            The IDs of the Tools in your organization that the model can choose to call if Tool calling is supported. The default deployed version of that tool is called.

        commit_message : typing.Optional[str]
            Message describing the changes made.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import ChatMessage
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.upsert(
            path="Personal Projects/Coding Assistant",
            model="gpt-4o",
            endpoint="chat",
            template=[
                ChatMessage(
                    content="You are a helpful coding assistant specialising in {{language}}",
                    role="system",
                )
            ],
            provider="openai",
            max_tokens=-1,
            temperature=0.7,
            top_p=1.0,
            presence_penalty=0.0,
            frequency_penalty=0.0,
            other={},
            tools=[],
            linked_tools=[],
            commit_message="Initial commit",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts",
            method="POST",
            json={
                "path": path,
                "id": id,
                "model": model,
                "endpoint": endpoint,
                "template": template,
                "provider": provider,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": stop,
                "presence_penalty": presence_penalty,
                "frequency_penalty": frequency_penalty,
                "other": other,
                "seed": seed,
                "response_format": response_format,
                "tools": tools,
                "linked_tools": linked_tools,
                "commit_message": commit_message,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Retrieve the Prompt with the given ID.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.get(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="GET",
            params={"version_id": version_id, "environment": environment},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Prompt with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.delete(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}", method="DELETE", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Move the Prompt to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        path : typing.Optional[str]
            Path of the Prompt including the Prompt name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.move(
            id="pr_30gco7dx6JDq4200GVOHa",
            path="new directory/new name",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="PATCH",
            json={"path": path, "name": name},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_versions(
        self,
        id: str,
        *,
        status: typing.Optional[VersionStatus] = None,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListPrompts:
        """
        Get a list of all the versions of a Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        status : typing.Optional[VersionStatus]
            Filter versions by status: 'uncommitted', 'committed'. If no status is provided, all versions are returned.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListPrompts
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.list_versions(
            id="pr_30gco7dx6JDq4200GVOHa",
            status="committed",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions",
            method="GET",
            params={"status": status, "evaluator_aggregates": evaluator_aggregates},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(ListPrompts, construct_type(type_=ListPrompts, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def commit(
        self, id: str, version_id: str, *, commit_message: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Commit a version of the Prompt with a commit message.

        If the version is already committed, an exception will be raised.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        commit_message : str
            Message describing the changes made.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.commit(
            id="pr_30gco7dx6JDq4200GVOHa",
            version_id="prv_F34aba5f3asp0",
            commit_message="Reiterated point about not discussing sentience",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}/commit",
            method="POST",
            json={"commit_message": commit_message},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItem]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItem]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Activate and deactivate Evaluators for monitoring the Prompt.

        An activated Evaluator will automatically be run on all new Logs
        within the Prompt for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItem]]
            Evaluators to activate on Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItem]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import MonitoringEvaluatorVersionRequest
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.update_monitoring(
            id="pr_30gco7dx6JDq4200GVOHa",
            activate=[
                MonitoringEvaluatorVersionRequest(
                    evaluator_version_id="evv_1abc4308abd",
                )
            ],
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/evaluators",
            method="POST",
            json={"activate": activate, "deactivate": deactivate},
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Deploy Prompt to an Environment.

        Set the deployed version for the specified Environment. This Prompt
        will be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.set_deployment(
            id="id",
            environment_id="environment_id",
            version_id="version_id",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="POST",
            params={"version_id": version_id},
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(PromptResponse, construct_type(type_=PromptResponse, object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Prompt from the Environment.

        Remove the deployed version for the specified Environment. This Prompt
        will no longer be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.remove_deployment(
            id="id",
            environment_id="environment_id",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        from humanloop.client import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )
        await client.prompts.list_environments(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments", method="GET", request_options=request_options
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(typing.List[FileEnvironmentResponse], construct_type(type_=typing.List[FileEnvironmentResponse], object_=_response.json()))  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(HttpValidationError, construct_type(type_=HttpValidationError, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
