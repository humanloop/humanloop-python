# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from ..requests.chat_message import ChatMessageParams
from .requests.prompt_log_request_tool_choice import PromptLogRequestToolChoiceParams
from ..requests.prompt_kernel_request import PromptKernelRequestParams
import datetime as dt
from ..core.request_options import RequestOptions
from ..types.create_prompt_log_response import CreatePromptLogResponse
from ..core.serialization import convert_and_respect_annotation_metadata
from ..core.unchecked_base_model import construct_type
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from .requests.prompt_log_update_request_tool_choice import PromptLogUpdateRequestToolChoiceParams
from ..types.log_response import LogResponse
from ..core.jsonable_encoder import jsonable_encoder
from .requests.prompts_call_stream_request_tool_choice import PromptsCallStreamRequestToolChoiceParams
from ..requests.provider_api_keys import ProviderApiKeysParams
from ..types.prompt_call_stream_response import PromptCallStreamResponse
import httpx_sse
import json
from .requests.prompts_call_request_tool_choice import PromptsCallRequestToolChoiceParams
from ..types.prompt_call_response import PromptCallResponse
from ..types.project_sort_by import ProjectSortBy
from ..types.sort_order import SortOrder
from ..core.pagination import SyncPager
from ..types.prompt_response import PromptResponse
from ..types.paginated_data_prompt_response import PaginatedDataPromptResponse
from ..types.model_endpoints import ModelEndpoints
from .requests.prompt_request_template import PromptRequestTemplateParams
from ..types.model_providers import ModelProviders
from .requests.prompt_request_stop import PromptRequestStopParams
from ..requests.response_format import ResponseFormatParams
from ..requests.tool_function import ToolFunctionParams
from ..types.version_status import VersionStatus
from ..types.list_prompts import ListPrompts
from ..types.file_environment_response import FileEnvironmentResponse
from ..requests.evaluator_activation_deactivation_request_activate_item import (
    EvaluatorActivationDeactivationRequestActivateItemParams,
)
from ..requests.evaluator_activation_deactivation_request_deactivate_item import (
    EvaluatorActivationDeactivationRequestDeactivateItemParams,
)
from ..core.client_wrapper import AsyncClientWrapper
from ..core.pagination import AsyncPager

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class PromptsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        evaluation_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        batch_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreatePromptLogResponse:
        """
        Log to a Prompt.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise, the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        evaluation_id : typing.Optional[str]
            Unique identifier for the Evaluation Report to associate the Log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        batch_id : typing.Optional[str]
            Unique identifier for the Batch to add this Batch to. Batches are used to group Logs together for Evaluations. A Batch will be created if one with the given ID does not exist.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreatePromptLogResponse
            Successful Response

        Examples
        --------
        import datetime

        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.log(
            path="persona",
            prompt={
                "model": "gpt-4",
                "template": [
                    {
                        "role": "system",
                        "content": "You are {{person}}. Answer questions as this person. Do not break character.",
                    }
                ],
            },
            messages=[{"role": "user", "content": "What really happened at Roswell?"}],
            inputs={"person": "Trump"},
            created_at=datetime.datetime.fromisoformat(
                "2024-07-19 00:29:35.178000+00:00",
            ),
            provider_latency=6.5931549072265625,
            output_message={
                "content": "Well, you know, there is so much secrecy involved in government, folks, it's unbelievable. They don't want to tell you everything. They don't tell me everything! But about Roswell, it’s a very popular question. I know, I just know, that something very, very peculiar happened there. Was it a weather balloon? Maybe. Was it something extraterrestrial? Could be. I'd love to go down and open up all the classified documents, believe me, I would. But they don't let that happen. The Deep State, folks, the Deep State. They’re unbelievable. They want to keep everything a secret. But whatever the truth is, I can tell you this: it’s something big, very very big. Tremendous, in fact.",
                "role": "assistant",
            },
            prompt_tokens=100,
            output_tokens=220,
            prompt_cost=1e-05,
            output_cost=0.0002,
            finish_reason="stop",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts/log",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "evaluation_id": evaluation_id,
                "path": path,
                "id": id,
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptLogRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "start_time": start_time,
                "end_time": end_time,
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "batch_id": batch_id,
                "user": user,
                "environment": prompt_log_request_environment,
                "save": save,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    CreatePromptLogResponse,
                    construct_type(
                        type_=CreatePromptLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_log(
        self,
        id: str,
        log_id: str,
        *,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogUpdateRequestToolChoiceParams] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> LogResponse:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        log_id : str
            Unique identifier for the Log.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogUpdateRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        LogResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.update_log(
            id="id",
            log_id="log_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/log/{jsonable_encoder(log_id)}",
            method="PATCH",
            json={
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptLogUpdateRequestToolChoiceParams, direction="write"
                ),
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    LogResponse,
                    construct_type(
                        type_=LogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallStreamRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        batch_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[PromptCallStreamResponse]:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        batch_id : typing.Optional[str]
            Unique identifier for the Batch to add this Batch to. Batches are used to group Logs together for Evaluations. A Batch will be created if one with the given ID does not exist.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[PromptCallStreamResponse]


        Examples
        --------
        import datetime

        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.prompts.call_stream(
            version_id="string",
            environment="string",
            path="string",
            id="string",
            messages=[
                {
                    "content": "string",
                    "name": "string",
                    "tool_call_id": "string",
                    "role": "user",
                    "tool_calls": [
                        {
                            "id": "string",
                            "type": "function",
                            "function": {"name": "string"},
                        }
                    ],
                }
            ],
            prompt={"model": "string"},
            inputs={"string": {"key": "value"}},
            source="string",
            metadata={"string": {"key": "value"}},
            start_time=datetime.datetime.fromisoformat(
                "2024-01-15 09:30:00+00:00",
            ),
            end_time=datetime.datetime.fromisoformat(
                "2024-01-15 09:30:00+00:00",
            ),
            source_datapoint_id="string",
            trace_parent_id="string",
            batch_id="string",
            user="string",
            prompts_call_stream_request_environment="string",
            save=True,
            provider_api_keys={
                "openai": "string",
                "ai_21": "string",
                "mock": "string",
                "anthropic": "string",
                "bedrock": "string",
                "cohere": "string",
                "openai_azure": "string",
                "openai_azure_endpoint": "string",
            },
            num_samples=1,
            return_inputs=True,
            logprobs=1,
            suffix="string",
        )
        for chunk in response:
            yield chunk
        """
        with self._client_wrapper.httpx_client.stream(
            "prompts/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptsCallStreamRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "batch_id": batch_id,
                "user": user,
                "environment": prompts_call_stream_request_environment,
                "save": save,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "num_samples": num_samples,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    for _sse in _event_source.iter_sse():
                        try:
                            yield typing.cast(
                                PromptCallStreamResponse,
                                construct_type(
                                    type_=PromptCallStreamResponse,  # type: ignore
                                    object_=json.loads(_sse.data),
                                ),
                            )
                        except:
                            pass
                    return
                _response.read()
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        typing.cast(
                            HttpValidationError,
                            construct_type(
                                type_=HttpValidationError,  # type: ignore
                                object_=_response.json(),
                            ),
                        )
                    )
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        batch_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptCallResponse:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        batch_id : typing.Optional[str]
            Unique identifier for the Batch to add this Batch to. Batches are used to group Logs together for Evaluations. A Batch will be created if one with the given ID does not exist.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptCallResponse


        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.call(
            path="persona",
            prompt={
                "model": "gpt-4",
                "template": [
                    {
                        "role": "system",
                        "content": "You are stockbot. Return latest prices.",
                    }
                ],
                "tools": [
                    {
                        "name": "get_stock_price",
                        "description": "Get current stock price",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "ticker_symbol": {
                                    "type": "string",
                                    "name": "Ticker Symbol",
                                    "description": "Ticker symbol of the stock",
                                }
                            },
                            "required": [],
                        },
                    }
                ],
            },
            messages=[{"role": "user", "content": "latest apple"}],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptsCallRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "batch_id": batch_id,
                "user": user,
                "environment": prompts_call_request_environment,
                "save": save,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "num_samples": num_samples,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptCallResponse,
                    construct_type(
                        type_=PromptCallResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[ProjectSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> SyncPager[PromptResponse]:
        """
        Get a list of all Prompts.

        Parameters
        ----------
        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Prompts to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Prompt name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Prompt. This filter matches against both email address and name of users.

        sort_by : typing.Optional[ProjectSortBy]
            Field to sort Prompts by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        SyncPager[PromptResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        response = client.prompts.list(
            size=1,
        )
        for item in response:
            yield item
        # alternatively, you can paginate page-by-page
        for page in response.iter_pages():
            yield page
        """
        page = page if page is not None else 1
        _response = self._client_wrapper.httpx_client.request(
            "prompts",
            method="GET",
            params={
                "page": page,
                "size": size,
                "name": name,
                "user_filter": user_filter,
                "sort_by": sort_by,
                "order": order,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(
                    PaginatedDataPromptResponse,
                    construct_type(
                        type_=PaginatedDataPromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                _has_next = True
                _get_next = lambda: self.list(
                    page=page + 1,
                    size=size,
                    name=name,
                    user_filter=user_filter,
                    sort_by=sort_by,
                    order=order,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return SyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[PromptRequestTemplateParams] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[PromptRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolFunctionParams]] = OMIT,
        linked_tools: typing.Optional[typing.Sequence[str]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        commit_message: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Create a Prompt or update it with a new version if it already exists.

        Prompts are identified by the `ID` or their `path`. The parameters (i.e. the prompt template, temperature, model etc.) determine the versions of the Prompt.

        If you provide a commit message, then the new version will be committed;
        otherwise it will be uncommitted. If you try to commit an already committed version,
        an exception will be raised.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[PromptRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[PromptRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        tools : typing.Optional[typing.Sequence[ToolFunctionParams]]
            The tool specification that the model can choose to call if Tool calling is supported.

        linked_tools : typing.Optional[typing.Sequence[str]]
            The IDs of the Tools in your organization that the model can choose to call if Tool calling is supported. The default deployed version of that tool is called.

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        commit_message : typing.Optional[str]
            Message describing the changes made.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.upsert(
            path="Personal Projects/Coding Assistant",
            model="gpt-4o",
            endpoint="chat",
            template=[
                {
                    "content": "You are a helpful coding assistant specialising in {{language}}",
                    "role": "system",
                }
            ],
            provider="openai",
            max_tokens=-1,
            temperature=0.7,
            top_p=1.0,
            presence_penalty=0.0,
            frequency_penalty=0.0,
            other={},
            tools=[],
            linked_tools=[],
            commit_message="Initial commit",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "prompts",
            method="POST",
            json={
                "path": path,
                "id": id,
                "model": model,
                "endpoint": endpoint,
                "template": convert_and_respect_annotation_metadata(
                    object_=template, annotation=PromptRequestTemplateParams, direction="write"
                ),
                "provider": provider,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": convert_and_respect_annotation_metadata(
                    object_=stop, annotation=PromptRequestStopParams, direction="write"
                ),
                "presence_penalty": presence_penalty,
                "frequency_penalty": frequency_penalty,
                "other": other,
                "seed": seed,
                "response_format": convert_and_respect_annotation_metadata(
                    object_=response_format, annotation=ResponseFormatParams, direction="write"
                ),
                "tools": convert_and_respect_annotation_metadata(
                    object_=tools, annotation=typing.Sequence[ToolFunctionParams], direction="write"
                ),
                "linked_tools": linked_tools,
                "attributes": attributes,
                "commit_message": commit_message,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Retrieve the Prompt with the given ID.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.get(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Prompt with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.delete(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Move the Prompt to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        path : typing.Optional[str]
            Path of the Prompt including the Prompt name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.move(
            id="pr_30gco7dx6JDq4200GVOHa",
            path="new directory/new name",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "path": path,
                "name": name,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_versions(
        self,
        id: str,
        *,
        status: typing.Optional[VersionStatus] = None,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListPrompts:
        """
        Get a list of all the versions of a Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        status : typing.Optional[VersionStatus]
            Filter versions by status: 'uncommitted', 'committed'. If no status is provided, all versions are returned.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListPrompts
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.list_versions(
            id="pr_30gco7dx6JDq4200GVOHa",
            status="committed",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions",
            method="GET",
            params={
                "status": status,
                "evaluator_aggregates": evaluator_aggregates,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ListPrompts,
                    construct_type(
                        type_=ListPrompts,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def commit(
        self, id: str, version_id: str, *, commit_message: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Commit a version of the Prompt with a commit message.

        If the version is already committed, an exception will be raised.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        commit_message : str
            Message describing the changes made.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.commit(
            id="pr_30gco7dx6JDq4200GVOHa",
            version_id="prv_F34aba5f3asp0",
            commit_message="Reiterated point about not discussing sentience",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}/commit",
            method="POST",
            json={
                "commit_message": commit_message,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Deploy Prompt to an Environment.

        Set the deployed version for the specified Environment. This Prompt
        will be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.set_deployment(
            id="id",
            environment_id="environment_id",
            version_id="version_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="POST",
            params={
                "version_id": version_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Prompt from the Environment.

        Remove the deployed version for the specified Environment. This Prompt
        will no longer be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.remove_deployment(
            id="id",
            environment_id="environment_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.list_environments(
            id="pr_30gco7dx6JDq4200GVOHa",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[FileEnvironmentResponse],
                    construct_type(
                        type_=typing.List[FileEnvironmentResponse],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Activate and deactivate Evaluators for monitoring the Prompt.

        An activated Evaluator will automatically be run on all new Logs
        within the Prompt for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        from humanloop import Humanloop

        client = Humanloop(
            api_key="YOUR_API_KEY",
        )
        client.prompts.update_monitoring(
            id="pr_30gco7dx6JDq4200GVOHa",
            activate=[{"evaluator_version_id": "evv_1abc4308abd"}],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/evaluators",
            method="POST",
            json={
                "activate": convert_and_respect_annotation_metadata(
                    object_=activate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams],
                    direction="write",
                ),
                "deactivate": convert_and_respect_annotation_metadata(
                    object_=deactivate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams],
                    direction="write",
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncPromptsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def log(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        evaluation_id: typing.Optional[str] = OMIT,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        batch_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompt_log_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CreatePromptLogResponse:
        """
        Log to a Prompt.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise, the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        evaluation_id : typing.Optional[str]
            Unique identifier for the Evaluation Report to associate the Log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        batch_id : typing.Optional[str]
            Unique identifier for the Batch to add this Batch to. Batches are used to group Logs together for Evaluations. A Batch will be created if one with the given ID does not exist.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompt_log_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CreatePromptLogResponse
            Successful Response

        Examples
        --------
        import asyncio
        import datetime

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.log(
                path="persona",
                prompt={
                    "model": "gpt-4",
                    "template": [
                        {
                            "role": "system",
                            "content": "You are {{person}}. Answer questions as this person. Do not break character.",
                        }
                    ],
                },
                messages=[
                    {"role": "user", "content": "What really happened at Roswell?"}
                ],
                inputs={"person": "Trump"},
                created_at=datetime.datetime.fromisoformat(
                    "2024-07-19 00:29:35.178000+00:00",
                ),
                provider_latency=6.5931549072265625,
                output_message={
                    "content": "Well, you know, there is so much secrecy involved in government, folks, it's unbelievable. They don't want to tell you everything. They don't tell me everything! But about Roswell, it’s a very popular question. I know, I just know, that something very, very peculiar happened there. Was it a weather balloon? Maybe. Was it something extraterrestrial? Could be. I'd love to go down and open up all the classified documents, believe me, I would. But they don't let that happen. The Deep State, folks, the Deep State. They’re unbelievable. They want to keep everything a secret. But whatever the truth is, I can tell you this: it’s something big, very very big. Tremendous, in fact.",
                    "role": "assistant",
                },
                prompt_tokens=100,
                output_tokens=220,
                prompt_cost=1e-05,
                output_cost=0.0002,
                finish_reason="stop",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts/log",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "evaluation_id": evaluation_id,
                "path": path,
                "id": id,
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptLogRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "start_time": start_time,
                "end_time": end_time,
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "batch_id": batch_id,
                "user": user,
                "environment": prompt_log_request_environment,
                "save": save,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    CreatePromptLogResponse,
                    construct_type(
                        type_=CreatePromptLogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_log(
        self,
        id: str,
        log_id: str,
        *,
        output_message: typing.Optional[ChatMessageParams] = OMIT,
        prompt_tokens: typing.Optional[int] = OMIT,
        output_tokens: typing.Optional[int] = OMIT,
        prompt_cost: typing.Optional[float] = OMIT,
        output_cost: typing.Optional[float] = OMIT,
        finish_reason: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptLogUpdateRequestToolChoiceParams] = OMIT,
        output: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        error: typing.Optional[str] = OMIT,
        provider_latency: typing.Optional[float] = OMIT,
        stdout: typing.Optional[str] = OMIT,
        provider_request: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        provider_response: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> LogResponse:
        """
        Update a Log.

        Update the details of a Log with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        log_id : str
            Unique identifier for the Log.

        output_message : typing.Optional[ChatMessageParams]
            The message returned by the provider.

        prompt_tokens : typing.Optional[int]
            Number of tokens in the prompt used to generate the output.

        output_tokens : typing.Optional[int]
            Number of tokens in the output generated by the model.

        prompt_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the prompt.

        output_cost : typing.Optional[float]
            Cost in dollars associated to the tokens in the output.

        finish_reason : typing.Optional[str]
            Reason the generation finished.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptLogUpdateRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        output : typing.Optional[str]
            Generated output from your model for the provided inputs. Can be `None` if logging an error, or if creating a parent Log with the intention to populate it later.

        created_at : typing.Optional[dt.datetime]
            User defined timestamp for when the log was created.

        error : typing.Optional[str]
            Error message if the log is an error.

        provider_latency : typing.Optional[float]
            Duration of the logged event in seconds.

        stdout : typing.Optional[str]
            Captured log and debug statements.

        provider_request : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw request sent to provider.

        provider_response : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Raw response received the provider.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        LogResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.update_log(
                id="id",
                log_id="log_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/log/{jsonable_encoder(log_id)}",
            method="PATCH",
            json={
                "output_message": convert_and_respect_annotation_metadata(
                    object_=output_message, annotation=ChatMessageParams, direction="write"
                ),
                "prompt_tokens": prompt_tokens,
                "output_tokens": output_tokens,
                "prompt_cost": prompt_cost,
                "output_cost": output_cost,
                "finish_reason": finish_reason,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptLogUpdateRequestToolChoiceParams, direction="write"
                ),
                "output": output,
                "created_at": created_at,
                "error": error,
                "provider_latency": provider_latency,
                "stdout": stdout,
                "provider_request": provider_request,
                "provider_response": provider_response,
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    LogResponse,
                    construct_type(
                        type_=LogResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def call_stream(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallStreamRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        batch_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_stream_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[PromptCallStreamResponse]:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallStreamRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        batch_id : typing.Optional[str]
            Unique identifier for the Batch to add this Batch to. Batches are used to group Logs together for Evaluations. A Batch will be created if one with the given ID does not exist.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_stream_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[PromptCallStreamResponse]


        Examples
        --------
        import asyncio
        import datetime

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.prompts.call_stream(
                version_id="string",
                environment="string",
                path="string",
                id="string",
                messages=[
                    {
                        "content": "string",
                        "name": "string",
                        "tool_call_id": "string",
                        "role": "user",
                        "tool_calls": [
                            {
                                "id": "string",
                                "type": "function",
                                "function": {"name": "string"},
                            }
                        ],
                    }
                ],
                prompt={"model": "string"},
                inputs={"string": {"key": "value"}},
                source="string",
                metadata={"string": {"key": "value"}},
                start_time=datetime.datetime.fromisoformat(
                    "2024-01-15 09:30:00+00:00",
                ),
                end_time=datetime.datetime.fromisoformat(
                    "2024-01-15 09:30:00+00:00",
                ),
                source_datapoint_id="string",
                trace_parent_id="string",
                batch_id="string",
                user="string",
                prompts_call_stream_request_environment="string",
                save=True,
                provider_api_keys={
                    "openai": "string",
                    "ai_21": "string",
                    "mock": "string",
                    "anthropic": "string",
                    "bedrock": "string",
                    "cohere": "string",
                    "openai_azure": "string",
                    "openai_azure_endpoint": "string",
                },
                num_samples=1,
                return_inputs=True,
                logprobs=1,
                suffix="string",
            )
            async for chunk in response:
                yield chunk


        asyncio.run(main())
        """
        async with self._client_wrapper.httpx_client.stream(
            "prompts/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptsCallStreamRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "batch_id": batch_id,
                "user": user,
                "environment": prompts_call_stream_request_environment,
                "save": save,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "num_samples": num_samples,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    async for _sse in _event_source.aiter_sse():
                        try:
                            yield typing.cast(
                                PromptCallStreamResponse,
                                construct_type(
                                    type_=PromptCallStreamResponse,  # type: ignore
                                    object_=json.loads(_sse.data),
                                ),
                            )
                        except:
                            pass
                    return
                await _response.aread()
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        typing.cast(
                            HttpValidationError,
                            construct_type(
                                type_=HttpValidationError,  # type: ignore
                                object_=_response.json(),
                            ),
                        )
                    )
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    async def call(
        self,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        messages: typing.Optional[typing.Sequence[ChatMessageParams]] = OMIT,
        tool_choice: typing.Optional[PromptsCallRequestToolChoiceParams] = OMIT,
        prompt: typing.Optional[PromptKernelRequestParams] = OMIT,
        inputs: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        source: typing.Optional[str] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        start_time: typing.Optional[dt.datetime] = OMIT,
        end_time: typing.Optional[dt.datetime] = OMIT,
        source_datapoint_id: typing.Optional[str] = OMIT,
        trace_parent_id: typing.Optional[str] = OMIT,
        batch_id: typing.Optional[str] = OMIT,
        user: typing.Optional[str] = OMIT,
        prompts_call_request_environment: typing.Optional[str] = OMIT,
        save: typing.Optional[bool] = OMIT,
        provider_api_keys: typing.Optional[ProviderApiKeysParams] = OMIT,
        num_samples: typing.Optional[int] = OMIT,
        return_inputs: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptCallResponse:
        """
        Call a Prompt.

        Calling a Prompt calls the model provider before logging
        the request, responses and metadata to Humanloop.

        You can use query parameters `version_id`, or `environment`, to target
        an existing version of the Prompt. Otherwise the default deployed version will be chosen.

        Instead of targeting an existing version explicitly, you can instead pass in
        Prompt details in the request body. In this case, we will check if the details correspond
        to an existing version of the Prompt. If they do not, we will create a new version. This is helpful
        in the case where you are storing or deriving your Prompt details in code.

        Parameters
        ----------
        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to log to.

        environment : typing.Optional[str]
            Name of the Environment identifying a deployed version to log to.

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        messages : typing.Optional[typing.Sequence[ChatMessageParams]]
            The messages passed to the to provider chat endpoint.

        tool_choice : typing.Optional[PromptsCallRequestToolChoiceParams]
            Controls how the model uses tools. The following options are supported:
            - `'none'` means the model will not call any tool and instead generates a message; this is the default when no tools are provided as part of the Prompt.
            - `'auto'` means the model can decide to call one or more of the provided tools; this is the default when tools are provided as part of the Prompt.
            - `'required'` means the model can decide to call one or more of the provided tools.
            - `{'type': 'function', 'function': {name': <TOOL_NAME>}}` forces the model to use the named function.

        prompt : typing.Optional[PromptKernelRequestParams]
            Details of your Prompt. A new Prompt version will be created if the provided details are new.

        inputs : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            The inputs passed to the prompt template.

        source : typing.Optional[str]
            Identifies where the model was called from.

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Any additional metadata to record.

        start_time : typing.Optional[dt.datetime]
            When the logged event started.

        end_time : typing.Optional[dt.datetime]
            When the logged event ended.

        source_datapoint_id : typing.Optional[str]
            Unique identifier for the Datapoint that this Log is derived from. This can be used by Humanloop to associate Logs to Evaluations. If provided, Humanloop will automatically associate this Log to Evaluations that require a Log for this Datapoint-Version pair.

        trace_parent_id : typing.Optional[str]
            The ID of the parent Log to nest this Log under in a Trace.

        batch_id : typing.Optional[str]
            Unique identifier for the Batch to add this Batch to. Batches are used to group Logs together for Evaluations. A Batch will be created if one with the given ID does not exist.

        user : typing.Optional[str]
            End-user ID related to the Log.

        prompts_call_request_environment : typing.Optional[str]
            The name of the Environment the Log is associated to.

        save : typing.Optional[bool]
            Whether the request/response payloads will be stored on Humanloop.

        provider_api_keys : typing.Optional[ProviderApiKeysParams]
            API keys required by each provider to make API calls. The API keys provided here are not stored by Humanloop. If not specified here, Humanloop will fall back to the key saved to your organization.

        num_samples : typing.Optional[int]
            The number of generations.

        return_inputs : typing.Optional[bool]
            Whether to return the inputs in the response. If false, the response will contain an empty dictionary under inputs. This is useful for reducing the size of the response. Defaults to true.

        logprobs : typing.Optional[int]
            Include the log probabilities of the top n tokens in the provider_response

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text. Useful for completions that act like inserts.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptCallResponse


        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.call(
                path="persona",
                prompt={
                    "model": "gpt-4",
                    "template": [
                        {
                            "role": "system",
                            "content": "You are stockbot. Return latest prices.",
                        }
                    ],
                    "tools": [
                        {
                            "name": "get_stock_price",
                            "description": "Get current stock price",
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    "ticker_symbol": {
                                        "type": "string",
                                        "name": "Ticker Symbol",
                                        "description": "Ticker symbol of the stock",
                                    }
                                },
                                "required": [],
                            },
                        }
                    ],
                },
                messages=[{"role": "user", "content": "latest apple"}],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts/call",
            method="POST",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            json={
                "path": path,
                "id": id,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessageParams], direction="write"
                ),
                "tool_choice": convert_and_respect_annotation_metadata(
                    object_=tool_choice, annotation=PromptsCallRequestToolChoiceParams, direction="write"
                ),
                "prompt": convert_and_respect_annotation_metadata(
                    object_=prompt, annotation=PromptKernelRequestParams, direction="write"
                ),
                "inputs": inputs,
                "source": source,
                "metadata": metadata,
                "start_time": start_time,
                "end_time": end_time,
                "source_datapoint_id": source_datapoint_id,
                "trace_parent_id": trace_parent_id,
                "batch_id": batch_id,
                "user": user,
                "environment": prompts_call_request_environment,
                "save": save,
                "provider_api_keys": convert_and_respect_annotation_metadata(
                    object_=provider_api_keys, annotation=ProviderApiKeysParams, direction="write"
                ),
                "num_samples": num_samples,
                "return_inputs": return_inputs,
                "logprobs": logprobs,
                "suffix": suffix,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptCallResponse,
                    construct_type(
                        type_=PromptCallResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list(
        self,
        *,
        page: typing.Optional[int] = None,
        size: typing.Optional[int] = None,
        name: typing.Optional[str] = None,
        user_filter: typing.Optional[str] = None,
        sort_by: typing.Optional[ProjectSortBy] = None,
        order: typing.Optional[SortOrder] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncPager[PromptResponse]:
        """
        Get a list of all Prompts.

        Parameters
        ----------
        page : typing.Optional[int]
            Page number for pagination.

        size : typing.Optional[int]
            Page size for pagination. Number of Prompts to fetch.

        name : typing.Optional[str]
            Case-insensitive filter for Prompt name.

        user_filter : typing.Optional[str]
            Case-insensitive filter for users in the Prompt. This filter matches against both email address and name of users.

        sort_by : typing.Optional[ProjectSortBy]
            Field to sort Prompts by

        order : typing.Optional[SortOrder]
            Direction to sort by.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncPager[PromptResponse]
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.prompts.list(
                size=1,
            )
            async for item in response:
                yield item
            # alternatively, you can paginate page-by-page
            async for page in response.iter_pages():
                yield page


        asyncio.run(main())
        """
        page = page if page is not None else 1
        _response = await self._client_wrapper.httpx_client.request(
            "prompts",
            method="GET",
            params={
                "page": page,
                "size": size,
                "name": name,
                "user_filter": user_filter,
                "sort_by": sort_by,
                "order": order,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _parsed_response = typing.cast(
                    PaginatedDataPromptResponse,
                    construct_type(
                        type_=PaginatedDataPromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                _has_next = True
                _get_next = lambda: self.list(
                    page=page + 1,
                    size=size,
                    name=name,
                    user_filter=user_filter,
                    sort_by=sort_by,
                    order=order,
                    request_options=request_options,
                )
                _items = _parsed_response.records
                return AsyncPager(has_next=_has_next, items=_items, get_next=_get_next)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert(
        self,
        *,
        model: str,
        path: typing.Optional[str] = OMIT,
        id: typing.Optional[str] = OMIT,
        endpoint: typing.Optional[ModelEndpoints] = OMIT,
        template: typing.Optional[PromptRequestTemplateParams] = OMIT,
        provider: typing.Optional[ModelProviders] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        stop: typing.Optional[PromptRequestStopParams] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        other: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        seed: typing.Optional[int] = OMIT,
        response_format: typing.Optional[ResponseFormatParams] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolFunctionParams]] = OMIT,
        linked_tools: typing.Optional[typing.Sequence[str]] = OMIT,
        attributes: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        commit_message: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Create a Prompt or update it with a new version if it already exists.

        Prompts are identified by the `ID` or their `path`. The parameters (i.e. the prompt template, temperature, model etc.) determine the versions of the Prompt.

        If you provide a commit message, then the new version will be committed;
        otherwise it will be uncommitted. If you try to commit an already committed version,
        an exception will be raised.

        Parameters
        ----------
        model : str
            The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models)

        path : typing.Optional[str]
            Path of the Prompt, including the name. This locates the Prompt in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`.

        id : typing.Optional[str]
            ID for an existing Prompt.

        endpoint : typing.Optional[ModelEndpoints]
            The provider model endpoint used.

        template : typing.Optional[PromptRequestTemplateParams]
            The template contains the main structure and instructions for the model, including input variables for dynamic values.

            For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
            For completion models, provide a prompt template as a string.

            Input variables should be specified with double curly bracket syntax: `{{input_name}}`.

        provider : typing.Optional[ModelProviders]
            The company providing the underlying model service.

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt

        temperature : typing.Optional[float]
            What sampling temperature to use when making a generation. Higher values means the model will be more creative.

        top_p : typing.Optional[float]
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.

        stop : typing.Optional[PromptRequestStopParams]
            The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence.

        presence_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far.

        frequency_penalty : typing.Optional[float]
            Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far.

        other : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Other parameter values to be passed to the provider call.

        seed : typing.Optional[int]
            If specified, model will make a best effort to sample deterministically, but it is not guaranteed.

        response_format : typing.Optional[ResponseFormatParams]
            The format of the response. Only `{"type": "json_object"}` is currently supported for chat.

        tools : typing.Optional[typing.Sequence[ToolFunctionParams]]
            The tool specification that the model can choose to call if Tool calling is supported.

        linked_tools : typing.Optional[typing.Sequence[str]]
            The IDs of the Tools in your organization that the model can choose to call if Tool calling is supported. The default deployed version of that tool is called.

        attributes : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used.

        commit_message : typing.Optional[str]
            Message describing the changes made.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.upsert(
                path="Personal Projects/Coding Assistant",
                model="gpt-4o",
                endpoint="chat",
                template=[
                    {
                        "content": "You are a helpful coding assistant specialising in {{language}}",
                        "role": "system",
                    }
                ],
                provider="openai",
                max_tokens=-1,
                temperature=0.7,
                top_p=1.0,
                presence_penalty=0.0,
                frequency_penalty=0.0,
                other={},
                tools=[],
                linked_tools=[],
                commit_message="Initial commit",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "prompts",
            method="POST",
            json={
                "path": path,
                "id": id,
                "model": model,
                "endpoint": endpoint,
                "template": convert_and_respect_annotation_metadata(
                    object_=template, annotation=PromptRequestTemplateParams, direction="write"
                ),
                "provider": provider,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": convert_and_respect_annotation_metadata(
                    object_=stop, annotation=PromptRequestStopParams, direction="write"
                ),
                "presence_penalty": presence_penalty,
                "frequency_penalty": frequency_penalty,
                "other": other,
                "seed": seed,
                "response_format": convert_and_respect_annotation_metadata(
                    object_=response_format, annotation=ResponseFormatParams, direction="write"
                ),
                "tools": convert_and_respect_annotation_metadata(
                    object_=tools, annotation=typing.Sequence[ToolFunctionParams], direction="write"
                ),
                "linked_tools": linked_tools,
                "attributes": attributes,
                "commit_message": commit_message,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get(
        self,
        id: str,
        *,
        version_id: typing.Optional[str] = None,
        environment: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Retrieve the Prompt with the given ID.

        By default, the deployed version of the Prompt is returned. Use the query parameters
        `version_id` or `environment` to target a specific version of the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : typing.Optional[str]
            A specific Version ID of the Prompt to retrieve.

        environment : typing.Optional[str]
            Name of the Environment to retrieve a deployed Version from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.get(
                id="pr_30gco7dx6JDq4200GVOHa",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="GET",
            params={
                "version_id": version_id,
                "environment": environment,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete the Prompt with the given ID.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.delete(
                id="pr_30gco7dx6JDq4200GVOHa",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def move(
        self,
        id: str,
        *,
        path: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Move the Prompt to a different path or change the name.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        path : typing.Optional[str]
            Path of the Prompt including the Prompt name, which is used as a unique identifier.

        name : typing.Optional[str]
            Name of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.move(
                id="pr_30gco7dx6JDq4200GVOHa",
                path="new directory/new name",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "path": path,
                "name": name,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_versions(
        self,
        id: str,
        *,
        status: typing.Optional[VersionStatus] = None,
        evaluator_aggregates: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ListPrompts:
        """
        Get a list of all the versions of a Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        status : typing.Optional[VersionStatus]
            Filter versions by status: 'uncommitted', 'committed'. If no status is provided, all versions are returned.

        evaluator_aggregates : typing.Optional[bool]
            Whether to include Evaluator aggregate results for the versions in the response

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ListPrompts
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.list_versions(
                id="pr_30gco7dx6JDq4200GVOHa",
                status="committed",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions",
            method="GET",
            params={
                "status": status,
                "evaluator_aggregates": evaluator_aggregates,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ListPrompts,
                    construct_type(
                        type_=ListPrompts,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def commit(
        self, id: str, version_id: str, *, commit_message: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Commit a version of the Prompt with a commit message.

        If the version is already committed, an exception will be raised.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        commit_message : str
            Message describing the changes made.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.commit(
                id="pr_30gco7dx6JDq4200GVOHa",
                version_id="prv_F34aba5f3asp0",
                commit_message="Reiterated point about not discussing sentience",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/versions/{jsonable_encoder(version_id)}/commit",
            method="POST",
            json={
                "commit_message": commit_message,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def set_deployment(
        self, id: str, environment_id: str, *, version_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> PromptResponse:
        """
        Deploy Prompt to an Environment.

        Set the deployed version for the specified Environment. This Prompt
        will be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to deploy the Version to.

        version_id : str
            Unique identifier for the specific version of the Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.set_deployment(
                id="id",
                environment_id="environment_id",
                version_id="version_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="POST",
            params={
                "version_id": version_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def remove_deployment(
        self, id: str, environment_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Remove deployed Prompt from the Environment.

        Remove the deployed version for the specified Environment. This Prompt
        will no longer be used for calls made to the Prompt in this Environment.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        environment_id : str
            Unique identifier for the Environment to remove the deployment from.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.remove_deployment(
                id="id",
                environment_id="environment_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments/{jsonable_encoder(environment_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_environments(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[FileEnvironmentResponse]:
        """
        List all Environments and their deployed versions for the Prompt.

        Parameters
        ----------
        id : str
            Unique identifier for Prompt.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[FileEnvironmentResponse]
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.list_environments(
                id="pr_30gco7dx6JDq4200GVOHa",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/environments",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[FileEnvironmentResponse],
                    construct_type(
                        type_=typing.List[FileEnvironmentResponse],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_monitoring(
        self,
        id: str,
        *,
        activate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]] = OMIT,
        deactivate: typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptResponse:
        """
        Activate and deactivate Evaluators for monitoring the Prompt.

        An activated Evaluator will automatically be run on all new Logs
        within the Prompt for monitoring purposes.

        Parameters
        ----------
        id : str

        activate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams]]
            Evaluators to activate for Monitoring. These will be automatically run on new Logs.

        deactivate : typing.Optional[typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams]]
            Evaluators to deactivate. These will not be run on new Logs.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptResponse
            Successful Response

        Examples
        --------
        import asyncio

        from humanloop import AsyncHumanloop

        client = AsyncHumanloop(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.update_monitoring(
                id="pr_30gco7dx6JDq4200GVOHa",
                activate=[{"evaluator_version_id": "evv_1abc4308abd"}],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"prompts/{jsonable_encoder(id)}/evaluators",
            method="POST",
            json={
                "activate": convert_and_respect_annotation_metadata(
                    object_=activate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestActivateItemParams],
                    direction="write",
                ),
                "deactivate": convert_and_respect_annotation_metadata(
                    object_=deactivate,
                    annotation=typing.Sequence[EvaluatorActivationDeactivationRequestDeactivateItemParams],
                    direction="write",
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptResponse,
                    construct_type(
                        type_=PromptResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        construct_type(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
