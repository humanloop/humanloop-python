# This file was auto-generated by Fern from our API Definition.

import typing

import pytest

from humanloop import AsyncHumanloop, CodeEvaluatorRequest, Humanloop

from .utilities import validate_response


async def test_upsert(client: Humanloop, async_client: AsyncHumanloop) -> None:
    expected_response: typing.Any = {
        "id": "ev_890bcd",
        "name": "Accuracy Evaluator",
        "path": "Shared Evaluators/Accuracy Evaluator",
        "version_id": "evv_012def",
        "type": "evaluator",
        "created_at": "2024-05-01T12:00:00Z",
        "updated_at": "2024-05-01T12:00:00Z",
        "status": "committed",
        "last_used_at": "2024-05-01T12:00:00Z",
        "spec": {
            "arguments_type": "target_required",
            "return_type": "number",
            "evaluator_type": "python",
            "code": "def evaluate(answer, target):\\n    return 0.5",
        },
        "version_logs_count": 1,
        "total_logs_count": 1,
        "inputs": [{"name": "answer"}],
    }
    expected_types: typing.Any = {
        "id": None,
        "name": None,
        "path": None,
        "version_id": None,
        "type": None,
        "created_at": "datetime",
        "updated_at": "datetime",
        "status": None,
        "last_used_at": "datetime",
        "spec": {"arguments_type": None, "return_type": None, "evaluator_type": None, "code": None},
        "version_logs_count": "integer",
        "total_logs_count": "integer",
        "inputs": ("list", {0: {"name": None}}),
    }
    response = client.evaluators.upsert(
        path="Shared Evaluators/Accuracy Evaluator",
        spec=CodeEvaluatorRequest(
            arguments_type="target_required",
            return_type="number",
            code="def evaluate(answer, target):\\n    return 0.5",
        ),
        commit_message="Initial commit",
    )
    validate_response(response, expected_response, expected_types)

    async_response = await async_client.evaluators.upsert(
        path="Shared Evaluators/Accuracy Evaluator",
        spec=CodeEvaluatorRequest(
            arguments_type="target_required",
            return_type="number",
            code="def evaluate(answer, target):\\n    return 0.5",
        ),
        commit_message="Initial commit",
    )
    validate_response(async_response, expected_response, expected_types)


async def test_get(client: Humanloop, async_client: AsyncHumanloop) -> None:
    expected_response: typing.Any = {
        "id": "ev_890bcd",
        "name": "Accuracy Evaluator",
        "path": "Shared Evaluators/Accuracy Evaluator",
        "version_id": "evv_012def",
        "type": "evaluator",
        "created_at": "2024-05-01T12:00:00Z",
        "updated_at": "2024-05-01T12:00:00Z",
        "status": "committed",
        "last_used_at": "2024-05-01T12:00:00Z",
        "spec": {
            "arguments_type": "target_required",
            "return_type": "number",
            "evaluator_type": "python",
            "code": "def evaluate(answer, target):\\n    return 0.5",
        },
        "version_logs_count": 1,
        "total_logs_count": 1,
        "inputs": [{"name": "answer"}],
    }
    expected_types: typing.Any = {
        "id": None,
        "name": None,
        "path": None,
        "version_id": None,
        "type": None,
        "created_at": "datetime",
        "updated_at": "datetime",
        "status": None,
        "last_used_at": "datetime",
        "spec": {"arguments_type": None, "return_type": None, "evaluator_type": None, "code": None},
        "version_logs_count": "integer",
        "total_logs_count": "integer",
        "inputs": ("list", {0: {"name": None}}),
    }
    response = client.evaluators.get(id="ev_890bcd")
    validate_response(response, expected_response, expected_types)

    async_response = await async_client.evaluators.get(id="ev_890bcd")
    validate_response(async_response, expected_response, expected_types)


async def test_delete(client: Humanloop, async_client: AsyncHumanloop) -> None:
    # Type ignore to avoid mypy complaining about the function not being meant to return a value
    assert client.evaluators.delete(id="ev_890bcd") is None  # type: ignore[func-returns-value]

    assert await async_client.evaluators.delete(id="ev_890bcd") is None  # type: ignore[func-returns-value]


async def test_move(client: Humanloop, async_client: AsyncHumanloop) -> None:
    expected_response: typing.Any = {
        "id": "ev_890bcd",
        "name": "Accuracy Evaluator",
        "path": "Shared Evaluators/Accuracy Evaluator",
        "version_id": "evv_012def",
        "type": "evaluator",
        "created_at": "2024-05-01T12:00:00Z",
        "updated_at": "2024-05-01T12:00:00Z",
        "status": "committed",
        "last_used_at": "2024-05-01T12:00:00Z",
        "spec": {
            "arguments_type": "target_required",
            "return_type": "number",
            "evaluator_type": "python",
            "code": "def evaluate(answer, target):\\n    return 0.5",
        },
        "version_logs_count": 1,
        "total_logs_count": 1,
        "inputs": [{"name": "answer"}],
    }
    expected_types: typing.Any = {
        "id": None,
        "name": None,
        "path": None,
        "version_id": None,
        "type": None,
        "created_at": "datetime",
        "updated_at": "datetime",
        "status": None,
        "last_used_at": "datetime",
        "spec": {"arguments_type": None, "return_type": None, "evaluator_type": None, "code": None},
        "version_logs_count": "integer",
        "total_logs_count": "integer",
        "inputs": ("list", {0: {"name": None}}),
    }
    response = client.evaluators.move(id="ev_890bcd", path="new directory/new name")
    validate_response(response, expected_response, expected_types)

    async_response = await async_client.evaluators.move(id="ev_890bcd", path="new directory/new name")
    validate_response(async_response, expected_response, expected_types)


async def test_list_versions(client: Humanloop, async_client: AsyncHumanloop) -> None:
    expected_response: typing.Any = {
        "records": [
            {
                "id": "ev_890bcd",
                "name": "Accuracy Evaluator",
                "path": "Shared Evaluators/Accuracy Evaluator",
                "version_id": "evv_012def",
                "type": "evaluator",
                "created_at": "2024-05-01T12:00:00Z",
                "updated_at": "2024-05-01T12:00:00Z",
                "status": "committed",
                "last_used_at": "2024-05-01T12:00:00Z",
                "spec": {
                    "arguments_type": "target_required",
                    "return_type": "number",
                    "evaluator_type": "python",
                    "code": "def evaluate(answer, target):\\n    return 0.5",
                },
                "version_logs_count": 1,
                "total_logs_count": 1,
                "inputs": [{"name": "answer"}],
            }
        ]
    }
    expected_types: typing.Any = {
        "records": (
            "list",
            {
                0: {
                    "id": None,
                    "name": None,
                    "path": None,
                    "version_id": None,
                    "type": None,
                    "created_at": "datetime",
                    "updated_at": "datetime",
                    "status": None,
                    "last_used_at": "datetime",
                    "spec": {"arguments_type": None, "return_type": None, "evaluator_type": None, "code": None},
                    "version_logs_count": "integer",
                    "total_logs_count": "integer",
                    "inputs": ("list", {0: {"name": None}}),
                }
            },
        )
    }
    response = client.evaluators.list_versions(id="ev_890bcd")
    validate_response(response, expected_response, expected_types)

    async_response = await async_client.evaluators.list_versions(id="ev_890bcd")
    validate_response(async_response, expected_response, expected_types)


async def test_commit(client: Humanloop, async_client: AsyncHumanloop) -> None:
    expected_response: typing.Any = {
        "id": "ev_890bcd",
        "name": "Accuracy Evaluator",
        "path": "Shared Evaluators/Accuracy Evaluator",
        "version_id": "evv_012def",
        "type": "evaluator",
        "created_at": "2024-05-01T12:00:00Z",
        "updated_at": "2024-05-01T12:00:00Z",
        "status": "committed",
        "last_used_at": "2024-05-01T12:00:00Z",
        "spec": {
            "arguments_type": "target_required",
            "return_type": "number",
            "evaluator_type": "python",
            "code": "def evaluate(answer, target):\\n    return 0.5",
        },
        "version_logs_count": 1,
        "total_logs_count": 1,
        "inputs": [{"name": "answer"}],
    }
    expected_types: typing.Any = {
        "id": None,
        "name": None,
        "path": None,
        "version_id": None,
        "type": None,
        "created_at": "datetime",
        "updated_at": "datetime",
        "status": None,
        "last_used_at": "datetime",
        "spec": {"arguments_type": None, "return_type": None, "evaluator_type": None, "code": None},
        "version_logs_count": "integer",
        "total_logs_count": "integer",
        "inputs": ("list", {0: {"name": None}}),
    }
    response = client.evaluators.commit(id="ev_890bcd", version_id="evv_012def", commit_message="Initial commit")
    validate_response(response, expected_response, expected_types)

    async_response = await async_client.evaluators.commit(
        id="ev_890bcd", version_id="evv_012def", commit_message="Initial commit"
    )
    validate_response(async_response, expected_response, expected_types)


async def test_set_deployment(client: Humanloop, async_client: AsyncHumanloop) -> None:
    expected_response: typing.Any = {
        "id": "ev_890bcd",
        "name": "Accuracy Evaluator",
        "path": "Shared Evaluators/Accuracy Evaluator",
        "version_id": "evv_012def",
        "type": "evaluator",
        "created_at": "2024-05-01T12:00:00Z",
        "updated_at": "2024-05-01T12:00:00Z",
        "status": "committed",
        "last_used_at": "2024-05-01T12:00:00Z",
        "spec": {
            "arguments_type": "target_required",
            "return_type": "number",
            "evaluator_type": "python",
            "code": "def evaluate(answer, target):\\n    return 0.5",
        },
        "version_logs_count": 1,
        "total_logs_count": 1,
        "inputs": [{"name": "answer"}],
    }
    expected_types: typing.Any = {
        "id": None,
        "name": None,
        "path": None,
        "version_id": None,
        "type": None,
        "created_at": "datetime",
        "updated_at": "datetime",
        "status": None,
        "last_used_at": "datetime",
        "spec": {"arguments_type": None, "return_type": None, "evaluator_type": None, "code": None},
        "version_logs_count": "integer",
        "total_logs_count": "integer",
        "inputs": ("list", {0: {"name": None}}),
    }
    response = client.evaluators.set_deployment(id="ev_890bcd", environment_id="staging", version_id="evv_012def")
    validate_response(response, expected_response, expected_types)

    async_response = await async_client.evaluators.set_deployment(
        id="ev_890bcd", environment_id="staging", version_id="evv_012def"
    )
    validate_response(async_response, expected_response, expected_types)


async def test_remove_deployment(client: Humanloop, async_client: AsyncHumanloop) -> None:
    # Type ignore to avoid mypy complaining about the function not being meant to return a value
    assert client.evaluators.remove_deployment(id="ev_890bcd", environment_id="staging") is None  # type: ignore[func-returns-value]

    assert await async_client.evaluators.remove_deployment(id="ev_890bcd", environment_id="staging") is None  # type: ignore[func-returns-value]


@pytest.mark.skip(reason="Untested")
async def test_list_environments(client: Humanloop, async_client: AsyncHumanloop) -> None:
    expected_response: typing.Any = [
        {
            "id": "env_abc123",
            "created_at": "2024-05-01T12:00:00Z",
            "name": "production",
            "tag": "default",
            "file": {
                "id": "ev_890bcd",
                "name": "Accuracy Evaluator",
                "path": "Shared Evaluators/Accuracy Evaluator",
                "version_id": "evv_012def",
                "type": "evaluator",
                "created_at": "2024-05-01T12:00:00Z",
                "updated_at": "2024-05-01T12:00:00Z",
                "status": "committed",
                "last_used_at": "2024-05-01T12:00:00Z",
                "spec": {
                    "arguments_type": "target_required",
                    "return_type": "number",
                    "evaluator_type": "python",
                    "code": "def evaluate(answer, target):\\n    return 0.5",
                },
                "version_logs_count": 1,
                "total_logs_count": 1,
                "inputs": [{"name": "answer"}],
            },
        }
    ]
    expected_types: typing.Tuple[typing.Any, typing.Any] = (
        "list",
        {
            0: {
                "id": None,
                "created_at": "datetime",
                "name": None,
                "tag": None,
                "file": {
                    "id": None,
                    "name": None,
                    "path": None,
                    "version_id": None,
                    "type": None,
                    "created_at": "datetime",
                    "updated_at": "datetime",
                    "status": None,
                    "last_used_at": "datetime",
                    "spec": {"arguments_type": None, "return_type": None, "evaluator_type": None, "code": None},
                    "version_logs_count": "integer",
                    "total_logs_count": "integer",
                    "inputs": ("list", {0: {"name": None}}),
                },
            }
        },
    )
    response = client.evaluators.list_environments(id="ev_890bcd")
    validate_response(response, expected_response, expected_types)

    async_response = await async_client.evaluators.list_environments(id="ev_890bcd")
    validate_response(async_response, expected_response, expected_types)


async def test_log(client: Humanloop, async_client: AsyncHumanloop) -> None:
    expected_response: typing.Any = {
        "id": "id",
        "parent_id": "parent_id",
        "session_id": "session_id",
        "version_id": "version_id",
    }
    expected_types: typing.Any = {"id": None, "parent_id": None, "session_id": None, "version_id": None}
    response = client.evaluators.log(parent_id="parent_id")
    validate_response(response, expected_response, expected_types)

    async_response = await async_client.evaluators.log(parent_id="parent_id")
    validate_response(async_response, expected_response, expected_types)
